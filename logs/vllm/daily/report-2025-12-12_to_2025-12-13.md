# vLLM å¼€å‘åŠ¨æ€æŠ¥å‘Š - 2025-12-12

> æ—¶é—´çª—å£: 2025-12-12 10:29 (UTC+8) ~ 2025-12-13 10:29 (UTC+8)
> æ•°æ®ç»Ÿè®¡: æ–° Issue 11 | å…³é—­ Issue 7 | æ–° PR 44 | åˆå¹¶ PR 29 | å…³é—­æœªåˆå¹¶ PR 7

---

### ğŸ“Š æ¯æ—¥å¼€å‘çŠ¶æ€æ‘˜è¦
åœ¨2025å¹´12æœˆ12æ—¥è‡³13æ—¥çš„24å°æ—¶çª—å£æœŸå†…ï¼ŒvLLMé¡¹ç›®ä¿æŒé«˜é€Ÿå¼€å‘èŠ‚å¥ï¼Œå…±è®¡æ–°å¢11ä¸ªIssueã€åˆå¹¶29ä¸ªPRï¼Œå¹¶å…³é—­äº†7ä¸ªIssueã€‚å¼€å‘ç„¦ç‚¹é›†ä¸­åœ¨å¤šæ¨¡æ€æ¨¡å‹æ”¯æŒï¼ˆå¦‚GLM-4.6Vã€AudioFlamingo3ï¼‰ã€æ€§èƒ½ä¼˜åŒ–ï¼ˆç‰¹åˆ«æ˜¯é’ˆå¯¹AMDå¹³å°å’ŒMXFP4 Tritonåç«¯ï¼‰ã€ä»¥åŠå„ç±»bugä¿®å¤ä¸Šã€‚AMDç”Ÿæ€ç›¸å…³æ”¹è¿›æŒç»­è¿›è¡Œï¼Œç¤¾åŒºå¯¹CUDAç‰ˆæœ¬å…¼å®¹æ€§ã€æ–°ç¡¬ä»¶æ”¯æŒï¼ˆå¦‚Blackwellï¼‰ä»¥åŠå·¥å…·è°ƒç”¨åŠŸèƒ½çš„è®¨è®ºè¾ƒä¸ºçƒ­çƒˆã€‚

### ğŸ¯ AMD/ROCm ç”Ÿæ€ç›¸å…³åŠ¨æ€
æœ¬å‘¨æœŸå†…ï¼ŒAMD/ROCmç”Ÿæ€ç›¸å…³çš„å¼€å‘ä¸ä¿®å¤æ´»åŠ¨æ´»è·ƒï¼Œä¸»è¦é›†ä¸­åœ¨æ€§èƒ½ä¼˜åŒ–ã€Bugä¿®å¤å’Œæµ‹è¯•ç¨³å®šæ€§æå‡ä¸Šï¼Œæœªå‘ç°æ¶‰åŠQuarké‡åŒ–å·¥å…·çš„æ›´æ–°ã€‚

**æ–°å¢ PR (ROCmæ ‡ç­¾)ï¼š**
1.  **PR #30582: [ROCm] Restore 16-wide fast path in Triton unified attention**
    *   **ç”¨æˆ·:** `hyoon1` (éAMDå‘˜å·¥)
    *   **å†…å®¹:** é’ˆå¯¹AMD RDNA 3/4æ¶æ„ï¼ˆgfx11/gfx12ï¼‰çš„WMMAç‰¹æ€§ï¼Œä¿®å¤äº†å› PR #21197å¼•å…¥çš„Tritonç»Ÿä¸€æ³¨æ„åŠ›å†…æ ¸æ€§èƒ½å›å½’é—®é¢˜ã€‚é€šè¿‡æ·»åŠ å—å¯¹é½çš„å¿«é€Ÿè·¯å¾„ï¼Œå¹¶åœ¨ç‰¹å®šæ¶æ„ä¸Šè®¾ç½®é€‚å½“çš„ç“¦ç‰‡å¤§å°ï¼Œä½¿å†…æ ¸é‡æ–°åŒ¹é…16åˆ—çš„ç¡¬ä»¶WMMAç‰‡æ®µï¼Œæ¢å¤äº†æ€§èƒ½ã€‚**è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å¹³å°ç‰¹å®šæ€§èƒ½ä¼˜åŒ–ã€‚**
2.  **PR #30586: [ROCm] [AITER] [DOC] Add usage description about check functions...**
    *   **ç”¨æˆ·:** `tjtanaa` (éAMDå‘˜å·¥)
    *   **å†…å®¹:** ä¸º`_aiter_ops`ç±»æ·»åŠ ä½¿ç”¨æè¿°ï¼Œå¹¶åºŸå¼ƒäº†æ—§çš„`VLLM_ROCM_USE_AITER_PAGED_ATTN`ç¯å¢ƒå˜é‡ã€‚è¿™å±äºæ–‡æ¡£æ•´ç†å’Œä»£ç æ¸…ç†ã€‚
3.  **PR #30576: [ROCm][CI] Add retry logic and xfail handling for flaky ROCm test...**
    *   **ç”¨æˆ·:** `AndreasKaratzas` (éAMDå‘˜å·¥)
    *   **å†…å®¹:** ä¸ºROCmä¸Šå› æµ®ç‚¹éç¡®å®šæ€§å¯¼è‡´é—´æ­‡æ€§å¤±è´¥çš„å¼‚æ­¥è°ƒåº¦æµ‹è¯•æ·»åŠ äº†é‡è¯•é€»è¾‘å’Œé¢„æœŸå¤±è´¥ï¼ˆxfailï¼‰æ ‡è®°ï¼Œä»¥æå‡CIç¨³å®šæ€§ã€‚

**å·²å…³é—­ Issue (AMDç›¸å…³)ï¼š**
1.  **Issue #14397: `triton_scaled_mm` never used on ROCm**
    *   **çŠ¶æ€:** å·²å…³é—­ (ç”± PR #26668 è§£å†³)
    *   **åˆ†æ:** è¿™æ˜¯ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„bugï¼Œå¯¼è‡´ROCmå¹³å°æ— æ³•å›é€€åˆ°Tritonå®ç°çš„FP8ç¼©æ”¾çŸ©é˜µä¹˜æ³•å†…æ ¸ã€‚ç»è¿‡ç¤¾åŒºè´¡çŒ®è€…ï¼ˆ`shivampr`ï¼‰æ•°æœˆçš„å¼€å‘ã€è°ƒè¯•å’Œæµ‹è¯•éªŒè¯ï¼ˆåŒ…æ‹¬åœ¨MI300Xä¸Šçš„åŠŸèƒ½æµ‹è¯•å’Œ`lm-eval`è¯„ä¼°ï¼‰ï¼Œæœ€ç»ˆé€šè¿‡PR #26668ä¿®å¤ã€‚**æ­¤ä¿®å¤æ ‡å¿—ç€AMDå¹³å°åœ¨FP8é‡åŒ–è®¡ç®—è·¯å¾„ä¸Šçš„ä¸€ä¸ªé‡è¦è¡¥å…¨ã€‚**

**å·²åˆå¹¶ PR (AMDç›¸å…³)ï¼š**
1.  **PR #30292 & PR #30291:** è¿™ä¸¤ä¸ªPRç”±`rasmith`æäº¤ï¼Œä¿®å¤äº†ROCmå¹³å°ä¸ŠFP8ç»„é‡åŒ–åŠå…¶å‚è€ƒå®ç°ä¸­å› ä½¿ç”¨é”™è¯¯çš„FP8æ•°å€¼èŒƒå›´ï¼ˆ-240/240 è€Œé -224/224ï¼‰å¯¼è‡´çš„ç²¾åº¦é—®é¢˜å’Œæµ‹è¯•å¤±è´¥ã€‚**è¿™ç¡®ä¿äº†AMDå¹³å°ä¸Šé‡åŒ–æ“ä½œçš„æ­£ç¡®æ€§å’Œæ•°å€¼ä¸€è‡´æ€§ã€‚**
2.  **PR #26668: [ROCm] Enable Triton ScaledMM fallback + kernel selection fix**
    *   **ç”¨æˆ·:** `shivampr` (éAMDå‘˜å·¥)
    *   **å†…å®¹:** å¦‚ä¸Šæ–‡æ‰€è¿°ï¼Œæ­¤PRè§£å†³äº†Issue #14397ï¼Œå®ç°äº†åœ¨ROCmå¹³å°ä¸Šå½“AITritonä¸å¯ç”¨æ—¶ï¼Œå¯å›é€€è‡³Triton `scaled_mm`å†…æ ¸çš„èƒ½åŠ›ï¼Œå¹¶ä¿®å¤äº†å†…æ ¸é€‰æ‹©é€»è¾‘ã€‚
3.  **PR #30272: [CI/Build] Use spawn subprocess for ROCm**
    *   **ç”¨æˆ·:** `rjrock` (éAMDå‘˜å·¥)
    *   **å†…å®¹:** ä¿®å¤äº†ROCmåç«¯åœ¨æ•°æ®å¹¶è¡Œç¤ºä¾‹ä¸­å› ä½¿ç”¨`fork`åˆ›å»ºå­è¿›ç¨‹å¯¼è‡´çš„Torché‡æ–°åˆå§‹åŒ–é”™è¯¯ï¼Œå¼ºåˆ¶ä½¿ç”¨`spawn`æ–¹æ³•ï¼Œæå‡äº†ç¤ºä¾‹çš„å…¼å®¹æ€§ã€‚

**å°ç»“ï¼š** æœ¬å‘¨æœŸAMDç”Ÿæ€çš„æ›´æ–°ä»¥**æ€§èƒ½è°ƒä¼˜ï¼ˆTritonæ³¨æ„åŠ›ï¼‰**å’Œ**è´¨é‡åŠ å›ºï¼ˆä¿®å¤é‡åŒ–Bugã€ç¨³å®šCIæµ‹è¯•ï¼‰**ä¸ºä¸»ï¼Œå¹¶ä¸”æˆåŠŸå…³é—­äº†ä¸€ä¸ªå…³é”®çš„é•¿æœŸé—ç•™é—®é¢˜ï¼ˆ`triton_scaled_mm`ï¼‰ï¼Œè¡¨æ˜ç¤¾åŒºå¯¹AMDå¹³å°çš„æ”¯æŒæ­£ä»â€œåŠŸèƒ½å¯ç”¨â€å‘â€œæ€§èƒ½ä¼˜åŒ–ä¸ç¨³å®šæ€§â€é˜¶æ®µæ·±åŒ–ã€‚

### ğŸ’¬ é«˜çƒ­åº¦è®¨è®ºåˆ†æ
1.  **Issue #30543: Ministral-3-14B-Instruct-2512 C++ Compilation Error...**
    *   **æ ¸å¿ƒè®®é¢˜:** ç”¨æˆ·åœ¨å°è¯•ä½¿ç”¨`Ministral-3-14B-Instruct-2512`æ¨¡å‹æ—¶ï¼Œé‡åˆ°Torch Inductor C++ç¼–è¯‘é”™è¯¯ï¼Œä»¥åŠåœ¨ç¦ç”¨`enforce_eager`æ—¶æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥çš„é—®é¢˜ã€‚
    *   **è®¨è®ºçŠ¶æ€:** ç›®å‰å°šæ— å…¶ä»–å¼€å‘è€…è·Ÿè¿›è¯„è®ºã€‚è¯¥Issueä¸»è¦æ˜¯ä¸€ä¸ªè¯¦ç»†çš„é”™è¯¯æŠ¥å‘Šï¼Œæœªå½¢æˆå¤šæ–¹è®¨è®ºï¼Œä½†å› å…¶æ¶‰åŠè¾ƒæ–°çš„æ¨¡å‹å’Œç¼–è¯‘é—®é¢˜è€Œå€¼å¾—å…³æ³¨ã€‚
2.  **Issue #30324: RuntimeError: bmm_fp8_internal_cublaslt failed when deploying**
    *   **æ ¸å¿ƒè®®é¢˜:** å¤šä¸ªç”¨æˆ·ï¼ˆä½¿ç”¨GB10ã€RTX 6000 Proã€B200ç­‰ä¸åŒBlackwellæ¶æ„GPUï¼‰åœ¨éƒ¨ç½²ç‰¹å®šæ¨¡å‹ï¼ˆå¦‚Mistral 123B Instruct 2512ï¼‰æ—¶ï¼Œé­é‡`bmm_fp8_internal_cublaslt failed`é”™è¯¯ã€‚
    *   **è§‚ç‚¹ä¸è§£å†³æ–¹æ¡ˆ:**
        *   **ç”¨æˆ· `LukeKeywalker` æå‡ºæ ¹å› :** ç³»ç»Ÿå®‰è£…çš„CUDAç‰ˆæœ¬ï¼ˆ13.0ï¼‰ä¸ç¼–è¯‘PyTorchæ‰€ç”¨çš„CUDAç‰ˆæœ¬ï¼ˆ12.8/12.9ï¼‰ä¸åŒ¹é…ã€‚
        *   **è§£å†³æ–¹æ¡ˆå…±è¯†:** ç”¨æˆ·ä»¬é€šè¿‡å¹¶è¡Œå®‰è£…ä¸PyTorchåŒ¹é…çš„CUDAå·¥å…·åŒ…ï¼ˆå¦‚12.8/12.9ï¼‰ï¼Œå¹¶æ­£ç¡®è®¾ç½®ç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆ`VLLM_CUDA_HOME`, `CUDA_HOME`, `PATH`, `LD_LIBRARY_PATH`ï¼‰ï¼ŒåŒæ—¶æ¸…é™¤`~/.cache/flashinfer`ç¼“å­˜ï¼ŒæˆåŠŸè§£å†³äº†é—®é¢˜ã€‚
    *   **æ€»ç»“:** è¿™æ˜¯ä¸€ä¸ªç”±**é©±åŠ¨/CUDAç¯å¢ƒé…ç½®æ··ä¹±**å¼•å‘çš„å…¸å‹é—®é¢˜ï¼Œè€ŒévLLMä»£ç æœ¬èº«çš„bugã€‚è®¨è®ºå½¢æˆäº†æœ‰æ•ˆçš„ã€å¯å¤ç°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¸®åŠ©äº†å¤šä½é‡åˆ°ç›¸åŒé—®é¢˜çš„ç”¨æˆ·ã€‚
3.  **Issue #28894 (å·²å…³é—­): [H200] GPT-OSS-120B + Triton MoE Backend Perf Issue**
    *   **æ ¸å¿ƒè®®é¢˜:** GPT-OSS-120Bæ¨¡å‹åœ¨ä½¿ç”¨Triton MoEåç«¯æ—¶ï¼Œåœ¨å°æ‰¹é‡ï¼ˆconcurrency=1,2,4ï¼‰åœºæ™¯ä¸‹æ€§èƒ½æ¯”Marlinåç«¯ä½çº¦30%ã€‚
    *   **è§‚ç‚¹ä¸æ’æŸ¥è¿‡ç¨‹:**
        1.  **æ€§èƒ½å¯¹æ¯”:** å¤šåç”¨æˆ·ï¼ˆ`jhaotingc`, `robertgshaw2-redhat`, `dcmaddix`ï¼‰ç¡®è®¤äº†æ€§èƒ½ä¸‹é™ç°è±¡ã€‚
        2.  **æ ¹å› åˆ†æ:** `xyang16` é€šè¿‡æ·±å…¥åˆ†æå‘ç°ï¼ŒTriton kernelsåœ¨è®¡ç®—`split_k`å¤§äº1æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨`float32`æ•°æ®ç±»å‹è¿›è¡Œè®¡ç®—ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è€Œ`split_k`çš„å¤§å°ä¸è®¡ç®—ç½‘æ ¼è§„æ¨¡ï¼ˆgrid_sizeï¼‰ç›¸å…³ï¼Œç½‘æ ¼è§„æ¨¡åˆå—æ‰¹æ¬¡å¤§å°ï¼ˆmï¼‰å½±å“ã€‚å¯¹äºå°æ‰¹æ¬¡ï¼Œ`split_k`å¯èƒ½å¤§äº1ï¼Œä»è€Œè§¦å‘ä½æ•ˆçš„fp32è®¡ç®—è·¯å¾„ã€‚
    *   **æœ€ç»ˆç»“è®ºä¸è§£å†³æ–¹æ¡ˆ:** è¯¥é—®é¢˜è¢«ç¡®è®¤ä¸ºä¸€ä¸ªæ€§èƒ½bugï¼Œå¹¶ç”± **PR #30528** ä¿®å¤ã€‚è¯¥PRé€šè¿‡ä¸ºHopperæ¶æ„ï¼ˆSM90ï¼‰å¼ºåˆ¶è®¾ç½®`split_k=1`ï¼Œç¡®ä¿Triton kernelså§‹ç»ˆä½¿ç”¨é«˜æ•ˆçš„bf16æ•°æ®ç±»å‹ï¼Œä»è€Œä½¿å°æ‰¹æ¬¡æ€§èƒ½ä¸MarlinæŒå¹³ã€‚æ­¤Issueå› ä¿®å¤è€Œå…³é—­ã€‚
4.  **PR #30550: [Frontend] Support passing custom score template as a CLI argument...**
    *   **æ ¸å¿ƒè®®é¢˜:** æ˜¯å¦åº”è¯¥ä»¥åŠå¦‚ä½•æ”¯æŒé€šè¿‡CLIå‚æ•°ä¸ºæ‰“åˆ†/é‡æ’æ¨¡å‹ï¼ˆå¦‚`nvidia/llama-nemotron-rerank-1b-v2`ï¼‰æŒ‡å®šè‡ªå®šä¹‰æç¤ºæ¨¡æ¿ã€‚
    *   **è§‚ç‚¹:**
        *   **PRä½œè€… (`jzakrzew`):** å½“å‰é€šè¿‡ä¿®æ”¹æ¨¡å‹ç±»æ¥æ”¯æŒè‡ªå®šä¹‰æ¨¡æ¿çš„æ–¹å¼è€¦åˆåº¦é«˜ï¼Œæè®®é€šè¿‡`--score-template` CLIå‚æ•°ä¼ é€’Jinjaæ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼Œå®ç°è§£è€¦ï¼Œä¾¿äºæ”¯æŒæ›´å¤šæ¨¡å‹ã€‚
        *   **ç¤¾åŒºåé¦ˆ (`noooop`, `Samoed`, `tomaarsen`):** å¼•å‘äº†å…³äºâ€œå¦‚ä½•æ ‡å‡†åŒ–è·¨ç¼–ç å™¨ï¼ˆCross-Encoderï¼‰æ¨¡å‹æç¤ºæ¨¡æ¿â€çš„æ›´å¹¿æ³›è®¨è®ºã€‚`tomaarsen`æŒ‡å‡ºå½“å‰ç¼ºä¹æ ‡å‡†ï¼Œå¹¶æå‡ºæ˜¯å¦å¯ä»¥å€Ÿé‰´`chat_template`æˆ–ä½¿ç”¨è‡ªå®šä¹‰`role`ï¼ˆå¦‚`document`ï¼‰ä½œä¸ºè§£å†³æ–¹æ¡ˆçš„æ€è·¯ã€‚
    *   **å½“å‰çŠ¶æ€:** PRå¤„äºå¼€æ”¾çŠ¶æ€ï¼Œè®¨è®ºä»å…·ä½“å®ç°æ‰©å±•åˆ°äº†è¡Œä¸šæ ‡å‡†åŒ–çš„å±‚é¢ï¼Œå°šæœªå½¢æˆæœ€ç»ˆç»“è®ºã€‚è¿™åæ˜ äº†ç¤¾åŒºåœ¨è§£å†³æ­¤ç±»é€šç”¨æ€§é—®é¢˜æ—¶çš„æ·±å…¥æ€è€ƒã€‚

### ğŸ”¥ çƒ­é—¨è¯é¢˜ä¸è¶‹åŠ¿åˆ†æ
1.  **å¤šæ¨¡æ€æ¨¡å‹æ”¯æŒæŒç»­å‡æ¸©:** æ–°å¢äº†å¯¹`AudioFlamingo3`éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ”¯æŒï¼ˆPR #30539ï¼‰ï¼ŒåŒæ—¶å‡ºç°äº†`GLM-4.6V`ä¸Transformers v5å…¼å®¹æ€§çš„bugï¼ˆIssue #30584åŠä¿®å¤PR #30583ï¼‰ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹Qwen2-VLã€Gemma3ç­‰æ¨¡å‹çš„å›¾åƒç¼–ç å™¨ç¼“å­˜é¢„ç®—è®¡ç®—ä¹Ÿè¿›è¡Œäº†ä¿®å¤ï¼ˆPR #30536, #29692ï¼‰ã€‚
2.  **æ€§èƒ½ä¼˜åŒ–ä¸ç¡¬ä»¶é€‚é…:**
    *   **AMDå¹³å°:** å¦‚å‰è¿°ï¼Œé’ˆå¯¹Tritonæ³¨æ„åŠ›å†…æ ¸å’Œé‡åŒ–æ“ä½œè¿›è¡Œäº†ä¸“é¡¹ä¼˜åŒ–å’Œä¿®å¤ã€‚
    *   **BlackwellåŠå…¶ä»–æ–°ç¡¬ä»¶:** å‡ºç°äº†å…³äºGB10/B200ç­‰Blackwell GPUçš„å…¼å®¹æ€§é—®é¢˜ï¼ˆIssue #30579, #30324ï¼‰å’ŒGGUFæ ¼å¼åœ¨Blackwellä¸Šçš„ç²¾åº¦ä¿®å¤ï¼ˆPR #30408ï¼‰ã€‚åŒæ—¶ï¼Œæœ‰PRå°è¯•ä¸ºGB300ï¼ˆSM103ï¼‰æ·»åŠ FlashInferæ³¨æ„åŠ›æ”¯æŒï¼ˆPR #30565ï¼‰ã€‚
    *   **å†…æ ¸ä¼˜åŒ–:** é’ˆå¯¹Triton MoEåç«¯çš„å°æ‰¹é‡æ€§èƒ½é—®é¢˜è¿›è¡Œäº†å…³é”®ä¿®å¤ï¼ˆPR #30528ï¼‰ã€‚
3.  **å¼€å‘è€…ä½“éªŒä¸è¿ç»´å·¥å…·:** å‡ºç°äº†æ·»åŠ æœåŠ¡é¢„çƒ­ï¼ˆ`--warmup-config`ï¼ŒPR #30561ï¼‰ã€GPUå†…å­˜å¿«ç…§åˆ†æï¼ˆPR #30580ï¼‰ç­‰åŠŸèƒ½çš„æè®®ï¼Œæ—¨åœ¨æ”¹å–„ç”Ÿäº§ç¯å¢ƒä¸‹çš„ç¨³å®šæ€§å’Œå¯è°ƒè¯•æ€§ã€‚
4.  **å®‰å…¨ä¸åˆè§„æ€§å…³æ³¨:** æœ‰PRé’ˆå¯¹ä¹‹å‰ä¿®å¤çš„CVEæ¼æ´å¢åŠ äº†é¢å¤–çš„ä¿æŠ¤æªæ–½ï¼ˆPR #30572ï¼‰ï¼Œæ˜¾ç¤ºé¡¹ç›®å¯¹å®‰å…¨é—®é¢˜çš„æŒç»­å…³æ³¨ã€‚

### ğŸ› ï¸ é‡ç‚¹æŠ€æœ¯å˜æ›´
1.  **PR #30528 ([Perf] Set split_k to 1 for triton_kernels):** **æ ¸å¿ƒæ€§èƒ½ä¿®å¤ã€‚** é€šè¿‡å¼ºåˆ¶è®¾ç½®`split_k=1`ï¼Œè§£å†³äº†Triton MXFP4åç«¯åœ¨å°æ‰¹é‡æ¨ç†æ—¶å› é”™è¯¯ä½¿ç”¨fp32æ•°æ®ç±»å‹å¯¼è‡´çš„ä¸¥é‡æ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œå¯¹GPT-OSSç­‰ä½¿ç”¨è¯¥åç«¯çš„æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—æå‡ã€‚
2.  **PR #30564 & #30563 (ç§»é™¤ VLLM_ATTENTION_BACKEND ç›¸å…³ä»£ç ):** **é…ç½®ç³»ç»Ÿæ¸…ç†ã€‚** éšç€PR #26315å°†æ³¨æ„åŠ›åç«¯é€‰æ‹©æœºåˆ¶ä»ç¯å¢ƒå˜é‡è¿ç§»åˆ°`--attention-config`å‚æ•°ï¼Œæœ¬æ¬¡æ›´æ–°æ¸…é™¤äº†æ–‡æ¡£å’Œæµ‹è¯•ä¸­æ‰€æœ‰å¯¹å·²å¼ƒç”¨ç¯å¢ƒå˜é‡`VLLM_ATTENTION_BACKEND`çš„å¼•ç”¨ï¼Œæ ‡å¿—ç€è¯¥é‡æ„å·¥ä½œçš„æ”¶å°¾ã€‚
3.  **PR #30580 ([memory] Add torch memory snapshot dump during model loading stage):** **å¯è§‚æµ‹æ€§å¢å¼ºã€‚** å¼•å…¥äº†åœ¨æ¨¡å‹åŠ è½½é˜¶æ®µè‡ªåŠ¨è½¬å‚¨PyTorchå†…å­˜å¿«ç…§çš„åŠŸèƒ½ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡`VLLM_MEMORY_SNAPSHOT_DIR`æ§åˆ¶ï¼‰ï¼Œç”Ÿæˆçš„å¿«ç…§æ–‡ä»¶å¯é€šè¿‡PyTorchå®˜æ–¹å·¥å…·å¯è§†åŒ–ã€‚è¿™å°†æå¤§ä¾¿åˆ©å¼€å‘è€…åˆ†æå’Œè°ƒè¯•GPUå†…å­˜ä¸è¶³ï¼ˆOOMï¼‰é—®é¢˜ã€‚
4.  **PR #30490 ([DeepSeek V3.2] Proper drop_thinking logic):** **æ¨¡å‹è¡Œä¸ºæ ¡æ­£ã€‚** æ ¹æ®DeepSeek V3.2æŠ€æœ¯æŠ¥å‘Šï¼Œä¿®æ­£äº†å…¶â€œä¸¢å¼ƒæ€è€ƒå†…å®¹â€ï¼ˆdrop_thinkingï¼‰çš„é€»è¾‘ï¼Œä»åŸæ¥çš„â€œä»…å½“é™„å¸¦å·¥å…·è°ƒç”¨æ—¶ä¸ä¸¢å¼ƒâ€æ”¹ä¸ºâ€œå½“æœ€åä¸€æ¡æ¶ˆæ¯æ¥è‡ªç”¨æˆ·æ—¶ä¸¢å¼ƒâ€ã€‚è¿™ä¸€ä¿®æ­£ä¸ä»…æå‡äº†æ¨ç†æ•ˆç‡ï¼ˆèŠ‚çœtokensï¼‰ï¼Œæ›´ç›´æ¥å¸¦æ¥äº†åœ¨TauÂ² AirlineåŸºå‡†æµ‹è¯•ä¸Šçº¦5ä¸ªç™¾åˆ†ç‚¹çš„æ€§èƒ½æå‡ã€‚
5.  **PR #30556 (feat: batched shared encoder for whisper beam search):** **ç®—æ³•ä¼˜åŒ–ã€‚** ä¸ºç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼ˆä»¥Whisperä¸ºä¾‹ï¼‰çš„æŸæœç´¢ï¼ˆbeam searchï¼‰å®ç°äº†ç¼–ç å™¨è¾“å‡ºå¤ç”¨ã€è¯·æ±‚æ‰¹å¤„ç†å’Œæ—©æœŸåœæ­¢ç­‰å…³é”®ä¼˜åŒ–ï¼Œæ—¨åœ¨è§£å†³æ­¤ç±»æ¨¡å‹é¦–æ¬¡æ¨ç†å› ç¼–ç å™¨è®¡ç®—æ˜‚è´µè€Œå¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆã€‚

### ğŸ“ˆ å¼€å‘æ´»è·ƒåº¦è§‚å¯Ÿ
*   **é«˜æ•ˆåˆå¹¶:** åœ¨44ä¸ªæ–°å¢PRä¸­ï¼Œæœ‰29ä¸ªåœ¨åŒæœŸè¢«åˆå¹¶ï¼Œåˆå¹¶ç‡è¾ƒé«˜ï¼Œæ˜¾ç¤ºæ ¸å¿ƒå›¢é˜Ÿå®¡æŸ¥å’Œé›†æˆä»£ç çš„æ•ˆç‡è‰¯å¥½ã€‚
*   **è´¡çŒ®è€…å¤šæ ·åŒ–:** æ–°å¢çš„PRå’ŒIssueæ¥è‡ªå¤§é‡ä¸åŒçš„ç”¨æˆ·ï¼ˆå¦‚`dbotwinick`, `MatthewBonanni`, `WangErXiao`, `xyang16`, `lashahub`ç­‰ï¼‰ï¼Œæ—¢æœ‰ç†Ÿæ‚‰çš„æ ¸å¿ƒè´¡çŒ®è€…ï¼Œä¹Ÿæœ‰æ–°é¢å­”ï¼Œè¡¨æ˜ç¤¾åŒºå‚ä¸åº¦å¹¿æ³›ã€‚
*   **èšç„¦é—®é¢˜è§£å†³:** å¤§å¤šæ•°åˆå¹¶çš„PRå±äºbugä¿®å¤ã€æ€§èƒ½ä¼˜åŒ–å’ŒåŠŸèƒ½å®Œå–„ï¼Œè¡¨æ˜å¼€å‘ä¸»çº¿å¥åº·ï¼Œæ­£è‡´åŠ›äºæå‡é¡¹ç›®çš„ç¨³å®šæ€§ã€æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

### ğŸ’¡ å€¼å¾—å…³æ³¨çš„é—®é¢˜
1.  **Issue #30579: CUDA Illegal Memory Access on 4xB200:** åœ¨æœ€æ–°çš„B200 GPUé›†ç¾¤ä¸Šè¿è¡Œå¤§æ¨¡å‹ï¼ˆQwen3-Next-80Bï¼‰æ—¶å‡ºç°éæ³•å†…å­˜è®¿é—®é”™è¯¯ï¼Œå¯èƒ½ä¸æ–°ç¡¬ä»¶æˆ–é©±åŠ¨å…¼å®¹æ€§æœ‰å…³ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ã€‚
2.  **Issue #30541 & #30554: å·¥å…·è°ƒç”¨ç›¸å…³é—®é¢˜:** DeepSeek-V3.2å·¥å…·è°ƒç”¨ç¼ºå°‘DSMLä»¤ç‰Œã€MiniMax M2å·¥å…·è°ƒç”¨è§£æé”™è¯¯ï¼Œè¿™äº›Issueåæ˜ äº†åœ¨å¤šæ¨¡å‹å·¥å…·è°ƒç”¨æ”¯æŒä¸Šä»å­˜åœ¨ç»†èŠ‚ä¸ä¸€è‡´å’Œbugï¼Œå½±å“åŠŸèƒ½å®Œæ•´æ€§ã€‚
3.  **Issue #30570: Why is VLLM still using SSE...:** ç”¨æˆ·è´¨ç–‘vLLMä¸ºä½•ä»åœ¨å¤šå®¢æˆ·ç«¯åè®®ï¼ˆMCPï¼‰ä¸­ä¾èµ–å·²å¼ƒç”¨åŠå¹´çš„SSEï¼Œè¿™å¯èƒ½æ¶‰åŠæŠ€æœ¯å€ºåŠ¡æˆ–å‘åå…¼å®¹æ€§çš„æƒè¡¡ï¼Œå€¼å¾—ç¤¾åŒºè®¨è®ºæ˜ç¡®æ–¹å‘ã€‚
4.  **PR #30535 ([Core] Add repetitive token detection...):** è¯¥PRæè®®æ·»åŠ é‡å¤ä»¤ç‰Œæ£€æµ‹ä»¥é˜»æ­¢æ¨¡å‹å¹»è§‰ï¼Œæ˜¯ä¸€ä¸ªå®ç”¨çš„æ–°åŠŸèƒ½ã€‚è™½ç„¶å·²é€šè¿‡å†…éƒ¨ä»£ç å®¡æŸ¥ï¼Œä½†ä½œä¸ºå½±å“æ ¸å¿ƒç”Ÿæˆé€»è¾‘çš„å˜æ›´ï¼Œå…¶åˆå¹¶åçš„å®é™…æ•ˆæœå’Œæ½œåœ¨å½±å“å€¼å¾—è§‚å¯Ÿã€‚

---

## ğŸ“‹ é™„å½•ï¼šè¯¦ç»†æ•°æ®åˆ—è¡¨

### æ–°å¢ Issue
- [#30584](https://github.com/vllm-project/vllm/issues/30584) [Bug]: GLM-4.6V-Flash + transformers v5: vLLM passes MediaWithBytes to HF image processor, causing 400 errors for multimodal requests â€” bug â€” by dbotwinick (åˆ›å»ºäº: 2025-12-13 09:26 (UTC+8))
- [#30579](https://github.com/vllm-project/vllm/issues/30579) [Bug]: CUDA Illegal Memory Access when running Qwen3-Next-80B-A3B-Instruct on 4xB200 GPUs â€” bug â€” by BolinSNLHM (åˆ›å»ºäº: 2025-12-13 06:24 (UTC+8))
- [#30541](https://github.com/vllm-project/vllm/issues/30541) [Usage]: missing dsml token "| DSML | " with DeepSeek-V3.2 tools call â€” usage â€” by crischeng (åˆ›å»ºäº: 2025-12-12 14:47 (UTC+8))
- [#30571](https://github.com/vllm-project/vllm/issues/30571) [Bug]: DeepGEMM MoE generating tons of warnings â€” bug â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-13 04:14 (UTC+8))
- [#30570](https://github.com/vllm-project/vllm/issues/30570) [Usage]: Why is VLLM still using SSE at all for mcp? â€” usage â€” by bags307 (åˆ›å»ºäº: 2025-12-13 04:02 (UTC+8))
- [#30569](https://github.com/vllm-project/vllm/issues/30569) [Feature]: Add --ssl-ciphers to CLI arguments â€” feature request â€” by GraceMoreau (åˆ›å»ºäº: 2025-12-13 03:41 (UTC+8))
- [#30548](https://github.com/vllm-project/vllm/issues/30548) [Feature]: Support for Q.ANT Photonic Computing ? â€” feature request â€” by plitc (åˆ›å»ºäº: 2025-12-12 18:16 (UTC+8))
- [#30554](https://github.com/vllm-project/vllm/issues/30554) [Bug]: When using minimax m2 model tool call, throw IndexError in serving_chat.py â€” bug â€” by WangErXiao (åˆ›å»ºäº: 2025-12-12 22:05 (UTC+8))
- [#30546](https://github.com/vllm-project/vllm/issues/30546) [Bug]: Incorrect dimension movement in reduce_scatter (first movedim(0, dim) should be movedim(dim, 0)) â€” bug â€” by RKai025 (åˆ›å»ºäº: 2025-12-12 17:16 (UTC+8))
- [#30543](https://github.com/vllm-project/vllm/issues/30543) [Bug]: Ministral-3-14B-Instruct-2512 C++ Compilation Error with Torch Inductor and Batch Generation Failure with enforce_eager=False â€” bug â€” by KosmoCHE (åˆ›å»ºäº: 2025-12-12 16:47 (UTC+8))
- [#30533](https://github.com/vllm-project/vllm/issues/30533) [Bug]: Fail to run Qwen3-Next-80B-A3B-Instruct vllm with CPU backend â€” bug,cpu â€” by MaoJianwei (åˆ›å»ºäº: 2025-12-12 11:32 (UTC+8))

### å·²å…³é—­ Issue
- [#22479](https://github.com/vllm-project/vllm/issues/22479) [Bug]: --tensor-parallel-size 2 seems broken for Blackwell 6000 pro since version 10 â€” bug,stale â€” by fernandaspets (å…³é—­äº: 2025-12-13 10:10 (UTC+8))
- [#26582](https://github.com/vllm-project/vllm/issues/26582) [Bug]: which triton-kernels version for MXFP4 Triton backend? â€” bug â€” by matkle (å…³é—­äº: 2025-12-13 04:30 (UTC+8))
- [#28894](https://github.com/vllm-project/vllm/issues/28894) [Bug]: [H200] GPT-OSS-120B + Triton MoE Backend Perf Issue â€” bug â€” by jhaotingc (å…³é—­äº: 2025-12-13 04:29 (UTC+8))
- [#14397](https://github.com/vllm-project/vllm/issues/14397) [Bug]: `triton_scaled_mm` never used on ROCm â€” bug,good first issue,unstale â€” by ProExpertProg (å…³é—­äº: 2025-12-13 02:28 (UTC+8))
- [#30511](https://github.com/vllm-project/vllm/issues/30511) Potential Deadlock? â€” æ— æ ‡ç­¾ â€” by ChuanLi1101 (å…³é—­äº: 2025-12-13 02:00 (UTC+8))
- [#30324](https://github.com/vllm-project/vllm/issues/30324) [Bug]: RuntimeError: bmm_fp8_internal_cublaslt failed when deploying â€” bug â€” by Austin-Liang (å…³é—­äº: 2025-12-12 14:25 (UTC+8))
- [#30326](https://github.com/vllm-project/vllm/issues/30326) [Bug]: H200 GPT-OSS-120B has perf drop â€” bug â€” by shyeh25 (å…³é—­äº: 2025-12-12 13:39 (UTC+8))

### æ–°å¢ PR
- [#30555](https://github.com/vllm-project/vllm/pull/30555) [Bugfix][Frontend] Prevent IndexError in MiniMax M2 tool parser during streaming extraction â€” frontend,tool-calling â€” by WangErXiao (åˆ›å»ºäº: 2025-12-12 22:07 (UTC+8))
- [#30587](https://github.com/vllm-project/vllm/pull/30587) [Bugfix] Record request stats when request is aborted â€” v1 â€” by pooyadavoodi (åˆ›å»ºäº: 2025-12-13 09:48 (UTC+8))
- [#30563](https://github.com/vllm-project/vllm/pull/30563) [Attention] Update tests to remove deprecated env vars â€” rocm,speculative-decoding,ready,ci/build,v1,multi-modality,kv-connector,nvidia â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-13 01:12 (UTC+8))
- [#30564](https://github.com/vllm-project/vllm/pull/30564) [Docs] Remove references to `VLLM_ATTENTION_BACKEND` â€” documentation,ready â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-13 01:31 (UTC+8))
- [#30582](https://github.com/vllm-project/vllm/pull/30582) [ROCm] Restore 16-wide fast path in Triton unified attention â€” rocm â€” by hyoon1 (åˆ›å»ºäº: 2025-12-13 09:02 (UTC+8))
- [#30586](https://github.com/vllm-project/vllm/pull/30586) [ROCm] [AITER] [DOC] Add usage description about check functions in `_aiter_ops` â€” rocm â€” by tjtanaa (åˆ›å»ºäº: 2025-12-13 09:38 (UTC+8))
- [#30585](https://github.com/vllm-project/vllm/pull/30585) [Bugfix] Fix NaN issue in attention output â€” v1 â€” by xyang16 (åˆ›å»ºäº: 2025-12-13 09:35 (UTC+8))
- [#30575](https://github.com/vllm-project/vllm/pull/30575) [Bugfix] Pass FA version in `MultiHeadAttention` â€” ready â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-13 05:31 (UTC+8))
- [#30581](https://github.com/vllm-project/vllm/pull/30581) Add IBM and Red Hat to compute resources sponsors â€” documentation,ready â€” by mgoin (åˆ›å»ºäº: 2025-12-13 07:32 (UTC+8))
- [#30583](https://github.com/vllm-project/vllm/pull/30583) Update get_processor_data to use get_all method â€” multi-modality â€” by dbotwinick (åˆ›å»ºäº: 2025-12-13 09:24 (UTC+8))
- [#30553](https://github.com/vllm-project/vllm/pull/30553) [Bug][CPU Backend]: Improve L2 cache size detection and usage on aarch64 â€” æ— æ ‡ç­¾ â€” by Radu2k (åˆ›å»ºäº: 2025-12-12 21:40 (UTC+8))
- [#30568](https://github.com/vllm-project/vllm/pull/30568) [WIP][CI] Speed up sequence parallel tests  â€” ready,ci/build â€” by LucasWilkinson (åˆ›å»ºäº: 2025-12-13 03:16 (UTC+8))
- [#30580](https://github.com/vllm-project/vllm/pull/30580) [memory] Add torch memory snapshot dump during model loading stage â€” v1 â€” by minosfuture (åˆ›å»ºäº: 2025-12-13 06:56 (UTC+8))
- [#30574](https://github.com/vllm-project/vllm/pull/30574) MLA Based Eagle3 â€” new-model,speculative-decoding,v1,deepseek â€” by IzzyPutterman (åˆ›å»ºäº: 2025-12-13 05:05 (UTC+8))
- [#30577](https://github.com/vllm-project/vllm/pull/30577) [Bug][KVConnector][Metrics] Remove a vacuous assertion breaking external-launcher â€” kv-connector,fb-exported,meta-exported â€” by QierLi (åˆ›å»ºäº: 2025-12-13 06:01 (UTC+8))
- [#30578](https://github.com/vllm-project/vllm/pull/30578) [ci] Mark PrimeRL integration test as soft fail â€” ci/build â€” by khluu (åˆ›å»ºäº: 2025-12-13 06:09 (UTC+8))
- [#30576](https://github.com/vllm-project/vllm/pull/30576) [ROCm][CI] Add retry logic and xfail handling for flaky ROCm test in test_async_scheduling â€” rocm,v1 â€” by AndreasKaratzas (åˆ›å»ºäº: 2025-12-13 05:44 (UTC+8))
- [#30573](https://github.com/vllm-project/vllm/pull/30573) [Misc][Refactor] Separate router from FusedMoE class â€” æ— æ ‡ç­¾ â€” by bnellnm (åˆ›å»ºäº: 2025-12-13 04:16 (UTC+8))
- [#30535](https://github.com/vllm-project/vllm/pull/30535) [Core] Add repetitive token detection for hallucination prevention â€” documentation,frontend,v1 â€” by jeremyteboul (åˆ›å»ºäº: 2025-12-12 13:07 (UTC+8))
- [#30539](https://github.com/vllm-project/vllm/pull/30539) Add AudioFlamingo3 model support â€” documentation,new-model,multi-modality â€” by lashahub (åˆ›å»ºäº: 2025-12-12 14:22 (UTC+8))
- [#30572](https://github.com/vllm-project/vllm/pull/30572) Add additional protection for CVE-2025-62164 â€” frontend,multi-modality â€” by russellb (åˆ›å»ºäº: 2025-12-13 04:16 (UTC+8))
- [#30565](https://github.com/vllm-project/vllm/pull/30565) [Bug] add sm100f check for trtllm-gen flashinfer attention and moe â€” v1,nvidia â€” by IwakuraRein (åˆ›å»ºäº: 2025-12-13 01:42 (UTC+8))
- [#30561](https://github.com/vllm-project/vllm/pull/30561) feat(serve): add warmup support for consistent first-request performance â€” documentation,frontend â€” by TheCodeWrangler (åˆ›å»ºäº: 2025-12-13 00:52 (UTC+8))
- [#30550](https://github.com/vllm-project/vllm/pull/30550) [Frontend] Support passing custom score template as a CLI argument to vllm serve â€” documentation,frontend â€” by jzakrzew (åˆ›å»ºäº: 2025-12-12 20:16 (UTC+8))
- [#30567](https://github.com/vllm-project/vllm/pull/30567) [Bug] Fix AttributeError: 'Qwen3VLMoeConfig' object has no attribute 'intermediate_size' â€” ready,qwen â€” by yewentao256 (åˆ›å»ºäº: 2025-12-13 02:14 (UTC+8))
- [#30540](https://github.com/vllm-project/vllm/pull/30540) [Doc]: fixing typos in various files â€” documentation,frontend,ready,needs-rebase,nvidia â€” by didier-durand (åˆ›å»ºäº: 2025-12-12 14:43 (UTC+8))
- [#30547](https://github.com/vllm-project/vllm/pull/30547) [CustomOp] Support object-level enable for CustomOp â€” æ— æ ‡ç­¾ â€” by shen-shanshan (åˆ›å»ºäº: 2025-12-12 18:12 (UTC+8))
- [#30551](https://github.com/vllm-project/vllm/pull/30551) docs: Clarify block_quant_to_tensor_quant docstring (fixes #30098) â€” æ— æ ‡ç­¾ â€” by yurekami (åˆ›å»ºäº: 2025-12-12 21:36 (UTC+8))
- [#30556](https://github.com/vllm-project/vllm/pull/30556) feat: batched shared encoder for whisper beam search â€” documentation,performance,rocm,structured-output,frontend,speculative-decoding,ci/build,v1,multi-modality,tool-calling â€” by TheCodeWrangler (åˆ›å»ºäº: 2025-12-12 22:23 (UTC+8))
- [#30562](https://github.com/vllm-project/vllm/pull/30562) [Refactor] Small refactor for group topk â€” ready,v1 â€” by yewentao256 (åˆ›å»ºäº: 2025-12-13 00:58 (UTC+8))
- [#30566](https://github.com/vllm-project/vllm/pull/30566) update to transformers v5 â€” ready,ci/build â€” by hmellor (åˆ›å»ºäº: 2025-12-13 01:44 (UTC+8))
- [#30558](https://github.com/vllm-project/vllm/pull/30558) [Core] Support multi prompt for AsyncLLM.generate() and encode() â€” documentation,v1 â€” by buaazp (åˆ›å»ºäº: 2025-12-13 00:28 (UTC+8))
- [#30560](https://github.com/vllm-project/vllm/pull/30560) My bad - ignore. â€” documentation,needs-rebase,ci/build â€” by fadara01 (åˆ›å»ºäº: 2025-12-13 00:36 (UTC+8))
- [#30559](https://github.com/vllm-project/vllm/pull/30559) [Feat] Enable eplb with default all2all backend â€” ready â€” by yewentao256 (åˆ›å»ºäº: 2025-12-13 00:31 (UTC+8))
- [#30557](https://github.com/vllm-project/vllm/pull/30557) [GPT OSS] Fix tool_choice required â€” frontend,gpt-oss â€” by southfreebird (åˆ›å»ºäº: 2025-12-12 22:30 (UTC+8))
- [#30537](https://github.com/vllm-project/vllm/pull/30537) Filter safetensors files to download if .safetensors.index.json exists â€” æ— æ ‡ç­¾ â€” by mgoin (åˆ›å»ºäº: 2025-12-12 13:36 (UTC+8))
- [#30534](https://github.com/vllm-project/vllm/pull/30534) [Bug] Fix attention_backend arg string parsing â€” bug,ready â€” by mgoin (åˆ›å»ºäº: 2025-12-12 12:31 (UTC+8))
- [#30545](https://github.com/vllm-project/vllm/pull/30545) [Frontend] Map `service_tier` to `priority` for OpenAI API endpoints â€” frontend â€” by meffmadd (åˆ›å»ºäº: 2025-12-12 17:12 (UTC+8))
- [#30542](https://github.com/vllm-project/vllm/pull/30542) [Bugfix] Revert Qwen2-VL part of change in #28271 â€” ready,qwen â€” by zifeitong (åˆ›å»ºäº: 2025-12-12 15:10 (UTC+8))
- [#30552](https://github.com/vllm-project/vllm/pull/30552) typing: Add type hints to TurnMetrics class in context.py â€” frontend,gpt-oss â€” by yurekami (åˆ›å»ºäº: 2025-12-12 21:38 (UTC+8))
- [#30549](https://github.com/vllm-project/vllm/pull/30549) [Core] WhisperEncoder support `torch.compile`  â€” v1 â€” by NickLucche (åˆ›å»ºäº: 2025-12-12 18:48 (UTC+8))
- [#30544](https://github.com/vllm-project/vllm/pull/30544) [KVEvent] User request.block_hash for parent block_hash â€” v1 â€” by heheda12345 (åˆ›å»ºäº: 2025-12-12 16:50 (UTC+8))
- [#30538](https://github.com/vllm-project/vllm/pull/30538) [XPU] decrease IGC_ForceOCLSIMDWidth for speculative decoding triton-xpu kernel compilation â€” ci/build â€” by yma11 (åˆ›å»ºäº: 2025-12-12 14:05 (UTC+8))
- [#30536](https://github.com/vllm-project/vllm/pull/30536) encoder cache optimization budget alignment â€” v1,multi-modality,qwen â€” by sunYtokki (åˆ›å»ºäº: 2025-12-12 13:15 (UTC+8))

### å·²åˆå¹¶ PR
- [#30514](https://github.com/vllm-project/vllm/pull/30514) [CI] Update several models in registry that are available online now â€” ready,ci/build â€” by mgoin (åˆå¹¶äº: 2025-12-13 10:28 (UTC+8))
- [#30564](https://github.com/vllm-project/vllm/pull/30564) [Docs] Remove references to `VLLM_ATTENTION_BACKEND` â€” documentation,ready â€” by MatthewBonanni (åˆå¹¶äº: 2025-12-13 10:20 (UTC+8))
- [#30575](https://github.com/vllm-project/vllm/pull/30575) [Bugfix] Pass FA version in `MultiHeadAttention` â€” ready â€” by MatthewBonanni (åˆå¹¶äº: 2025-12-13 08:02 (UTC+8))
- [#30581](https://github.com/vllm-project/vllm/pull/30581) Add IBM and Red Hat to compute resources sponsors â€” documentation,ready â€” by mgoin (åˆå¹¶äº: 2025-12-13 09:34 (UTC+8))
- [#30292](https://github.com/vllm-project/vllm/pull/30292) [CI/Build][Kernel][BugFix][AMD] Fix per_token_group_quant_fp8 to use correct fp8 min/max values and update atol/rtol in test_quantfp8_group_functionality  â€” rocm,ready â€” by rasmith (åˆå¹¶äº: 2025-12-13 07:41 (UTC+8))
- [#30578](https://github.com/vllm-project/vllm/pull/30578) [ci] Mark PrimeRL integration test as soft fail â€” ci/build â€” by khluu (åˆå¹¶äº: 2025-12-13 06:13 (UTC+8))
- [#30496](https://github.com/vllm-project/vllm/pull/30496) [Refactor] Reduce duplicate code in `per_token_group_quant` cuda kernels â€” ready,nvidia â€” by yewentao256 (åˆå¹¶äº: 2025-12-13 05:45 (UTC+8))
- [#29748](https://github.com/vllm-project/vllm/pull/29748) [MoE-FP8-modelopt] Add FlashInfer alignment padding for intermediate dimensions â€” ready â€” by danielafrimi (åˆå¹¶äº: 2025-12-13 04:42 (UTC+8))
- [#30528](https://github.com/vllm-project/vllm/pull/30528) [Perf] Set split_k to 1 for triton_kernels â€” performance,moe,ready,gpt-oss,nvidia â€” by xyang16 (åˆå¹¶äº: 2025-12-13 03:07 (UTC+8))
- [#29980](https://github.com/vllm-project/vllm/pull/29980) [Fix]Load kv-cache dtype from hf_quant_config.json automatically â€” quantization,ready â€” by danielafrimi (åˆå¹¶äº: 2025-12-13 03:27 (UTC+8))
- [#28848](https://github.com/vllm-project/vllm/pull/28848) [CI/Build] Add x86 CPU wheel release pipeline â€” x86-cpu,ready,ci/build,cpu,aarch64-cpu,nvidia â€” by bigPYJ1151 (åˆå¹¶äº: 2025-12-13 03:21 (UTC+8))
- [#26668](https://github.com/vllm-project/vllm/pull/26668) [ROCm] Enable Triton ScaledMM fallback + kernel selection fix â€” rocm,ready,ci/build,nvidia â€” by shivampr (åˆå¹¶äº: 2025-12-13 02:28 (UTC+8))
- [#30517](https://github.com/vllm-project/vllm/pull/30517) [CI] Fix mypy for vllm/v1/executor â€” ready,v1 â€” by yewentao256 (åˆå¹¶äº: 2025-12-13 02:05 (UTC+8))
- [#30059](https://github.com/vllm-project/vllm/pull/30059) [bugfix] fix bug when top_logprobs=0 with spec decoding â€” ready,v1 â€” by realliujiaxu (åˆå¹¶äº: 2025-12-13 01:03 (UTC+8))
- [#30266](https://github.com/vllm-project/vllm/pull/30266) [Frontend] Fixes anthropic streaming message_start usage nesting â€” frontend,ready â€” by bbartels (åˆå¹¶äº: 2025-12-13 00:28 (UTC+8))
- [#28306](https://github.com/vllm-project/vllm/pull/28306) [Kernel] Support CUDA Graphs in 3D Triton Attention Kernel â€” ready,v1,nvidia â€” by jvlunteren (åˆå¹¶äº: 2025-12-12 23:55 (UTC+8))
- [#30425](https://github.com/vllm-project/vllm/pull/30425) [LMCache] Relax lmcache version requirement â€” ready,ci/build,kv-connector â€” by njhill (åˆå¹¶äº: 2025-12-12 11:18 (UTC+8))
- [#30534](https://github.com/vllm-project/vllm/pull/30534) [Bug] Fix attention_backend arg string parsing â€” bug,ready â€” by mgoin (åˆå¹¶äº: 2025-12-12 23:40 (UTC+8))
- [#30408](https://github.com/vllm-project/vllm/pull/30408) fix(gguf): Disable bfloat16 for GGUF on blackwell device â€” ready â€” by kitaekatt (åˆå¹¶äº: 2025-12-12 23:10 (UTC+8))
- [#30490](https://github.com/vllm-project/vllm/pull/30490) [DeepSeek V3.2] Proper drop_thinking logic â€” ready,deepseek â€” by vladnosiv (åˆå¹¶äº: 2025-12-12 23:01 (UTC+8))
- [#27532](https://github.com/vllm-project/vllm/pull/27532) [Attention] Use sparse prefill kernel for fp8 kv-cache in DeepSeek-v3.2 â€” ready,v1,deepseek,gpt-oss,nvidia,ready-run-all-tests â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-12 21:57 (UTC+8))
- [#28729](https://github.com/vllm-project/vllm/pull/28729) [Bugfix] Multiple fixes for gpt-oss Chat Completion prompting â€” frontend,ready,tool-calling,gpt-oss â€” by bbrowning (åˆå¹¶äº: 2025-12-12 12:59 (UTC+8))
- [#21804](https://github.com/vllm-project/vllm/pull/21804) [Bugfix] Fix CMakeLists Environment Variable â€” ready,ci/build,unstale â€” by wu-kan (åˆå¹¶äº: 2025-12-12 18:54 (UTC+8))
- [#29692](https://github.com/vllm-project/vllm/pull/29692) [Bugfix] Schedule failure due to wrong get_image_size_with_most_features â€” ready,multi-modality,qwen â€” by tomtomjhj (åˆå¹¶äº: 2025-12-12 18:27 (UTC+8))
- [#30291](https://github.com/vllm-project/vllm/pull/30291) [CI/Build][AMD] Fix ref_dynamic_per_token_quant reference implementation on ROCm. â€” rocm,ready â€” by rasmith (åˆå¹¶äº: 2025-12-12 17:30 (UTC+8))
- [#30516](https://github.com/vllm-project/vllm/pull/30516) [compile] Parse compile range cache keys as Range during cache loading. â€” ready â€” by zhxchen17 (åˆå¹¶äº: 2025-12-12 12:30 (UTC+8))
- [#30527](https://github.com/vllm-project/vllm/pull/30527) [ROCm][CI] Skip multi-GPU speculative decoding tests when insufficient GPUs available â€” rocm,ready,v1 â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-12 11:54 (UTC+8))
- [#30272](https://github.com/vllm-project/vllm/pull/30272) [CI/Build] Use spawn subprocess for ROCm â€” documentation,rocm,ready â€” by rjrock (åˆå¹¶äº: 2025-12-12 11:33 (UTC+8))
- [#30505](https://github.com/vllm-project/vllm/pull/30505) [Bugfix][Model] Fix Afmoe rope_parameters issue â€” bug,ready â€” by mgoin (åˆå¹¶äº: 2025-12-12 10:53 (UTC+8))

### å…³é—­ä½†æœªåˆå¹¶çš„ PR
- [#29377](https://github.com/vllm-project/vllm/pull/29377) Draft: DS Arch Eagle3 â€” deepseek â€” by IzzyPutterman (å…³é—­äº: 2025-12-13 04:47 (UTC+8))
- [#30560](https://github.com/vllm-project/vllm/pull/30560) My bad - ignore. â€” documentation,needs-rebase,ci/build â€” by fadara01 (å…³é—­äº: 2025-12-13 00:39 (UTC+8))
- [#30479](https://github.com/vllm-project/vllm/pull/30479) [BugFix] Fix unmap_and_release by tag not done correctly â€” æ— æ ‡ç­¾ â€” by Crispig (å…³é—­äº: 2025-12-13 00:20 (UTC+8))
- [#30366](https://github.com/vllm-project/vllm/pull/30366) [Bug Fix] Fix Kimi-Linear model initialization crash due to missing 'indexer_rotary_emb' arg â€” æ— æ ‡ç­¾ â€” by yonasTMC (å…³é—­äº: 2025-12-12 18:03 (UTC+8))
- [#26924](https://github.com/vllm-project/vllm/pull/26924) [Performance] Optimize encoder cache memory consumption by storing encoder outputs only â€” v1,multi-modality â€” by imkero (å…³é—­äº: 2025-12-12 13:36 (UTC+8))
- [#30446](https://github.com/vllm-project/vllm/pull/30446) Added a test for invalid inputs for parse_raw_prompts â€” æ— æ ‡ç­¾ â€” by mivehk (å…³é—­äº: 2025-12-12 11:49 (UTC+8))
- [#30486](https://github.com/vllm-project/vllm/pull/30486) [BugFix] Fix minimax m2 model partial_rotary_factor â€” æ— æ ‡ç­¾ â€” by rogeryoungh (å…³é—­äº: 2025-12-12 10:57 (UTC+8))