# vLLM å¼€å‘åŠ¨æ€æŠ¥å‘Š - 2025-12-10

> æ—¶é—´çª—å£: 2025-12-10 10:48 (UTC+8) ~ 2025-12-11 10:48 (UTC+8)
> æ•°æ®ç»Ÿè®¡: æ–° Issue 19 | å…³é—­ Issue 30 | æ–° PR 60 | åˆå¹¶ PR 28 | å…³é—­æœªåˆå¹¶ PR 26

---

### ğŸ“Š æ¯æ—¥å¼€å‘çŠ¶æ€æ‘˜è¦
åœ¨æœ¬æ¬¡è§‚å¯Ÿçª—å£å†…ï¼ŒvLLM é¡¹ç›®ä¿æŒäº†æé«˜çš„å¼€å‘æ´»è·ƒåº¦ï¼Œæ–°å¢å’Œåˆå¹¶äº†å¤§é‡ PRï¼ˆ60/28ï¼‰ï¼Œé‡ç‚¹é›†ä¸­äºæ¨¡å‹æ”¯æŒä¼˜åŒ–ã€æ€§èƒ½è°ƒä¼˜åŠæµ‹è¯•åŸºç¡€è®¾æ–½å®Œå–„ã€‚AMD/ROCm ç”Ÿæ€æ”¯æŒæˆä¸ºæ˜¾æ€§ç„¦ç‚¹ï¼Œå¤šä¸ªä¸ ROCm ç›¸å…³çš„ CI ä¿®å¤å’ŒåŠŸèƒ½å¢å¼º PR è¢«æäº¤æˆ–åˆå¹¶ã€‚ç¤¾åŒºè®¨è®ºçƒ­çƒˆï¼Œä¸»è¦å›´ç»•åŸºå‡†æµ‹è¯•å·¥å…·çš„æœªæ¥æ–¹å‘ã€æ–°æ¨¡å‹ï¼ˆå¦‚ DeepSeek-V3.2ï¼Œ Gemma2 GGUFï¼‰çš„æ”¯æŒé—®é¢˜ä»¥åŠ KV ç¼“å­˜æ•°æ®ä¼ è¾“çš„å¯é æ€§å±•å¼€ã€‚

### ğŸ¯ AMD/ROCm ç”Ÿæ€ç›¸å…³åŠ¨æ€
æœ¬å‘¨æœŸ AMD ç”Ÿæ€ç›¸å…³æ´»åŠ¨éå¸¸æ´»è·ƒï¼Œæ¶µç›–äº†é—®é¢˜ä¿®å¤ã€CI ç®¡é“å»ºè®¾å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

1.  **æ ¸å¿ƒé—®é¢˜ä¿®å¤**ï¼š
    *   **PR #30432 ([ROCm] Fix broken import in platform attention backend dispatching)**ï¼šä¿®å¤äº†å› å¯¼å…¥ `get_env_variable_attn_backend` å¯¼è‡´çš„ ROCm å¹³å°åˆå§‹åŒ–å¤±è´¥ã€‚è¯¥é—®é¢˜æºäºå¯¹æ­£åœ¨é‡æ„çš„æ³¨æ„åŠ›åç«¯é€‰æ‹©å™¨ï¼ˆPR #30396ï¼‰çš„ä¾èµ–ï¼Œæœ¬ä¿®å¤é€šè¿‡ç›´æ¥æ£€æŸ¥ç¯å¢ƒå˜é‡æ¥ç»•è¿‡è¯¥ä¾èµ–ï¼Œç¡®ä¿ ROCm å¹³å°æ­£å¸¸å¯åŠ¨ã€‚
    *   **PR #30308 ([bugfix][quantization] fix quark qwen3 kv_cache quantization) (å·²åˆå¹¶)**ï¼šç”± AMD å‘˜å·¥ (`haoyangli-amd`) æäº¤ï¼Œä¿®å¤äº† Quark é‡åŒ–å·¥å…·åœ¨å¤„ç† Qwen3 MoE æ¨¡å‹çš„ KV ç¼“å­˜é‡åŒ–æ—¶ï¼Œå› æœªæ­£ç¡®è°ƒç”¨åŸºç±»æ–¹æ³•è€Œå¯¼è‡´çš„ç¼“å­˜ç¼©æ”¾è¯†åˆ«å¤±è´¥é—®é¢˜ï¼Œç¡®ä¿äº†é‡åŒ–æ¨¡å‹çš„æ­£ç¡®æ€§ã€‚
    *   **PR #30430 ([ROCm][Bugfix] Add MLACommonMetadata to allowed attention types for speculative decoding)**ï¼šä¿®å¤äº†åœ¨ ROCm ä¸Šä½¿ç”¨ MLA (å¦‚ DeepSeek æ¨¡å‹) å¹¶å¯ç”¨æ¨æµ‹è§£ç ï¼ˆ`num_speculative_tokens > 1`ï¼‰æ—¶çš„å…¼å®¹æ€§é—®é¢˜ï¼Œå°† `MLACommonMetadata` æ·»åŠ åˆ°äº†æ”¯æŒåˆ—è¡¨ä¸­ã€‚

2.  **æµ‹è¯•ä¸ CI å¢å¼º**ï¼š
    *   **PR #30417 ([CI/Build][AMD] Skip tests in test_fusions_e2e and test_dbo_dp_ep_gsm8k that require non-existing imports for ROCm)**ï¼šé’ˆå¯¹ ROCm ç¯å¢ƒä¸­ç¼ºå¤±ç‰¹å®š CUDA/DeepEP æ¨¡å—çš„é—®é¢˜ï¼Œè·³è¿‡äº†ç›¸å…³æµ‹è¯•ï¼Œä»¥ä½¿åˆ†å¸ƒå¼æµ‹è¯•ç»„èƒ½å¤Ÿé€šè¿‡ã€‚
    *   **PR #30422 ([ROCm][CI][Bugfix] Fallback for grouped_topk when num_experts canâ€™t be grouped properly)**ï¼šè§£å†³äº† ROCm ä¸Šè¿è¡Œ MTP æ¨æµ‹è§£ç æµ‹è¯•æ—¶ï¼Œå› ä¸“å®¶æ•°ä¸ç¬¦åˆåˆ†ç»„æ¡ä»¶è€Œå¼•å‘çš„ `grouped_topk` å†…æ ¸é”™è¯¯ï¼Œå¢åŠ äº†å›é€€æœºåˆ¶ã€‚
    *   **PR #29358 ([ROCm][CI] Attempt to fix the failures under a subgroup of the e2e the test group) (å·²åˆå¹¶)**ï¼šè°ƒæ•´äº†å¼‚æ­¥è°ƒåº¦æµ‹è¯•åœ¨ ROCm ä¸Šçš„é…ç½®ï¼Œç¡®ä¿ä½¿ç”¨æ”¯æŒçš„æ³¨æ„åŠ›åç«¯ï¼ˆ`TRITON_ATTN`ï¼‰å¹¶æ”¾å®½äº†æ•°å€¼å®¹å·®ï¼Œä»¥ä½¿æµ‹è¯•é€šè¿‡ã€‚

3.  **å‘å¸ƒä¸éƒ¨ç½²ç®¡é“å»ºè®¾**ï¼š
    *   **PR #30395 ([ROCm] [CI] [Release] Add rocm wheel release pipeline)**ï¼šè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„åŸºç¡€è®¾æ–½ PRï¼Œæ—¨åœ¨ä¸º ROCm å¹³å°å»ºç«‹é¢„ç¼–è¯‘ wheel åŒ…çš„å‘å¸ƒç®¡é“ã€‚è¯¥è®¾è®¡åŒ…æ‹¬ä¾èµ–æ„å»ºç¼“å­˜ã€S3 å­˜å‚¨ä¸Šä¼ å’Œæ‰‹åŠ¨è§¦å‘æœºåˆ¶ï¼Œç›®æ ‡æ˜¯é™ä½ç”¨æˆ·åœ¨ AMD æœºå™¨ä¸Šçš„ä½¿ç”¨é—¨æ§›ã€‚

**æ€»ç»“**ï¼šæœ¬å‘¨æœŸ AMD ç›¸å…³çš„æ´»åŠ¨ä»ä¿®å¤ç´§æ€¥è¿è¡Œæ—¶é”™è¯¯ï¼Œæ‰©å±•åˆ°å®Œå–„æµ‹è¯•è¦†ç›–ç‡å’Œæ„å»ºé•¿æœŸå¯æŒç»­çš„äº¤ä»˜ç®¡é“ï¼ˆé¢„ç¼–è¯‘åŒ…ï¼‰ï¼Œæ˜¾ç¤ºå‡ºå¯¹ AMD å¹³å°æ”¯æŒæ­£ä»â€œåŠŸèƒ½å¯ç”¨â€å‘â€œä½“éªŒä¼˜åŒ–â€å’Œâ€œç”Ÿæ€å»ºè®¾â€é˜¶æ®µè¿ˆè¿›ã€‚

### ğŸ’¬ é«˜çƒ­åº¦è®¨è®ºåˆ†æ

1.  **Issue #30383: [RFC]: Multi-Process Benchmark Architecture for Scaling Beyond Single-Core Limits**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šç”¨æˆ·æŒ‡å‡ºå½“å‰ `vllm benchmark` å·¥å…·çš„å•è¿›ç¨‹è®¾è®¡åœ¨é«˜å‹ä¸‹ï¼ˆé«˜ QPSã€é«˜å¹¶å‘ï¼‰ä¼šæˆä¸ºç“¶é¢ˆï¼Œå¯¼è‡´æ€§èƒ½æŒ‡æ ‡å¤±çœŸï¼Œå¹¶æå‡ºäº†æ„å»ºå¤šè¿›ç¨‹æ¶æ„çš„ææ¡ˆã€‚
    *   **è§‚ç‚¹ä¸ç«‹åœº**ï¼š
        *   **ææ¡ˆè€… (`GaoHuaZhang`)**ï¼šé€šè¿‡è¯¦ç»†çš„æµ‹è¯•æ•°æ®è®ºè¯äº†å•è¿›ç¨‹é™åˆ¶çš„ä¸¥é‡æ€§ï¼Œå¼ºè°ƒéœ€è¦ä¸€ä¸ªèƒ½åæ˜ çœŸå®é«˜å¹¶å‘åœºæ™¯çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚
        *   **æ”¯æŒè€… (`wenba0`)**ï¼šè®¤ä¸ºè¯¥åŠŸèƒ½è‡³å…³é‡è¦ï¼Œæ„¿æ„æä¾›å¸®åŠ©ã€‚
        *   **æ ¸å¿ƒç»´æŠ¤è€… (`ywang96`)**ï¼šæä¾›äº†é‡è¦ä¸Šä¸‹æ–‡ï¼ŒæŒ‡å‡º `vllm bench serve` æœ€åˆè®¾è®¡ç”¨äºå—æ§çš„å•å®¹å™¨ç¯å¢ƒï¼Œå¹¶å¼•å¯¼ç¤¾åŒºå…³æ³¨ç°æœ‰çš„ **guidellm** å­é¡¹ç›®ï¼Œè¯¥å­é¡¹ç›®ä¸“ä¸ºå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•è®¾è®¡ã€‚å»ºè®®ä¸ guidellm å¼€å‘è€…åä½œï¼Œé¿å…ç»´æŠ¤é‡å¤å·¥å…·ï¼Œå¹¶è®¡åˆ’åœ¨æœªæ¥æ›´å¥½åœ°é›†æˆå’Œæ¨å¹¿ guidellmã€‚
    *   **äº‰è®®ç„¦ç‚¹**ï¼šæ— å®è´¨æ€§äº‰è®®ï¼Œä½†æ­ç¤ºäº†é¡¹ç›®å†…éƒ¨åˆ†å·¥å’Œå·¥å…·å®šä½çš„æˆ˜ç•¥è€ƒé‡ã€‚æ ¸å¿ƒå›¢é˜Ÿæ›´å€¾å‘äºé›†ä¸­åŠ›é‡å‘å±•ä¸€ä¸ªä¸“é—¨çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é¡¹ç›®ï¼ˆguidellmï¼‰ï¼Œè€Œéæ‰©å±•ç°æœ‰å·¥å…·çš„é€‚ç”¨èŒƒå›´ã€‚
    *   **å½“å‰çŠ¶æ€**ï¼šè®¨è®ºå¼€æ”¾ï¼Œç»´æŠ¤è€…çš„å›å¤ä¸ºæœªæ¥çš„å¼€å‘æ–¹å‘æä¾›äº†æ˜ç¡®çš„æŒ‡å¼•ã€‚

2.  **Issue #30445: [Bug]: QuantTrio/MiniMax-M2-AWQ produces garbage in 12/10/2025 build**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šç”¨æˆ·åœ¨å½“æ—¥æ„å»ºç‰ˆæœ¬ä¸­è¿è¡Œç‰¹å®š AWQ é‡åŒ–æ¨¡å‹æ—¶å¾—åˆ°ä¹±ç è¾“å‡ºï¼Œè€Œå›é€€åˆ°å‰ä¸€æ—¥æ„å»ºåˆ™æ­£å¸¸ï¼Œæš—ç¤ºå½“æ—¥åˆå…¥çš„æŸä¸ªæ›´æ”¹å¼•å…¥äº†å›å½’ã€‚
    *   **è§‚ç‚¹ä¸ç«‹åœº**ï¼š
        *   **æŠ¥å‘Šè€… (`eugr`)**ï¼šæä¾›äº†è¯¦å°½çš„ç¯å¢ƒä¿¡æ¯å’Œå¤ç°æ­¥éª¤ï¼Œå¹¶ä¸»åŠ¨æµ‹è¯•æ’é™¤äº† `fastsafetensors` çš„å½±å“ã€‚
    *   **äº‰è®®ç„¦ç‚¹**ï¼šæ— ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„å›å½’é—®é¢˜æŠ¥å‘Šï¼Œå…³é”®åœ¨äºå®šä½æ˜¯å“ªä¸ªå…·ä½“çš„ PR æˆ–ä»£ç å˜æ›´å¯¼è‡´äº†è¯¥é—®é¢˜ã€‚
    *   **å½“å‰çŠ¶æ€**ï¼šé—®é¢˜å¼€æ”¾ï¼Œç­‰å¾…å¼€å‘äººå‘˜æ ¹æ®æ„å»ºæ—¶é—´çª—å£æ’æŸ¥å¼•å…¥é—®é¢˜çš„æäº¤ã€‚

3.  **Closed Issue #11247: [Bug]: disaggregated prefilling hangs when TP=2 (å·²å…³é—­)**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šè¿™æ˜¯ä¸€ä¸ªå­˜åœ¨äº†è¿‘ä¸€å¹´çš„è€é—®é¢˜ï¼Œå…³äºåœ¨å¼ é‡å¹¶è¡Œåº¦ä¸º 2 çš„åˆ†å¸ƒå¼é¢„å¡«å……/è§£ç ï¼ˆP/Dï¼‰æ¶æ„ä¸‹ï¼ŒæœåŠ¡ä¼šå‡ºç°æŒ‚èµ·ã€‚
    *   **è§‚ç‚¹ä¸ç«‹åœº**ï¼š
        *   **ä¼—å¤šé‡åˆ°ç›¸åŒé—®é¢˜çš„ç”¨æˆ·**ï¼šæŒç»­åœ¨è¯„è®ºåŒºåé¦ˆä»–ä»¬é‡åˆ°äº†ç›¸åŒæˆ–ç±»ä¼¼çš„é—®é¢˜ï¼Œå¹¶å°è¯•åˆ†äº«å„è‡ªçš„è§£å†³æ–¹æ¡ˆï¼ˆå¦‚è®¾ç½®ç¯å¢ƒå˜é‡ `CUDA_LAUNCH_BLOCKING=1`ã€è°ƒæ•´ `--max-num-seqs` å‚æ•°ç­‰ï¼‰ã€‚
        *   **åˆ†æè€… (`yjsunn`)**ï¼šæä¾›äº†ä¸€ä¸ªæ·±å…¥çš„åˆ†æï¼Œè®¤ä¸ºåœ¨é«˜ QPS å’Œå° `kv_buffer_size` æ—¶ï¼Œé¢„å¡«å……æ‰¹æ¬¡è¿‡å¤§å¯èƒ½å¯¼è‡´ KV ç¼“å­˜ä¼ è¾“éƒ¨åˆ†å¤±è´¥ï¼Œè¿›è€Œé˜»å¡æ•´ä¸ªæ‰¹æ¬¡ä¸­å·²æˆåŠŸçš„è¯·æ±‚ã€‚
    *   **äº‰è®®ç„¦ç‚¹**ï¼šæ— äº‰è®®ï¼Œæ›´å¤šæ˜¯ç”¨æˆ·äº’åŠ©å’Œé—®é¢˜ç°è±¡æ”¶é›†ã€‚
    *   **æœ€ç»ˆç»“è®º**ï¼šè¯¥ Issue å› è¶…è¿‡ 90 å¤©æ— æ ¸å¿ƒå¼€å‘äººå‘˜è·Ÿè¿›è€Œè¢«æ ‡è®°ä¸º `stale` å¹¶æœ€ç»ˆå…³é—­ã€‚ä½†å…³é—­ä¸ä»£è¡¨é—®é¢˜å·²è§£å†³ï¼Œç”¨æˆ·è®¨è®ºåŒºè¡¨æ˜è¿™ä»æ˜¯ä¸€ä¸ªå½±å“ç”Ÿäº§éƒ¨ç½²çš„ç—›ç‚¹ã€‚

### ğŸ”¥ çƒ­é—¨è¯é¢˜ä¸è¶‹åŠ¿åˆ†æ

1.  **æ¨¡å‹æ”¯æŒä¸å…¼å®¹æ€§**ï¼šè¿™æ˜¯ Issue äº§ç”Ÿçš„æœ€å¤§æ¥æºã€‚
    *   **æ–°æ¨¡å‹æ¶æ„**ï¼šDeepSeek-V3.2 (#30311, #30343, #30371)ã€Qwen3 ç³»åˆ— (#30378, #30387, #30433)ã€Gemma2 GGUF (#30404, #30411, #30424 ç­‰) çš„æ”¯æŒæ˜¯å½“å‰çƒ­ç‚¹ï¼Œé—®é¢˜é›†ä¸­åœ¨ tokenizer é›†æˆã€å·¥å…·è°ƒç”¨è§£æã€æ¨ç†è¾“å‡ºå¼‚å¸¸å’Œ GGUF æ ¼å¼åŠ è½½çš„å„ç±»ç»†èŠ‚ä¸Šã€‚
    *   **é‡åŒ–æ¨¡å‹**ï¼šå…³äº AWQ (#30445)ã€Quark (#30308)ã€FP8 KV ç¼“å­˜ (#30387) ç­‰é‡åŒ–æ¨¡å‹åœ¨å®é™…ä½¿ç”¨ä¸­å‡ºç°çš„è¾“å‡ºé”™è¯¯æˆ–å´©æºƒé—®é¢˜æŠ¥å‘Šå¢å¤šï¼Œè¡¨æ˜é‡åŒ–éƒ¨ç½²çš„å¤æ‚æ€§å’Œå¯¹ç¨³å®šæ€§çš„é«˜è¦æ±‚ã€‚

2.  **CI/CD ä¸æµ‹è¯•åŸºç¡€è®¾æ–½**ï¼šPR æ´»åŠ¨é«˜åº¦é›†ä¸­äºæ­¤ã€‚
    *   **AMD CI æ”»åš**ï¼šå¤§é‡ PR (#30417, #30422, #30432, #29358 ç­‰) ä¸“æ³¨äºä¿®å¤ ROCm å¹³å°ä¸Šçš„æµ‹è¯•å¤±è´¥ï¼Œæ¶‰åŠè·³è¿‡ä¸æ”¯æŒçš„æµ‹è¯•ã€ä¿®å¤å†…æ ¸å…¼å®¹æ€§ã€è°ƒæ•´æµ‹è¯•å‚æ•°ç­‰ï¼Œç›®æ ‡æ˜¯ä½¿ AMD CI æµæ°´çº¿å…¨é¢å˜ç»¿ã€‚
    *   **æµ‹è¯•ä¼˜åŒ–ä¸æ–°å¢**ï¼šä¸º CPU åç«¯ (#30347)ã€Whisper (#30072) ç­‰æ–°å¢æˆ–ä¼˜åŒ–æµ‹è¯•ï¼Œå¹¶ä¿®å¤æµ‹è¯•ä¸­çš„å¯¼å…¥é”™è¯¯ (#30476) å’Œç¯å¢ƒä¾èµ–é—®é¢˜ã€‚

3.  **æ€§èƒ½ä¼˜åŒ–ä¸æ ¸å¿ƒç‰¹æ€§**ï¼š
    *   **KV ç¼“å­˜ä¸æ•°æ®ä¼ è¾“**ï¼šå¼‚æ„ KV å¸ƒå±€ (#30448)ã€NIXL è¿æ¥å™¨ä¿®å¤ (#30419, #30420)ã€å…±äº«å†…å­˜å±éšœ (#30407) ç­‰ PR æ˜¾ç¤ºï¼Œåœ¨åˆ†å¸ƒå¼å’Œ P/D åœºæ™¯ä¸‹ï¼ŒKV ç¼“å­˜çš„ç®¡ç†å’Œæ•°æ®ä¼ è¾“çš„å¯é æ€§ã€æ€§èƒ½æ˜¯æŒç»­ä¼˜åŒ–çš„é‡ç‚¹ã€‚
    *   **ç¼–è¯‘ä¸å›¾å½¢åŒ–**ï¼šæ”¯æŒ Whisper æ¨¡å‹çš„ CUDA Graph (#30072) å’Œ `torch.compile` (#30385)ï¼Œä»¥åŠå¯¹ç¼–è¯‘ç¼“å­˜ç«äº‰é—®é¢˜çš„è®¨è®º (#24601)ï¼Œåæ˜ äº†å¯¹æ¨ç†å»¶è¿Ÿå’Œååé‡çš„æè‡´è¿½æ±‚ã€‚

4.  **æ–‡æ¡£ä¸ç”¨æˆ·ä½“éªŒ**ï¼šå¤šä¸ª PR è‡´åŠ›äºæ”¹å–„æ–‡æ¡£ï¼ŒåŒ…æ‹¬ç”Ÿæˆå®Œæ•´çš„ç›‘æ§æŒ‡æ ‡åˆ—è¡¨ (#30388)ã€æ›´æ–°ä¸“å®¶å¹¶è¡Œéƒ¨ç½²æŒ‡å— (#27933)ã€æ·»åŠ  CPU wheel å®‰è£…è¯´æ˜ (#30402) ç­‰ï¼Œæ˜¾ç¤ºé¡¹ç›®åœ¨å¿«é€Ÿå‘å±•çš„åŒæ—¶ï¼Œä¹Ÿå¼€å§‹ç³»ç»ŸåŒ–åœ°æå‡ç”¨æˆ·ä½“éªŒã€‚

### ğŸ› ï¸ é‡ç‚¹æŠ€æœ¯å˜æ›´

1.  **PR #30448: [NIXL] Heterogeneous KV Layout and block_size - prefill NHD and nP > nD support**ï¼šè¿™æ˜¯ä¸€ä¸ªå…³é”®çš„æ€§èƒ½ä¼˜åŒ– PRï¼Œæ”¯æŒåœ¨é¢„å¡«å……é˜¶æ®µä½¿ç”¨ä¸€ç§ KV å¸ƒå±€ï¼ˆNHDï¼‰å’Œå—å¤§å°ï¼Œåœ¨å¡«å……å®ŒæˆååŠ¨æ€è½¬æ¢ä¸ºè§£ç é˜¶æ®µæ‰€éœ€çš„å¸ƒå±€ï¼ˆHNDï¼‰å’Œå—å¤§å°ã€‚è¿™å…è®¸ä¸ºä¸åŒé˜¶æ®µé€‰æ‹©æœ€ä¼˜çš„å†…å­˜è®¿é—®æ¨¡å¼ï¼Œå°¤å…¶æœ‰åˆ©äºæå‡è§£ç æ€§èƒ½ã€‚å…¶å®ç°ä¾èµ–äºå‰ç¼€ç¼“å­˜è¢«ç¦ç”¨ã€‚

2.  **PR #30385: [Core] Whisper support `torch.compile`**ï¼šç»§æ”¯æŒ CUDA Graph åï¼Œæ­¤ PR è¿›ä¸€æ­¥ä¸º Whisper ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å¯ç”¨äº† `torch.compile` æ”¯æŒã€‚ç”±äºç¬¬ä¸€æ­¥è§£ç éœ€è¦å¤„ç†ç¼–ç å™¨è¾“å‡ºï¼Œå®ƒå·§å¦™åœ°é‡‡ç”¨äº†ä»…ç¼–è¯‘ç¬¬äºŒæ­¥åŠä»¥åæ­¥éª¤çš„ç­–ç•¥ï¼Œä»è€Œæ˜¾è‘—æå‡äº† Whisper çš„æ¨ç†é€Ÿåº¦ã€‚

3.  **PR #30440: [fix] Fix qwen3_coder tool call per parameter streaming**ï¼šä¿®å¤äº† Qwen3 Coder æ¨¡å‹å·¥å…·è°ƒç”¨å‚æ•°åœ¨æµå¼ä¼ è¾“æ—¶ï¼Œå‚æ•°å†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰ä¸èƒ½æŒ‰ token å®æ—¶æµå‡ºçš„é—®é¢˜ã€‚è¿™æå‡äº†å·¥å…·è°ƒç”¨åœºæ™¯ä¸‹çš„ç”¨æˆ·ä½“éªŒï¼Œç¡®ä¿äº†æµå¼å“åº”ä¸­å‚æ•°çš„å®æ—¶æ€§ã€‚

4.  **Issue #30343 -> PR #30351 (å·²åˆå¹¶): DeepSeek-V3.2 tokenizer æ€§èƒ½é—®é¢˜**ï¼šå‘ç°å¹¶ä¿®å¤äº†ä¸€ä¸ªå¯¼è‡´æœåŠ¡æŒ‚èµ·çš„ä¸¥é‡æ€§èƒ½é—®é¢˜ã€‚æ ¹æœ¬åŸå› æ˜¯ DeepSeekV32Tokenizer çš„ `__len__` æ–¹æ³•åœ¨æ¯ä¸ª token è§£ç æ—¶éƒ½ä¼šè°ƒç”¨æ˜‚è´µçš„ `get_added_vocab()`ï¼Œé˜»å¡äº†ä¸»çº¿ç¨‹ã€‚ä¿®å¤æ–¹æ³•æ˜¯åœ¨åˆå§‹åŒ–æ—¶ç¼“å­˜ `added_vocab` çš„å¤§å°ï¼Œæ¶ˆé™¤äº†æ¯ä»¤ç‰Œçš„å¼€é”€ã€‚

5.  **PR #30407 (å·²åˆå¹¶): fix(shm): Add memory barriers for cross-process shared memory visibility**ï¼šé€šè¿‡ä¸ºè·¨è¿›ç¨‹å…±äº«å†…å­˜å¹¿æ’­æ·»åŠ å†…å­˜å±éšœï¼Œä¿®å¤äº†æ½œåœ¨çš„æ•°æ®ç«äº‰é—®é¢˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæäº¤è€…åé¦ˆè¯¥ä¿®å¤ç”šè‡³åœ¨ä½å»¶è¿Ÿåœºæ™¯ä¸‹å¸¦æ¥äº†è½»å¾®çš„æ€§èƒ½æå‡ã€‚

### ğŸ“ˆ å¼€å‘æ´»è·ƒåº¦è§‚å¯Ÿ
*   **è´¡çŒ®è€…æ´»è·ƒ**ï¼šåœ¨æ–°å¢çš„ 60 ä¸ª PR ä¸­ï¼Œé™¤äº†æ ¸å¿ƒå›¢é˜Ÿæˆå‘˜ï¼ˆå¦‚ `LucasWilkinson`, `DarkLight1337`, `AndreasKaratzas`ï¼‰ï¼Œä¹Ÿå‡ºç°äº†å¤§é‡ç¤¾åŒºè´¡çŒ®è€…ï¼ˆå¦‚ `kitaekatt`, `anker-c2`, `rogeryoungh`ï¼‰ï¼Œä»–ä»¬ç§¯æä¿®å¤æ¨¡å‹æ”¯æŒã€æ–‡æ¡£å’Œå„ç±» Bugã€‚
*   **AMD å›¢é˜Ÿæ·±åº¦å‚ä¸**ï¼š`AndreasKaratzas`, `micah-wil`, `haoyangli-amd` ç­‰ï¼ˆåŒ…å«æˆ–å…³è” AMDï¼‰è´¡çŒ®è€…éå¸¸æ´»è·ƒï¼Œä¸»å¯¼äº† ROCm å¹³å°çš„å¤§éƒ¨åˆ†ä¿®å¤å’Œæµ‹è¯•å·¥ä½œï¼Œä½“ç°äº† AMD å¯¹ vLLM ç”Ÿæ€æŠ•å…¥çš„èµ„æºã€‚
*   **ä»£ç å®¡æŸ¥ä¸åˆå¹¶é€Ÿåº¦**ï¼š28 ä¸ª PR åœ¨è§‚å¯ŸæœŸå†…è¢«åˆå¹¶ï¼Œå…¶ä¸­è®¸å¤šæ˜¯ä¿®å¤å…³é”®é—®é¢˜çš„ PRï¼Œè¡¨æ˜æ ¸å¿ƒå›¢é˜Ÿå¯¹é‡è¦ä¿®å¤çš„å®¡æŸ¥å’Œåˆå¹¶æ•ˆç‡å¾ˆé«˜ã€‚åŒæ—¶ï¼Œä¹Ÿæœ‰åƒ `#30395` (ROCm wheel å‘å¸ƒç®¡é“) è¿™æ ·çš„å¤§å‹åŸºç¡€è®¾æ–½ PR ä»åœ¨è®¨è®ºå’Œæ¨è¿›ä¸­ã€‚

### ğŸ’¡ å€¼å¾—å…³æ³¨çš„é—®é¢˜
1.  **åŸºå‡†æµ‹è¯•å·¥å…·çš„æˆ˜ç•¥æ–¹å‘**ï¼ˆIssue #30383ï¼‰ï¼šç¤¾åŒºæå‡ºçš„å¢å¼ºéœ€æ±‚ä¸æ ¸å¿ƒå›¢é˜Ÿè§„åˆ’çš„ **guidellm** å­é¡¹ç›®å­˜åœ¨é‡å ã€‚å¦‚ä½•æ¸…æ™°å®šä¹‰ `vllm bench` ä¸ `guidellm` çš„è¾¹ç•Œï¼Œå¹¶å¹³æ»‘å¼•å¯¼ç”¨æˆ·å’Œå¼€å‘è€…ï¼Œéœ€è¦æ˜ç¡®çš„æ²Ÿé€šå’Œè·¯çº¿å›¾ã€‚
2.  **GGUF æ ¼å¼æ”¯æŒçš„æˆç†Ÿåº¦**ï¼šè¿‘æœŸæ¶Œç°äº†å¤§é‡ Gemma2 ç­‰æ¨¡å‹ GGUF æ ¼å¼åŠ è½½çš„é—®é¢˜ï¼ˆå¦‚æ•°æ®ç±»å‹å†²çªã€ç¼ºå¤±é…ç½®é¡¹ã€æƒé‡æ˜ å°„é”™è¯¯ç­‰ï¼‰ã€‚è¿™è¡¨æ˜ vLLM çš„ GGUF åŠ è½½å™¨åœ¨é¢å¯¹å¤šæ ·åŒ–çš„æ¨¡å‹æ¶æ„å’Œé‡åŒ–æ–¹æ¡ˆæ—¶ï¼Œä»éœ€åŠ å¼ºå¥å£®æ€§å’Œå…¼å®¹æ€§ã€‚
3.  **åˆ†å¸ƒå¼ P/D éƒ¨ç½²çš„ç¨³å®šæ€§**ï¼šå°½ç®¡è€ Issue #11247 å·²å…³é—­ï¼Œä½†ç”¨æˆ·åé¦ˆè¡¨æ˜ï¼Œåœ¨ disaggregated prefilling åœºæ™¯ä¸‹çš„æŒ‚èµ·é—®é¢˜ä¾ç„¶å½±å“éƒ¨åˆ†ç”¨æˆ·ã€‚è¿™ä»ç„¶æ˜¯é«˜å¹¶å‘ã€åˆ†å¸ƒå¼ç”Ÿäº§éƒ¨ç½²ä¸­çš„ä¸€ä¸ªé£é™©ç‚¹ã€‚
4.  **é‡åŒ–æ¨¡å‹æ¨ç†çš„å¯é æ€§**ï¼šå¤šä¸ª Issue è¡¨æ˜ï¼Œä¸åŒç±»å‹çš„é‡åŒ–æ¨¡å‹ï¼ˆAWQ, Quark, FP8 KV Cacheï¼‰åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä¼šäº§ç”Ÿé”™è¯¯è¾“å‡ºæˆ–å´©æºƒã€‚ç¡®ä¿é‡åŒ–æ¨ç†çš„æ•°å€¼ç¨³å®šæ€§å’Œæ­£ç¡®æ€§æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ã€‚

---

## ğŸ“‹ é™„å½•ï¼šè¯¦ç»†æ•°æ®åˆ—è¡¨

### æ–°å¢ Issue
- [#30383](https://github.com/vllm-project/vllm/issues/30383) [RFC]: Multi-Process Benchmark Architecture for Scaling Beyond Single-Core Limits â€” RFC â€” by GaoHuaZhang (åˆ›å»ºäº: 2025-12-10 18:02 (UTC+8))
- [#30447](https://github.com/vllm-project/vllm/issues/30447) [Usage]: how to load kv cache data into local file â€” usage â€” by chx725 (åˆ›å»ºäº: 2025-12-11 09:43 (UTC+8))
- [#30445](https://github.com/vllm-project/vllm/issues/30445) [Bug]: QuantTrio/MiniMax-M2-AWQ produces garbage in 12/10/2025 build â€” bug â€” by eugr (åˆ›å»ºäº: 2025-12-11 09:06 (UTC+8))
- [#30441](https://github.com/vllm-project/vllm/issues/30441) [Usage]: vllm serve setup issues on B300 â€” usage â€” by navmarri14 (åˆ›å»ºäº: 2025-12-11 07:50 (UTC+8))
- [#30439](https://github.com/vllm-project/vllm/issues/30439) [Bug]: Qwen3 Coder parser does not stream tool call arguments â€” bug â€” by koush (åˆ›å»ºäº: 2025-12-11 07:45 (UTC+8))
- [#30436](https://github.com/vllm-project/vllm/issues/30436) [Bug]: Speculative decode crashes on PP>1 because self.drafter missing â€” bug â€” by kvcop (åˆ›å»ºäº: 2025-12-11 07:26 (UTC+8))
- [#30435](https://github.com/vllm-project/vllm/issues/30435) [Bug]: vllm problem with cu130 â€” bug â€” by Reviveplugins (åˆ›å»ºäº: 2025-12-11 07:24 (UTC+8))
- [#30387](https://github.com/vllm-project/vllm/issues/30387) [Bug]: illegal memory access countered when using kv-cache-type=fp8  loading  a weight-fp8 model for evaltest  in flash-attn backend â€” bug â€” by youngze0016 (åˆ›å»ºäº: 2025-12-10 19:43 (UTC+8))
- [#30401](https://github.com/vllm-project/vllm/issues/30401) [Bug]: EAGLE3 failure with gpt-oss 20b and 120b â€” bug â€” by Mhdaw (åˆ›å»ºäº: 2025-12-11 00:42 (UTC+8))
- [#30394](https://github.com/vllm-project/vllm/issues/30394) [Feature]: Prometheus Metrics Abstraction â€” feature request â€” by mladjan-gadzic (åˆ›å»ºäº: 2025-12-10 22:22 (UTC+8))
- [#30392](https://github.com/vllm-project/vllm/issues/30392) [Usage]: Docker image v0.12.0 Fail to run inference via Docker image â€” usage â€” by kuopching (åˆ›å»ºäº: 2025-12-10 21:43 (UTC+8))
- [#30380](https://github.com/vllm-project/vllm/issues/30380) [Usage]: å¤§å®¶ä¸€èˆ¬æ€ä¹ˆä½¿ç”¨vllm/testsçš„ï¼Ÿ â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:27 (UTC+8))
- [#30382](https://github.com/vllm-project/vllm/issues/30382) [Bug]: Issues with mistralai/Ministral-3-14B-Instruct-2512 â€” bug â€” by eltorre (åˆ›å»ºäº: 2025-12-10 17:55 (UTC+8))
- [#30374](https://github.com/vllm-project/vllm/issues/30374) [Feature][CPU Backend]: Add Paged Attention Benchmarks for CPU backend â€” feature request,cpu â€” by fadara01 (åˆ›å»ºäº: 2025-12-10 15:53 (UTC+8))
- [#30381](https://github.com/vllm-project/vllm/issues/30381) [Usage]: â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:27 (UTC+8))
- [#30379](https://github.com/vllm-project/vllm/issues/30379) [Usage]: how to use vllm/tests/ï¼Ÿ â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:25 (UTC+8))
- [#30378](https://github.com/vllm-project/vllm/issues/30378) [Feature]: Automatically infer Qwen3 reranker settings (remove need for hf_overrides) â€” feature request â€” by ilopezluna (åˆ›å»ºäº: 2025-12-10 17:21 (UTC+8))
- [#30375](https://github.com/vllm-project/vllm/issues/30375) [Bug]: [TPU] ShapeDtypeStruct error when loading custom safetensors checkpoint on TPU v5litepod â€” bug â€” by Baltsat (åˆ›å»ºäº: 2025-12-10 16:12 (UTC+8))
- [#30372](https://github.com/vllm-project/vllm/issues/30372) [Bug]: vLLM (GPT-OSS) causes distorted tool argument names + infinite tool-call loop with Korean messenger tool â€” bug â€” by minmini2 (åˆ›å»ºäº: 2025-12-10 14:59 (UTC+8))

### å·²å…³é—­ Issue
- [#11247](https://github.com/vllm-project/vllm/issues/11247) [Bug]: disaggregated prefilling hangs when TP=2  â€” bug,stale â€” by Louis-99 (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#13040](https://github.com/vllm-project/vllm/issues/13040) [Bug]: torchvision.libs/libcudart.41118559.so.12 (deleted): cannot open shared object file: No such file or directory â€” bug,stale â€” by wuyifan18 (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#15513](https://github.com/vllm-project/vllm/issues/15513) [Bug]: â€” bug,stale â€” by znicelya (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#16545](https://github.com/vllm-project/vllm/issues/16545) [Bug]: Unexpected behavior of `returned_token_ids` in Reward Modeling for LlamaForCausalLM â€” bug,stale â€” by ryokamoi (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#17292](https://github.com/vllm-project/vllm/issues/17292) [Feature]: support reasoning output when offline batched inference â€” feature request,stale â€” by wa008 (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#17327](https://github.com/vllm-project/vllm/issues/17327) [Usage] Qwen3 Usage Guide â€” usage,stale â€” by simon-mo (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#17634](https://github.com/vllm-project/vllm/issues/17634) [Bug]: When using the LLaMA-Factory framework with InternVL3-8B-hf for batch inference, vLLM throws an error: ValueError: limit_mm_per_prompt is only supported for multimodal models. â€” bug,stale â€” by Fangyuan-Liu (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#17697](https://github.com/vllm-project/vllm/issues/17697) [Feature]: Addition of pre-built AMD wheel packages â€” feature request,stale â€” by Epliz (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#22470](https://github.com/vllm-project/vllm/issues/22470) [Bug]: gpt oss 20/120b generates wired characters and fails later when i use them â€” bug,stale â€” by ShlomoSMR (å…³é—­äº: 2025-12-11 10:17 (UTC+8))
- [#29453](https://github.com/vllm-project/vllm/issues/29453) [CI Failure]: mi325_1: Basic Correctness Test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:21 (UTC+8))
- [#29464](https://github.com/vllm-project/vllm/issues/29464) [CI Failure]: mi325_1: OpenAI API correctness â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:21 (UTC+8))
- [#29803](https://github.com/vllm-project/vllm/issues/29803) [CI Failure]: mi325_1: Cudagraph test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:20 (UTC+8))
- [#29465](https://github.com/vllm-project/vllm/issues/29465) [CI Failure]: mi325_2: Prime-RL Integration Test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:18 (UTC+8))
- [#29526](https://github.com/vllm-project/vllm/issues/29526) [CI Failure]: mi325_1: Entrypoints Integration Test (Pooling) â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:17 (UTC+8))
- [#29514](https://github.com/vllm-project/vllm/issues/29514) [CI Failure]: mi325_4: EPLB Execution Test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:17 (UTC+8))
- [#29443](https://github.com/vllm-project/vllm/issues/29443) [CI Failure]: mi325_1: Python-only Installation Test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:16 (UTC+8))
- [#29537](https://github.com/vllm-project/vllm/issues/29537) [CI Failure]: mi325_2: Weight Loading Multiple GPU Test - Large Models â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:09 (UTC+8))
- [#29534](https://github.com/vllm-project/vllm/issues/29534) [CI Failure]: mi325_8: LoRA Test %N â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:01 (UTC+8))
- [#14083](https://github.com/vllm-project/vllm/issues/14083) [Feature]: Improve Logging for Error Messages â€” help wanted,good first issue,feature request,unstale â€” by robertgshaw2-redhat (å…³é—­äº: 2025-12-11 07:17 (UTC+8))
- [#30214](https://github.com/vllm-project/vllm/issues/30214) [Bug]: DeepSeek V3.2 on B200 fails with "CUTLASS_MLA is not valid... Reason: ['sparse not supported']" â€” bug â€” by hadikoub (å…³é—­äº: 2025-12-11 04:20 (UTC+8))
- [#30240](https://github.com/vllm-project/vllm/issues/30240) [Bug]: Lots of "Current vLLM config is not set." warnings when FlashInfer attention is used â€” bug â€” by nvpohanh (å…³é—­äº: 2025-12-11 03:18 (UTC+8))
- [#24601](https://github.com/vllm-project/vllm/issues/24601) [Bug]: Launching multiple vLLM processes at the same time doesn't work well with vLLM's compile cache â€” bug,torch.compile â€” by zou3519 (å…³é—­äº: 2025-12-11 02:53 (UTC+8))
- [#30342](https://github.com/vllm-project/vllm/issues/30342) [Bug]: HunyuanOCR batching problem with variable sized images in a batch. â€” bug â€” by anker-c2 (å…³é—­äº: 2025-12-11 02:09 (UTC+8))
- [#15636](https://github.com/vllm-project/vllm/issues/15636) [Bug]: Outlines broken on vLLM 0.8+ â€” bug,structured-output,unstale â€” by cpfiffer (å…³é—­äº: 2025-12-10 21:18 (UTC+8))
- [#30382](https://github.com/vllm-project/vllm/issues/30382) [Bug]: Issues with mistralai/Ministral-3-14B-Instruct-2512 â€” bug â€” by eltorre (å…³é—­äº: 2025-12-10 18:47 (UTC+8))
- [#30381](https://github.com/vllm-project/vllm/issues/30381) [Usage]: â€” usage â€” by tobeprozy (å…³é—­äº: 2025-12-10 17:28 (UTC+8))
- [#30379](https://github.com/vllm-project/vllm/issues/30379) [Usage]: how to use vllm/tests/ï¼Ÿ â€” usage â€” by tobeprozy (å…³é—­äº: 2025-12-10 17:26 (UTC+8))
- [#30311](https://github.com/vllm-project/vllm/issues/30311) [Bug]: deepseekv32.DeepseekV32Tokenizer Runtime causes model to crash â€” bug â€” by magician-xin (å…³é—­äº: 2025-12-10 16:30 (UTC+8))
- [#28314](https://github.com/vllm-project/vllm/issues/28314) [AMD][CI Failure]: Tracking failure for AMD CI Dependencies & Environments â€” rocm,ci-failure â€” by zhewenl (å…³é—­äº: 2025-12-10 13:32 (UTC+8))
- [#30343](https://github.com/vllm-project/vllm/issues/30343) [Bug]: In DeepSeek-V3.2 tokenizer mode, detokenization saturates the main thread, causing the server to hang â€” bug â€” by scratch-ml (å…³é—­äº: 2025-12-10 12:05 (UTC+8))

### æ–°å¢ PR
- [#30428](https://github.com/vllm-project/vllm/pull/30428) [Chore] Fix torch precision warning â€” ready,v1 â€” by yewentao256 (åˆ›å»ºäº: 2025-12-11 05:34 (UTC+8))
- [#30448](https://github.com/vllm-project/vllm/pull/30448) [NIXL] Heterogeneous KV Layout and block_size - prefill NHD and nP > nD support â€” v1,kv-connector â€” by xuechendi (åˆ›å»ºäº: 2025-12-11 10:06 (UTC+8))
- [#30449](https://github.com/vllm-project/vllm/pull/30449) Add SLA-tiered scheduling (opt-in) â€” documentation,v1 â€” by ProdByBuddha (åˆ›å»ºäº: 2025-12-11 10:17 (UTC+8))
- [#30437](https://github.com/vllm-project/vllm/pull/30437) [Bugfix] missing tokens occur in harmony streaming â€” frontend,gpt-oss â€” by Ri0S (åˆ›å»ºäº: 2025-12-11 07:36 (UTC+8))
- [#30431](https://github.com/vllm-project/vllm/pull/30431) Revert "[CI] Add Async Eplb nightly CI tests (#29385)" â€” ready,ci/build â€” by SageMoore (åˆ›å»ºäº: 2025-12-11 06:27 (UTC+8))
- [#30446](https://github.com/vllm-project/vllm/pull/30446) Added a test for invalid inputs for parse_raw_prompts â€” æ— æ ‡ç­¾ â€” by mivehk (åˆ›å»ºäº: 2025-12-11 09:23 (UTC+8))
- [#30444](https://github.com/vllm-project/vllm/pull/30444) [Fix] Update lazing loading of video loader backend â€” multi-modality â€” by jeremyteboul (åˆ›å»ºäº: 2025-12-11 08:49 (UTC+8))
- [#30442](https://github.com/vllm-project/vllm/pull/30442) [Feature] AWQ marlin quantization support for fused moe with lora â€” æ— æ ‡ç­¾ â€” by princepride (åˆ›å»ºäº: 2025-12-11 08:10 (UTC+8))
- [#30390](https://github.com/vllm-project/vllm/pull/30390) fix: Update json features supported by xGrammar â€” structured-output,ready,v1 â€” by johannesflommersfeld (åˆ›å»ºäº: 2025-12-10 20:51 (UTC+8))
- [#30432](https://github.com/vllm-project/vllm/pull/30432) [ROCm] Fix broken import in platform attention backend dispatching â€” rocm,ready â€” by AndreasKaratzas (åˆ›å»ºäº: 2025-12-11 06:40 (UTC+8))
- [#30440](https://github.com/vllm-project/vllm/pull/30440) [fix] Fix qwen3_coder tool call per parameter streaming â€” frontend,tool-calling,qwen â€” by koush (åˆ›å»ºäº: 2025-12-11 07:46 (UTC+8))
- [#30395](https://github.com/vllm-project/vllm/pull/30395) [ROCm] [CI] [Release] Add rocm wheel release pipeline â€” rocm,ci/build â€” by tjtanaa (åˆ›å»ºäº: 2025-12-10 22:28 (UTC+8))
- [#30443](https://github.com/vllm-project/vllm/pull/30443) [PT nightlies] Remove nightly_torch Docker image and use standard â€” ci/build â€” by orionr (åˆ›å»ºäº: 2025-12-11 08:47 (UTC+8))
- [#30418](https://github.com/vllm-project/vllm/pull/30418) LoRA Slab Optimization â€” v1 â€” by Majid-Taheri (åˆ›å»ºäº: 2025-12-11 02:47 (UTC+8))
- [#30438](https://github.com/vllm-project/vllm/pull/30438) [Feature][Observability] Fine-grained model runner timing metrics â€” v1 â€” by andylolu2 (åˆ›å»ºäº: 2025-12-11 07:38 (UTC+8))
- [#30433](https://github.com/vllm-project/vllm/pull/30433) [Bugfix] Qwen3-next with  --hf-overrides \{\"num_hidden_layers\":8\}  â€” qwen â€” by heheda12345 (åˆ›å»ºäº: 2025-12-11 07:19 (UTC+8))
- [#30434](https://github.com/vllm-project/vllm/pull/30434) fix(gguf): Use EOS token ID from GGUF metadata instead of HF tokenizer â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 07:20 (UTC+8))
- [#30407](https://github.com/vllm-project/vllm/pull/30407) fix(shm): Add memory barriers for cross-process shared memory visibility â€” ready â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:43 (UTC+8))
- [#30417](https://github.com/vllm-project/vllm/pull/30417) [CI/Build][AMD] Skip tests in test_fusions_e2e and test_dbo_dp_ep_gsm8k that require non-existing imports for ROCm  â€” rocm,v1 â€” by rasmith (åˆ›å»ºäº: 2025-12-11 02:15 (UTC+8))
- [#30399](https://github.com/vllm-project/vllm/pull/30399) [BugFix] Fix `AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight_scale'` â€” bug,ready,deepseek â€” by LucasWilkinson (åˆ›å»ºäº: 2025-12-10 23:14 (UTC+8))
- [#30430](https://github.com/vllm-project/vllm/pull/30430) [ROCm][Bugfix] Add MLACommonMetadata to allowed attention types for speculative decoding â€” rocm,speculative-decoding,v1 â€” by AndreasKaratzas (åˆ›å»ºäº: 2025-12-11 06:15 (UTC+8))
- [#30429](https://github.com/vllm-project/vllm/pull/30429) test branch - not for merge â€” needs-rebase,v1 â€” by debroy-rh (åˆ›å»ºäº: 2025-12-11 05:38 (UTC+8))
- [#30408](https://github.com/vllm-project/vllm/pull/30408) fix(gguf): Disable bfloat16 for GGUF on sm120 device â€” ready â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:45 (UTC+8))
- [#30389](https://github.com/vllm-project/vllm/pull/30389) Standardise `get_rope` to use `rope_parameters["partial_rotary_factor"]`, not `rotary_dim` â€” performance,ready,llama,qwen,deepseek,gpt-oss â€” by hmellor (åˆ›å»ºäº: 2025-12-10 20:33 (UTC+8))
- [#30423](https://github.com/vllm-project/vllm/pull/30423) fix(gguf): Make GGUFMoEMethod.apply() parameters optional â€” needs-rebase â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 03:42 (UTC+8))
- [#30413](https://github.com/vllm-project/vllm/pull/30413) fix(nemotron_h): Add missing rotary positional embeddings to attention layers â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:46 (UTC+8))
- [#30427](https://github.com/vllm-project/vllm/pull/30427) fix(gguf): Extract attn_logit_softcapping from GGUF metadata â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 05:27 (UTC+8))
- [#30422](https://github.com/vllm-project/vllm/pull/30422) [ROCm][CI][Bugfix] Fallback for grouped_topk when num_experts can't be grouped properly â€” rocm â€” by micah-wil (åˆ›å»ºäº: 2025-12-11 03:25 (UTC+8))
- [#30426](https://github.com/vllm-project/vllm/pull/30426) [Docs] Update EPLB docs â€” documentation,ready â€” by mgoin (åˆ›å»ºäº: 2025-12-11 04:45 (UTC+8))
- [#30425](https://github.com/vllm-project/vllm/pull/30425) [LMCache] Relax lmcache version requirement â€” ready,ci/build,kv-connector â€” by njhill (åˆ›å»ºäº: 2025-12-11 04:04 (UTC+8))
- [#30424](https://github.com/vllm-project/vllm/pull/30424) fix(gemma2): Add quant_config to embedding layer for GGUF support â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 03:46 (UTC+8))
- [#30400](https://github.com/vllm-project/vllm/pull/30400) {Deprecation] Remove tokenizer setter â€” frontend,ready,v1 â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-10 23:33 (UTC+8))
- [#30376](https://github.com/vllm-project/vllm/pull/30376) [Fix]fix import error from lmcache â€” ready,kv-connector â€” by wz1qqx (åˆ›å»ºäº: 2025-12-10 16:38 (UTC+8))
- [#30420](https://github.com/vllm-project/vllm/pull/30420) [NIXL][BUG FIX] Fix a bug for PD with host_buffer after merging 29665 â€” kv-connector â€” by xuechendi (åˆ›å»ºäº: 2025-12-11 03:08 (UTC+8))
- [#30419](https://github.com/vllm-project/vllm/pull/30419) [NIXL][BUG FIX] Fix both failing issue and accuracy issue with nixl + host_buffer on CUDA â€” v1,kv-connector,nvidia â€” by xuechendi (åˆ›å»ºäº: 2025-12-11 02:54 (UTC+8))
- [#30421](https://github.com/vllm-project/vllm/pull/30421) fix(gemma2): Skip missing parameters during GGUF weight loading â€” structured-output,v1 â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 03:24 (UTC+8))
- [#30391](https://github.com/vllm-project/vllm/pull/30391) [IMPROVEMENT] Change MistralReasoningParser behavior â€” ready â€” by juliendenize (åˆ›å»ºäº: 2025-12-10 21:02 (UTC+8))
- [#30415](https://github.com/vllm-project/vllm/pull/30415) [V0 Deprecation] Deprecate use_v1 â€” documentation â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-11 02:08 (UTC+8))
- [#30416](https://github.com/vllm-project/vllm/pull/30416) [Deprecation] Remove old `_Backend` enum â€” documentation,ready â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-11 02:11 (UTC+8))
- [#30414](https://github.com/vllm-project/vllm/pull/30414) [Doc] Add instructions for building docker image on GB300 with CUDA13 â€” documentation,aarch64-cuda,nvidia â€” by soodoshll (åˆ›å»ºäº: 2025-12-11 02:05 (UTC+8))
- [#30409](https://github.com/vllm-project/vllm/pull/30409) [BugFix] Lazy tokenizer init in StructuredOutputManager to prevent GGUF semaphore leak â€” structured-output,v1 â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:45 (UTC+8))
- [#30410](https://github.com/vllm-project/vllm/pull/30410) fix(gguf): Auto-select compatible dtype for GGUF models on Blackwell â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:45 (UTC+8))
- [#30373](https://github.com/vllm-project/vllm/pull/30373) Implement LMDB-based multi-modal cache â€” ci/build,v1,multi-modality â€” by petersalas (åˆ›å»ºäº: 2025-12-10 15:21 (UTC+8))
- [#30385](https://github.com/vllm-project/vllm/pull/30385) [Core] Whisper support `torch.compile` â€” v1 â€” by NickLucche (åˆ›å»ºäº: 2025-12-10 19:11 (UTC+8))
- [#30403](https://github.com/vllm-project/vllm/pull/30403) [Misc] Consistent case for `vllm bench serve` results â€” documentation,performance,structured-output â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-11 00:56 (UTC+8))
- [#30398](https://github.com/vllm-project/vllm/pull/30398) [Chore] Delay recent deprecations â€” ready,v1,multi-modality â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-10 23:04 (UTC+8))
- [#30411](https://github.com/vllm-project/vllm/pull/30411) fix(gguf): Ensure Gemma2 configs have hidden_act for backward compatibility â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:45 (UTC+8))
- [#30412](https://github.com/vllm-project/vllm/pull/30412) fix(gguf): Skip lm_head mapping for models with tied word embeddings â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:46 (UTC+8))
- [#30406](https://github.com/vllm-project/vllm/pull/30406) fix(nemotron_h): Add missing rotary positional embeddings to attention layers â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:42 (UTC+8))
- [#30404](https://github.com/vllm-project/vllm/pull/30404) fix(gguf): Ensure Gemma2 configs have hidden_act for backward compatibility â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:40 (UTC+8))
- [#30405](https://github.com/vllm-project/vllm/pull/30405) fix(gguf): Skip lm_head mapping for models with tied word embeddings â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:41 (UTC+8))
- [#30397](https://github.com/vllm-project/vllm/pull/30397) [Deprecation] Remove deprecated task, seed and MM settings â€” documentation,performance,frontend,ready,qwen â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-10 22:54 (UTC+8))
- [#30396](https://github.com/vllm-project/vllm/pull/30396) [Deprecation] Remove deprecated plugin and compilation fields for v0.13 release â€” documentation,ready â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-10 22:45 (UTC+8))
- [#30402](https://github.com/vllm-project/vllm/pull/30402) [Docs][CPU Backend] Add nightly and per revision pre-built Arm CPU wheels â€” documentation â€” by ioghiban (åˆ›å»ºäº: 2025-12-11 00:47 (UTC+8))
- [#30388](https://github.com/vllm-project/vllm/pull/30388) [Docs] Generate full list of metrics in user docs â€” documentation,ready â€” by markmc (åˆ›å»ºäº: 2025-12-10 19:50 (UTC+8))
- [#30386](https://github.com/vllm-project/vllm/pull/30386) [v1] Add PrefixLM support to TritonAttention backend â€” v1 â€” by Isotr0py (åˆ›å»ºäº: 2025-12-10 19:32 (UTC+8))
- [#30393](https://github.com/vllm-project/vllm/pull/30393) qk-rmsnorm op â€” qwen â€” by ZYang6263 (åˆ›å»ºäº: 2025-12-10 22:00 (UTC+8))
- [#30384](https://github.com/vllm-project/vllm/pull/30384) [BugFix] Fix minimax m2 model rotary_dim â€” ready â€” by rogeryoungh (åˆ›å»ºäº: 2025-12-10 18:37 (UTC+8))
- [#30377](https://github.com/vllm-project/vllm/pull/30377) adding constraint updates of cos-sin to improve mrope performance â€” æ— æ ‡ç­¾ â€” by wujinyuan1 (åˆ›å»ºäº: 2025-12-10 16:48 (UTC+8))
- [#30371](https://github.com/vllm-project/vllm/pull/30371) [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output â€” structured-output,ready,v1,deepseek â€” by chaunceyjiang (åˆ›å»ºäº: 2025-12-10 12:26 (UTC+8))

### å·²åˆå¹¶ PR
- [#30431](https://github.com/vllm-project/vllm/pull/30431) Revert "[CI] Add Async Eplb nightly CI tests (#29385)" â€” ready,ci/build â€” by SageMoore (åˆå¹¶äº: 2025-12-11 08:48 (UTC+8))
- [#30432](https://github.com/vllm-project/vllm/pull/30432) [ROCm] Fix broken import in platform attention backend dispatching â€” rocm,ready â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-11 09:12 (UTC+8))
- [#30106](https://github.com/vllm-project/vllm/pull/30106) Add more docs for regex â€” documentation,structured-output,ready â€” by xu-song (åˆå¹¶äº: 2025-12-11 08:12 (UTC+8))
- [#28051](https://github.com/vllm-project/vllm/pull/28051) [Bugfix] fix confusing OOM errors during v1 init â€” ready,v1 â€” by shivampr (åˆå¹¶äº: 2025-12-11 07:17 (UTC+8))
- [#30407](https://github.com/vllm-project/vllm/pull/30407) fix(shm): Add memory barriers for cross-process shared memory visibility â€” ready â€” by kitaekatt (åˆå¹¶äº: 2025-12-11 07:01 (UTC+8))
- [#30399](https://github.com/vllm-project/vllm/pull/30399) [BugFix] Fix `AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight_scale'` â€” bug,ready,deepseek â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-10 23:59 (UTC+8))
- [#27933](https://github.com/vllm-project/vllm/pull/27933) [docs] Improve wide-EP performance + benchmarking documentation â€” documentation,ready â€” by eicherseiji (åˆå¹¶äº: 2025-12-11 06:15 (UTC+8))
- [#30426](https://github.com/vllm-project/vllm/pull/30426) [Docs] Update EPLB docs â€” documentation,ready â€” by mgoin (åˆå¹¶äº: 2025-12-11 04:56 (UTC+8))
- [#30216](https://github.com/vllm-project/vllm/pull/30216) [LMCache] Fix breakage due to new LMCache version â€” ready,ci/build,kv-connector â€” by njhill (åˆå¹¶äº: 2025-12-11 03:52 (UTC+8))
- [#30400](https://github.com/vllm-project/vllm/pull/30400) {Deprecation] Remove tokenizer setter â€” frontend,ready,v1 â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 03:10 (UTC+8))
- [#30241](https://github.com/vllm-project/vllm/pull/30241) [bug] Fix "Current vLLM config is not set." warnings when FlashInfer attention is used â€” bug,ready,v1,nvidia â€” by nvpohanh (åˆå¹¶äº: 2025-12-11 03:18 (UTC+8))
- [#29289](https://github.com/vllm-project/vllm/pull/29289) [Perf] Enable environment cache in EngineCore to enable the feature for UniProcExecutor as well â€” ready,v1 â€” by Jialin (åˆå¹¶äº: 2025-12-11 03:13 (UTC+8))
- [#26813](https://github.com/vllm-project/vllm/pull/26813) [P/D] KV Load Failure Recovery/Abort Configuration â€” frontend,ready,v1,kv-connector â€” by wseaton (åˆå¹¶äº: 2025-12-11 03:00 (UTC+8))
- [#30344](https://github.com/vllm-project/vllm/pull/30344) [Bugfix] Fix HunyuanOCR cross-image contamination in batch processing â€” ready â€” by anker-c2 (åˆå¹¶äº: 2025-12-11 02:09 (UTC+8))
- [#30403](https://github.com/vllm-project/vllm/pull/30403) [Misc] Consistent case for `vllm bench serve` results â€” documentation,performance,structured-output â€” by MatthewBonanni (åˆå¹¶äº: 2025-12-11 01:44 (UTC+8))
- [#30398](https://github.com/vllm-project/vllm/pull/30398) [Chore] Delay recent deprecations â€” ready,v1,multi-modality â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 01:48 (UTC+8))
- [#30388](https://github.com/vllm-project/vllm/pull/30388) [Docs] Generate full list of metrics in user docs â€” documentation,ready â€” by markmc (åˆå¹¶äº: 2025-12-11 00:09 (UTC+8))
- [#30331](https://github.com/vllm-project/vllm/pull/30331) [Bugfix] tpu_model_runner: set vllm config context when calling reset_dynamo_cache() â€” tpu,ready,v1 â€” by dtrifiro (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30072](https://github.com/vllm-project/vllm/pull/30072) [Core] Whisper enable `FULL_DECODE_ONLY` CudaGraph  â€” ready,v1,multi-modality,nvidia â€” by NickLucche (åˆå¹¶äº: 2025-12-10 22:14 (UTC+8))
- [#30384](https://github.com/vllm-project/vllm/pull/30384) [BugFix] Fix minimax m2 model rotary_dim â€” ready â€” by rogeryoungh (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30062](https://github.com/vllm-project/vllm/pull/30062) [CPU] Support for Whisper â€” ready,ci/build,v1,multi-modality â€” by aditew01 (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30351](https://github.com/vllm-project/vllm/pull/30351) [Bugfix] Cache added_vocab to avoid per-token overhead â€” ready â€” by scratch-ml (åˆå¹¶äº: 2025-12-10 12:05 (UTC+8))
- [#30371](https://github.com/vllm-project/vllm/pull/30371) [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output â€” structured-output,ready,v1,deepseek â€” by chaunceyjiang (åˆå¹¶äº: 2025-12-10 16:30 (UTC+8))
- [#30347](https://github.com/vllm-project/vllm/pull/30347) [cpu][ci] Add CPU Attention Tests for Neon Backend â€” ready â€” by fadara01 (åˆå¹¶äº: 2025-12-10 13:37 (UTC+8))
- [#29358](https://github.com/vllm-project/vllm/pull/29358) [ROCm][CI] Attempt to fix the failures under a subgroup of the e2e the test group â€” rocm,ready,ci/build,v1,multi-modality â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-10 13:33 (UTC+8))
- [#30339](https://github.com/vllm-project/vllm/pull/30339) [CMake][Build]: Remove unused ACL CMake env variables â€” ready,ci/build â€” by Radu2k (åˆå¹¶äº: 2025-12-10 12:27 (UTC+8))
- [#30345](https://github.com/vllm-project/vllm/pull/30345) Fix typos in comments across multiple files â€” documentation,ready,v1 â€” by wilsonwu (åˆå¹¶äº: 2025-12-10 12:05 (UTC+8))
- [#30308](https://github.com/vllm-project/vllm/pull/30308) [bugfix][quantization] fix quark qwen3 kv_cache quantization â€” ready,qwen â€” by haoyangli-amd (åˆå¹¶äº: 2025-12-10 11:24 (UTC+8))

### å…³é—­ä½†æœªåˆå¹¶çš„ PR
- [#30086](https://github.com/vllm-project/vllm/pull/30086) Revert "[CI] Add Async Eplb nightly CI tests" â€” ready,ci/build â€” by SageMoore (å…³é—­äº: 2025-12-11 06:18 (UTC+8))
- [#29357](https://github.com/vllm-project/vllm/pull/29357) Add request-ids to TranscriptionRequest, TranslationRequest â€” documentation,frontend â€” by eicherseiji (å…³é—­äº: 2025-12-11 06:10 (UTC+8))
- [#30429](https://github.com/vllm-project/vllm/pull/30429) test branch - not for merge â€” needs-rebase,v1 â€” by debroy-rh (å…³é—­äº: 2025-12-11 06:08 (UTC+8))
- [#29924](https://github.com/vllm-project/vllm/pull/29924) [WIP] Try fixing nightly build pipeline â€” ci/build â€” by atalman (å…³é—­äº: 2025-12-11 05:54 (UTC+8))
- [#30423](https://github.com/vllm-project/vllm/pull/30423) fix(gguf): Make GGUFMoEMethod.apply() parameters optional â€” needs-rebase â€” by kitaekatt (å…³é—­äº: 2025-12-11 05:44 (UTC+8))
- [#30415](https://github.com/vllm-project/vllm/pull/30415) [V0 Deprecation] Deprecate use_v1 â€” documentation â€” by MatthewBonanni (å…³é—­äº: 2025-12-11 02:25 (UTC+8))
- [#30416](https://github.com/vllm-project/vllm/pull/30416) [Deprecation] Remove old `_Backend` enum â€” documentation,ready â€” by MatthewBonanni (å…³é—­äº: 2025-12-11 02:26 (UTC+8))
- [#29198](https://github.com/vllm-project/vllm/pull/29198) [Model] Restore Gemma3 GGUF multimodal support with GGUF-only guards â€” ready,v1,multi-modality â€” by lucianommartins (å…³é—­äº: 2025-12-11 03:04 (UTC+8))
- [#29819](https://github.com/vllm-project/vllm/pull/29819) fix(shm): Add memory barriers for cross-process shared memory visibility â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30406](https://github.com/vllm-project/vllm/pull/30406) fix(nemotron_h): Add missing rotary positional embeddings to attention layers â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30404](https://github.com/vllm-project/vllm/pull/30404) fix(gguf): Ensure Gemma2 configs have hidden_act for backward compatibility â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:40 (UTC+8))
- [#30365](https://github.com/vllm-project/vllm/pull/30365) fix(gguf): Auto-select compatible dtype for GGUF models on Blackwell â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30284](https://github.com/vllm-project/vllm/pull/30284) [BugFix] Lazy tokenizer init in StructuredOutputManager to prevent GGUF semaphore leak â€” structured-output,v1 â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30090](https://github.com/vllm-project/vllm/pull/30090) fix: Force float16 dtype for GGUF models to fix incorrect output â€” ready â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30405](https://github.com/vllm-project/vllm/pull/30405) fix(gguf): Skip lm_head mapping for models with tied word embeddings â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:41 (UTC+8))
- [#28156](https://github.com/vllm-project/vllm/pull/28156) [CI/Build] Skip encoder-decoder models on AMD â€” rocm,ready,ci/build â€” by zhewenl (å…³é—­äº: 2025-12-11 01:22 (UTC+8))
- [#28353](https://github.com/vllm-project/vllm/pull/28353) try fix by record_stream() â€” needs-rebase â€” by zhewenl (å…³é—­äº: 2025-12-11 01:22 (UTC+8))
- [#28799](https://github.com/vllm-project/vllm/pull/28799) enable tests â€” needs-rebase,ci/build â€” by zhewenl (å…³é—­äº: 2025-12-11 01:22 (UTC+8))
- [#28523](https://github.com/vllm-project/vllm/pull/28523) debug tests/tool_use/test_tool_calls.py failure on AMD â€” documentation,performance,rocm,frontend,ci/build,tool-calling â€” by zhewenl (å…³é—­äº: 2025-12-11 01:22 (UTC+8))
- [#30258](https://github.com/vllm-project/vllm/pull/30258) [Feature]: OpenTelemetry Metrics Support â€” v1 â€” by mladjan-gadzic (å…³é—­äº: 2025-12-10 22:23 (UTC+8))
- [#23997](https://github.com/vllm-project/vllm/pull/23997) Feature/sampler benchmark #23977 â€” performance,unstale â€” by baonudesifeizhai (å…³é—­äº: 2025-12-10 21:14 (UTC+8))
- [#26701](https://github.com/vllm-project/vllm/pull/26701) [ROCm]: W8A8BlockFp8LinearOp should use AITER on MI355X â€” rocm,ready,needs-rebase â€” by gronsti-amd (å…³é—­äº: 2025-12-10 20:33 (UTC+8))
- [#30348](https://github.com/vllm-project/vllm/pull/30348) [Docs]: adds a new metric vllm:request_prefill_kv_computed_tokens in docs â€” documentation â€” by googs1025 (å…³é—­äº: 2025-12-10 19:59 (UTC+8))
- [#30349](https://github.com/vllm-project/vllm/pull/30349) [BugFix] Fix minimax m2 model rope_parameters â€” æ— æ ‡ç­¾ â€” by esmeetu (å…³é—­äº: 2025-12-10 18:47 (UTC+8))
- [#29653](https://github.com/vllm-project/vllm/pull/29653) fix potential object has no attribute 'bias' error â€” æ— æ ‡ç­¾ â€” by allerou4 (å…³é—­äº: 2025-12-10 15:16 (UTC+8))
- [#30297](https://github.com/vllm-project/vllm/pull/30297) [Core] Add SLA-tiered scheduling (opt-in) and docs â€” documentation,v1 â€” by ProdByBuddha (å…³é—­äº: 2025-12-10 13:13 (UTC+8))