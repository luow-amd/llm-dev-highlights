# vLLM å¼€å‘åŠ¨æ€æŠ¥å‘Š - 2025-12-10

> æ—¶é—´çª—å£: 2025-12-10 09:35 (UTC+8) ~ 2025-12-11 09:35 (UTC+8)
> æ•°æ®ç»Ÿè®¡: æ–° Issue 20 | å…³é—­ Issue 31 | æ–° PR 59 | åˆå¹¶ PR 31 | å…³é—­æœªåˆå¹¶ PR 36

---

### ğŸ“Š æ¯æ—¥å¼€å‘çŠ¶æ€æ‘˜è¦
åœ¨12æœˆ10æ—¥è‡³11æ—¥çš„è§‚å¯Ÿçª—å£å†…ï¼ŒvLLMé¡¹ç›®ä¿æŒäº†é«˜å¼ºåº¦çš„å¼€å‘ä¸é—®é¢˜ä¿®å¤èŠ‚å¥ï¼Œåˆå¹¶äº†31ä¸ªPRï¼Œå¹¶å…³é—­äº†31ä¸ªIssueã€‚å¼€å‘é‡ç‚¹é›†ä¸­åœ¨ä¿®å¤å„ç±»è¿è¡Œæ—¶çš„Bugï¼ˆå°¤å…¶æ˜¯ä¸æ–°æ¨¡å‹ã€é‡åŒ–åŠåˆ†å¸ƒå¼æ‰§è¡Œç›¸å…³çš„ï¼‰ã€ä¼˜åŒ–æ€§èƒ½ï¼ˆå¦‚ç¼“å­˜ã€ç¼–è¯‘ï¼‰ã€ä»¥åŠå®Œå–„å¯¹AMD ROCmå¹³å°çš„æ”¯æŒã€‚ç¤¾åŒºæ´»è·ƒï¼Œä»£ç å®¡æŸ¥ä¸åˆå¹¶æµç¨‹é«˜æ•ˆã€‚

### ğŸ¯ AMD/ROCm ç”Ÿæ€ç›¸å…³åŠ¨æ€
æœ¬å‘¨æœŸAMDç”Ÿæ€ç›¸å…³æ´»åŠ¨èšç„¦äºé—®é¢˜ä¿®å¤ä¸CI/CDæµç¨‹å®Œå–„ï¼Œæ˜¾ç¤ºå‡ºå¯¹å¹³å°ç¨³å®šæ€§çš„æŒç»­æŠ•å…¥ã€‚

1.  **PR #30432 (`[ROCm] Fix broken import in platform attention backend dispatching`) (å·²åˆå¹¶)**
    *   **æŠ€æœ¯ç»†èŠ‚**ï¼šä¿®å¤äº†ROCmå¹³å°åœ¨åˆå§‹åŒ–æ—¶å› å¯¼å…¥ `get_env_variable_attn_backend` è€Œå¤±è´¥çš„é—®é¢˜ã€‚è¯¥å‡½æ•°åœ¨#30396ä¸­å·²è¢«åºŸå¼ƒã€‚PRç›´æ¥ä¾èµ–ç¯å¢ƒå˜é‡ï¼ˆ`VLLM_ROCM_USE_AITER_UNIFIED_ATTENTION`ï¼Œ `VLLM_ROCM_USE_AITER`ï¼‰æ¥é…ç½®KVç¼“å­˜å—å¤§å°ï¼Œæ¶ˆé™¤äº†å¯¹å³å°†ç§»é™¤çš„æŠ½è±¡å±‚çš„ä¾èµ–ã€‚
    *   **å½±å“**ï¼šç¡®ä¿äº†ROCmå¹³å°èƒ½å¤Ÿæ­£å¸¸å¯åŠ¨ï¼Œæ˜¯ç»´æŠ¤AMDåç«¯å…¼å®¹æ€§çš„å¿…è¦ä¿®è¡¥ã€‚

2.  **PR #30395 (`[ROCm] [CI] [Release] Add rocm wheel release pipeline`) (è¿›è¡Œä¸­)**
    *   **æŠ€æœ¯ç»†èŠ‚**ï¼šç”±AMDå‘˜å·¥ (`tjtanaa`) æäº¤ï¼Œæ—¨åœ¨ä¸ºvLLMå»ºç«‹æ­£å¼çš„ROCm wheelå‘å¸ƒæµæ°´çº¿ã€‚è¯¥è®¾è®¡åŒ…å«å¤šé˜¶æ®µæ„å»ºã€ç¼“å­˜ä¼˜åŒ–ï¼ˆåŸºäºPythonç‰ˆæœ¬ã€ROCmæ¶æ„å’ŒDockerfileå†…å®¹å“ˆå¸Œï¼‰ä»¥åŠæ‰‹åŠ¨è§¦å‘æœºåˆ¶ï¼Œä»¥åº”å¯¹æ„å»ºè€—æ—¶ï¼ˆçº¦2å°æ—¶ï¼‰å’Œå­˜å‚¨ç©ºé—´å ç”¨å¤§çš„æŒ‘æˆ˜ã€‚
    *   **å½±å“**ï¼šæ­¤PRæ ‡å¿—ç€AMDç”Ÿæ€æ”¯æŒä»â€œå¯ç”¨â€å‘â€œæ˜“ç”¨ä¸”å¯äº¤ä»˜â€è¿ˆè¿›çš„å…³é”®ä¸€æ­¥ã€‚ä¸€æ—¦å®Œæˆï¼Œç”¨æˆ·å°†èƒ½æ›´æ–¹ä¾¿åœ°è·å–å’Œä½¿ç”¨é¢„ç¼–è¯‘çš„vLLM ROCmç‰ˆæœ¬ã€‚

3.  **PR #30430 (`[ROCm][Bugfix] Add MLACommonMetadata to allowed attention types for speculative decoding`) (è¿›è¡Œä¸­)**
    *   **æŠ€æœ¯ç»†èŠ‚**ï¼šä¿®å¤äº†åœ¨ROCmå¹³å°ä¸Šä½¿ç”¨MLAï¼ˆMulti-head Latent Attentionï¼‰æ³¨æ„åŠ›åç«¯ï¼ˆå¦‚DeepSeekæ¨¡å‹ï¼‰å¹¶è¿›è¡Œæ¨æµ‹è§£ç ï¼ˆ`num_speculative_tokens > 1`ï¼‰æ—¶ï¼Œå› å…ƒæ•°æ®ç±»å‹ä¸è¢«æ”¯æŒè€ŒæŠ¥é”™çš„é—®é¢˜ã€‚
    *   **å½±å“**ï¼šæ‰©å±•äº†ROCmå¹³å°ä¸Šæ¨æµ‹è§£ç åŠŸèƒ½çš„æ”¯æŒèŒƒå›´ï¼Œæå‡äº†å¤æ‚æ¨ç†å·¥ä½œæµåœ¨AMDç¡¬ä»¶ä¸Šçš„å¯ç”¨æ€§ã€‚

4.  **å·²å…³é—­ Issue #29453ï¼Œ #29464ï¼Œ #29803 ç­‰ï¼ˆå¤šä¸ªAMD CIå¤±è´¥è¿½è¸ªï¼‰**
    *   **æ¦‚è¿°**ï¼šå¤šä¸ªé•¿æœŸå­˜åœ¨çš„AMD CIæµ‹è¯•å¤±è´¥é—®é¢˜åœ¨æœ¬å‘¨æœŸå†…è¢«å…³é—­ï¼Œæ ‡è®°ä¸ºå·²ä¿®å¤ï¼ˆç»¿è‰²ï¼‰ã€‚è¿™äº›é—®é¢˜æ¶µç›–äº†åŸºç¡€æ­£ç¡®æ€§æµ‹è¯•ã€OpenAI APIéŸ³é¢‘è½¬å½•æµ‹è¯•ã€cudagraphæµ‹è¯•ç­‰ã€‚
    *   **å½±å“**ï¼šè¡¨æ˜AMDå›¢é˜Ÿåœ¨è§£å†³ROCmä¸vLLM V1å¼•æ“é›†æˆè¿‡ç¨‹ä¸­çš„å†å²é—ç•™é—®é¢˜æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæå‡äº†æ•´ä½“æµ‹è¯•é€šè¿‡ç‡å’Œä»£ç ä¿¡å¿ƒã€‚

### ğŸ’¬ é«˜çƒ­åº¦è®¨è®ºåˆ†æ
æœ¬å‘¨æœŸå†…æ–°å¼€è®®é¢˜è®¨è®ºçƒ­åº¦æ™®éä¸€èˆ¬ï¼Œä½†ä¸€ä¸ªå…³äº**é”™è¯¯æ—¥å¿—æ”¹è¿›**çš„é•¿æœŸIssueåœ¨å…³é—­æ—¶å¼•å‘äº†å›é¡¾æ€§å…³æ³¨ã€‚

1.  **Issue #14083 (`[Feature]: Improve Logging for Error Messages`) (å·²å…³é—­)**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šè¯·æ±‚ä¸ºV1å¼•æ“åˆå§‹åŒ–é˜¶æ®µå¸¸è§é”™è¯¯ï¼ˆå¦‚å†…å­˜ä¸è¶³ã€KVç¼“å­˜ç©ºé—´ä¸è¶³ï¼‰æä¾›æ›´æ¸…æ™°ã€æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯ï¼Œè€Œéæ™¦æ¶©çš„CUDAå¼‚å¸¸ã€‚
    *   **è§‚ç‚¹ä¸ç«‹åœº**ï¼š
        *   **å‘èµ·æ–¹/è´¡çŒ®è€…**ï¼šå¤šä½ç¤¾åŒºæˆå‘˜ï¼ˆå¦‚ `Sheehan20`, `elizabetht`, `rojagtap`ï¼‰è¡¨è¾¾äº†è§£å†³æ­¤é—®é¢˜çš„å…´è¶£å¹¶å°è¯•æäº¤PRï¼Œæ˜¾ç¤ºå‡ºå¯¹æ”¹å–„å¼€å‘è€…ä½“éªŒçš„å¼ºçƒˆéœ€æ±‚ã€‚
        *   **ç»´æŠ¤è€…**ï¼šè®¨è®ºä¸­æ¶‰åŠäº†æŠ€æœ¯å®ç°é€‰æ‹©ï¼Œä¾‹å¦‚æ˜¯æŠ›å‡ºæ›´å…·ä½“çš„å¼‚å¸¸æ¶ˆæ¯è¿˜æ˜¯åˆ›å»ºè‡ªå®šä¹‰å¼‚å¸¸ç±»å‹ã€‚
    *   **äº‰è®®ç„¦ç‚¹**ï¼šæ— æ˜¾è‘—äº‰è®®ï¼Œä¸»è¦å›´ç»•å®ç°æ–¹æ¡ˆè¿›è¡ŒæŠ€æœ¯æ€§è®¨è®ºã€‚
    *   **æœ€ç»ˆç»“è®º**ï¼šPR #28051 æœ€ç»ˆè¢«åˆå¹¶ï¼Œé€šè¿‡åœ¨æ¨¡å‹åŠ è½½å’ŒKVç¼“å­˜æ£€æŸ¥å¤„æ·»åŠ æ¸…æ™°çš„`try/except`åŒ…è£…å’Œé”™è¯¯ä¿¡æ¯ï¼Œè§£å†³äº†è¯¥éœ€æ±‚ã€‚è¿™ä½“ç°äº†ç¤¾åŒºè´¡çŒ®ä¸æ ¸å¿ƒå›¢é˜Ÿåä½œçš„æˆåŠŸæ¡ˆä¾‹ã€‚

### ğŸ”¥ çƒ­é—¨è¯é¢˜ä¸è¶‹åŠ¿åˆ†æ
1.  **é‡åŒ–æ¨¡å‹æ”¯æŒé—®é¢˜é›†ä¸­æ¶Œç°**ï¼šå¤šä¸ªIssueæŠ¥å‘Šäº†ç‰¹å®šé‡åŒ–æ¨¡å‹ï¼ˆå¦‚AWQ, GPTQ, FP8 KV Cacheï¼‰åœ¨ç‰¹å®šæ¡ä»¶ä¸‹äº§ç”Ÿé”™è¯¯è¾“å‡ºã€å´©æºƒæˆ–æ€§èƒ½ä¸è¾¾é¢„æœŸçš„é—®é¢˜ï¼ˆ#30445ï¼Œ #30370ï¼Œ #30387ï¼‰ã€‚è¿™å‡¸æ˜¾äº†éšç€é‡åŒ–æ ¼å¼å’Œç¡¬ä»¶çš„å¿«é€Ÿæ¼”è¿›ï¼Œæ¨ç†å¼•æ“åœ¨å…¼å®¹æ€§ä¸æ­£ç¡®æ€§æ–¹é¢é¢ä¸´çš„æŒç»­æŒ‘æˆ˜ã€‚
2.  **æ–°æ¨¡å‹/æ–°æ¶æ„çš„é›†æˆæŒ‘æˆ˜**ï¼šIssuesä¸­æ¶‰åŠäº†DeepSeek-V3.2ã€MiniMax-M2ã€Qwen3 rerankerã€Nemotron-Hç­‰è¾ƒæ–°æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨ã€è§£æã€æƒé‡åŠ è½½æ—¶é‡åˆ°çš„é—®é¢˜ã€‚è¡¨æ˜vLLmåœ¨å¿«é€Ÿé€‚é…ä¸šç•Œå±‚å‡ºä¸ç©·çš„æ–°æ¨¡å‹æ¶æ„æ–¹é¢å­˜åœ¨ä¸€å®šå‹åŠ›ã€‚
3.  **æ€§èƒ½åˆ†æä¸åŸºå‡†æµ‹è¯•å·¥å…·å—åˆ°å…³æ³¨**ï¼šå‡ºç°äº†å…³äºæ”¹è¿›åŸºå‡†æµ‹è¯•å·¥å…·ä»¥çªç ´å•æ ¸é™åˆ¶ï¼ˆ#30383ï¼‰å’Œä¸ºCPUåç«¯æ·»åŠ åˆ†é¡µæ³¨æ„åŠ›åŸºå‡†ï¼ˆ#30374ï¼‰çš„RFC/Feature Requestã€‚è¿™åæ˜ äº†ç¤¾åŒºå’Œå¼€å‘è€…å¯¹æ·±åº¦æ€§èƒ½å‰–æå’Œå‡†ç¡®æ€§èƒ½è¯„ä¼°çš„éœ€æ±‚å¢é•¿ã€‚
4.  **ç³»ç»Ÿå¯é æ€§ä¸å¯è§‚æµ‹æ€§**ï¼šé™¤äº†é”™è¯¯æ—¥å¿—æ”¹è¿›ï¼Œè¿˜æœ‰PRæ—¨åœ¨æ·»åŠ æ›´ç»†ç²’åº¦çš„æ¨¡å‹è¿è¡Œå™¨æ—¶åºæŒ‡æ ‡ï¼ˆ#30438ï¼‰å’ŒæŠ½è±¡ç›‘æ§æŒ‡æ ‡å±‚ä»¥æ”¯æŒOpenTelemetryï¼ˆ#30394ï¼‰ã€‚è¿™è¡¨æ˜é¡¹ç›®æ­£æœç€ç”Ÿäº§å°±ç»ªæ€§ï¼ˆProduction-readinessï¼‰å’Œæ›´å¼ºçš„å¯è¿ç»´æ€§æ–¹å‘å‘å±•ã€‚

### ğŸ› ï¸ é‡ç‚¹æŠ€æœ¯å˜æ›´
1.  **PR #30384 (`[BugFix] Fix minimax m2 model rotary_dim`) (å·²åˆå¹¶)**ï¼šä¿®å¤äº†åœ¨ #29966 ç»Ÿä¸€RoPEé…ç½®è¯»å–é€»è¾‘åï¼Œå¯¼è‡´MiniMax-M2ç­‰ä½¿ç”¨`rotary_dim`ï¼ˆè€Œé`partial_rotary_factor`ï¼‰é…ç½®çš„æ¨¡å‹æ—‹è½¬ç»´åº¦è¢«é‡å¤è®¡ç®—çš„é—®é¢˜ã€‚**å½±å“**ï¼šç¡®ä¿äº†å¯¹éæ ‡å‡†é…ç½®æ¨¡å‹çš„RoPEè®¡ç®—æ­£ç¡®æ€§ï¼Œç»´æŠ¤äº†æ¨¡å‹ç”Ÿæ€çš„å¹¿æ³›å…¼å®¹æ€§ã€‚
2.  **PR #30371 (`[Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output`) (å·²åˆå¹¶)**ï¼šä¿®å¤äº†DeepSeek-V3.2æ¨¡å‹æ— æ³•ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼ˆJSON Schemaï¼Œ Grammarç­‰ï¼‰åŠŸèƒ½çš„é—®é¢˜ã€‚**å½±å“**ï¼šè§£é”äº†è¯¥çƒ­é—¨æ¨¡å‹çš„é«˜çº§ç”Ÿæˆæ§åˆ¶èƒ½åŠ›ï¼Œæå‡äº†å…¶å¯ç”¨æ€§ã€‚
3.  **PR #30351 (`[Bugfix] Cache added_vocab to avoid per-token overhead`) (å·²åˆå¹¶)**ï¼šä¿®å¤äº†DeepSeek-V3.2åœ¨ç‰¹å®šTokenizeræ¨¡å¼ä¸‹ï¼Œå› æ¯æ¬¡è°ƒç”¨`__len__`éƒ½é‡æ–°è®¡ç®—æ·»åŠ è¯æ±‡è¡¨è€Œå¯¼è‡´ä¸»çº¿ç¨‹è¢«é˜»å¡ã€æœåŠ¡å¡é¡¿çš„é—®é¢˜ã€‚é€šè¿‡ç¼“å­˜ä¼˜åŒ–ï¼Œæ¶ˆé™¤äº†æ€§èƒ½ç“¶é¢ˆã€‚**å½±å“**ï¼šæ˜¾è‘—æå‡äº†ç‰¹å®šæ¨¡å‹åœ¨é«˜è´Ÿè½½ä¸‹çš„æœåŠ¡å“åº”èƒ½åŠ›å’Œç¨³å®šæ€§ï¼Œæ˜¯æ€§èƒ½ä¼˜åŒ–çš„ç»å…¸æ¡ˆä¾‹ã€‚
4.  **PR #30344 (`[Bugfix] Fix HunyuanOCR cross-image contamination in batch processing`) (å·²åˆå¹¶)**ï¼šä¿®å¤äº†å¤šæ¨¡æ€æ¨¡å‹HunyuanOCRåœ¨å¤„ç†åŒä¸€æ‰¹æ¬¡ä¸­ä¸åŒå°ºå¯¸å›¾ç‰‡æ—¶ï¼Œç‰¹å¾æ··æ·†å¯¼è‡´è¾“å‡ºé”™è¯¯çš„é—®é¢˜ã€‚é€šè¿‡é€å›¾ç‹¬ç«‹å¤„ç†æ¯ä¸€å±‚Transformerè§£å†³ã€‚**å½±å“**ï¼šä¿è¯äº†è§†è§‰æ¨¡å‹æ‰¹å¤„ç†çš„æ­£ç¡®æ€§ï¼Œä¸ºå¤šæ¨¡æ€ä»»åŠ¡çš„å¯é æ‰¹é‡æ¨ç†å¥ å®šäº†åŸºç¡€ã€‚

### ğŸ“ˆ å¼€å‘æ´»è·ƒåº¦è§‚å¯Ÿ
*   **é«˜æ•ˆåˆå¹¶**ï¼šåœ¨24å°æ—¶å†…åˆå¹¶äº†31ä¸ªPRï¼Œå…¶ä¸­è®¸å¤šå¸¦æœ‰ `ready` æ ‡ç­¾ï¼Œè¡¨æ˜ä»£ç å®¡æŸ¥å’ŒCIæµç¨‹è¿è¡Œé¡ºç•…ï¼Œæ ¸å¿ƒå›¢é˜Ÿèƒ½å¤Ÿå¿«é€Ÿæ¥çº³æœ‰ä»·å€¼çš„è´¡çŒ®ã€‚
*   **å¤šå…ƒåŒ–è´¡çŒ®**ï¼šè´¡çŒ®è€…ä¸ä»…åŒ…æ‹¬AMD (`AndreasKaratzas`, `micah-wil`)ã€NVIDIAç­‰ç¡¬ä»¶å‚å•†çš„å‘˜å·¥ï¼Œä¹Ÿæœ‰æ¥è‡ªå¹¿æ³›ç¤¾åŒºçš„ç‹¬ç«‹å¼€å‘è€…ï¼Œå›´ç»•æ¨¡å‹æ”¯æŒã€Bugä¿®å¤å’Œæ–‡æ¡£å®Œå–„è¿›è¡Œåä½œã€‚
*   **é—®é¢˜é—­ç¯æ•ˆç‡**ï¼šå…³é—­äº†31ä¸ªIssueï¼Œå…¶ä¸­åŒ…å«å¤šä¸ªé•¿æœŸæ‚¬è€Œæœªå†³çš„AMD CIå¤±è´¥é—®é¢˜ï¼Œæ˜¾ç¤ºå‡ºå›¢é˜Ÿåœ¨é—®é¢˜è¿½è¸ªå’Œè§£å†³ä¸Šçš„ä¸“æ³¨ä¸æ•ˆç‡ã€‚

### ğŸ’¡ å€¼å¾—å…³æ³¨çš„é—®é¢˜
1.  **Issue #30445 (`QuantTrio/MiniMax-M2-AWQ produces garbage in 12/10/2025 build`)**ï¼šç”¨æˆ·æŠ¥å‘Šç‰¹å®šAWQé‡åŒ–æ¨¡å‹åœ¨å½“æ—¥æ„å»ºç‰ˆæœ¬ä¸­è¾“å‡ºä¹±ç ï¼Œè€Œå‰ä¸€æ—¥æ„å»ºæ­£å¸¸ã€‚è¿™**å¯èƒ½è¡¨æ˜æœ€æ–°çš„ä»£ç æ›´æ”¹å¼•å…¥äº†å¯¹æŸäº›é‡åŒ–æ¨¡å‹æ”¯æŒçš„å›å½’**ï¼Œéœ€è¦åŠæ—¶æ’æŸ¥æ ¹æºã€‚
2.  **Issue #30374 (`[Feature][CPU Backend]: Add Paged Attention Benchmarks for CPU backend`)**ï¼šè¯¥è¯·æ±‚å¸Œæœ›ä¸ºCPUåç«¯çš„åˆ†é¡µæ³¨æ„åŠ›å†…æ ¸å»ºç«‹ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•è„šæœ¬ã€‚**CPUæ¨ç†æ˜¯ä¸€ä¸ªé‡è¦ä¸”æ´»è·ƒçš„æ–¹å‘**ï¼Œæ‹¥æœ‰æ ‡å‡†åŒ–çš„æ€§èƒ½è¯„ä¼°å·¥å…·å¯¹äºå…¶æŒç»­ä¼˜åŒ–è‡³å…³é‡è¦ã€‚
3.  **Issue #30383 (`[RFC]: Multi-Process Benchmark Architecture for Scaling Beyond Single-Core Limits`)**ï¼šä½œè€…æŒ‡å‡ºå½“å‰`vllm bench`å·¥å…·åœ¨é«˜å‹ä¸‹å› å•æ ¸æ€§èƒ½ç“¶é¢ˆå¯¼è‡´æŒ‡æ ‡å¤±çœŸï¼Œå¹¶æå‡ºåŸºäºå¤šè¿›ç¨‹æ¶æ„çš„æ–°è®¾è®¡ã€‚**è¿™æ˜¯ä¸€ä¸ªå¯¹è¯„ä¼°vLLMçœŸå®æœåŠ¡èƒ½åŠ›æœ‰æ·±è¿œå½±å“çš„æè®®**ï¼Œå€¼å¾—ç¤¾åŒºæ·±å…¥è®¨è®ºã€‚

---

## ğŸ“‹ é™„å½•ï¼šè¯¦ç»†æ•°æ®åˆ—è¡¨

### æ–°å¢ Issue
- [#30445](https://github.com/vllm-project/vllm/issues/30445) [Bug]: QuantTrio/MiniMax-M2-AWQ produces garbage in 12/10/2025 build â€” bug â€” by eugr (åˆ›å»ºäº: 2025-12-11 09:06 (UTC+8))
- [#30441](https://github.com/vllm-project/vllm/issues/30441) [Usage]: vllm serve setup issues on B300 â€” usage â€” by navmarri14 (åˆ›å»ºäº: 2025-12-11 07:50 (UTC+8))
- [#30439](https://github.com/vllm-project/vllm/issues/30439) [Bug]: Qwen3 Coder parser does not stream tool call arguments â€” bug â€” by koush (åˆ›å»ºäº: 2025-12-11 07:45 (UTC+8))
- [#30436](https://github.com/vllm-project/vllm/issues/30436) [Bug]: Speculative decode crashes on PP>1 because self.drafter missing â€” bug â€” by kvcop (åˆ›å»ºäº: 2025-12-11 07:26 (UTC+8))
- [#30435](https://github.com/vllm-project/vllm/issues/30435) [Bug]: vllm problem with cu130 â€” bug â€” by Reviveplugins (åˆ›å»ºäº: 2025-12-11 07:24 (UTC+8))
- [#30387](https://github.com/vllm-project/vllm/issues/30387) [Bug]: illegal memory access countered when using kv-cache-type=fp8  loading  a weight-fp8 model for evaltest  in flash-attn backend â€” bug â€” by youngze0016 (åˆ›å»ºäº: 2025-12-10 19:43 (UTC+8))
- [#30401](https://github.com/vllm-project/vllm/issues/30401) [Bug]: EAGLE3 failure with gpt-oss 20b and 120b â€” bug â€” by Mhdaw (åˆ›å»ºäº: 2025-12-11 00:42 (UTC+8))
- [#30394](https://github.com/vllm-project/vllm/issues/30394) [Feature]: Prometheus Metrics Abstraction â€” feature request â€” by mladjan-gadzic (åˆ›å»ºäº: 2025-12-10 22:22 (UTC+8))
- [#30392](https://github.com/vllm-project/vllm/issues/30392) [Usage]: Docker image v0.12.0 Fail to run inference via Docker image â€” usage â€” by kuopching (åˆ›å»ºäº: 2025-12-10 21:43 (UTC+8))
- [#30380](https://github.com/vllm-project/vllm/issues/30380) [Usage]: å¤§å®¶ä¸€èˆ¬æ€ä¹ˆä½¿ç”¨vllm/testsçš„ï¼Ÿ â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:27 (UTC+8))
- [#30382](https://github.com/vllm-project/vllm/issues/30382) [Bug]: Issues with mistralai/Ministral-3-14B-Instruct-2512 â€” bug â€” by eltorre (åˆ›å»ºäº: 2025-12-10 17:55 (UTC+8))
- [#30383](https://github.com/vllm-project/vllm/issues/30383) [RFC]: Multi-Process Benchmark Architecture for Scaling Beyond Single-Core Limits â€” RFC â€” by GaoHuaZhang (åˆ›å»ºäº: 2025-12-10 18:02 (UTC+8))
- [#30374](https://github.com/vllm-project/vllm/issues/30374) [Feature][CPU Backend]: Add Paged Attention Benchmarks for CPU backend â€” feature request,cpu â€” by fadara01 (åˆ›å»ºäº: 2025-12-10 15:53 (UTC+8))
- [#30381](https://github.com/vllm-project/vllm/issues/30381) [Usage]: â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:27 (UTC+8))
- [#30379](https://github.com/vllm-project/vllm/issues/30379) [Usage]: how to use vllm/tests/ï¼Ÿ â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:25 (UTC+8))
- [#30378](https://github.com/vllm-project/vllm/issues/30378) [Feature]: Automatically infer Qwen3 reranker settings (remove need for hf_overrides) â€” feature request â€” by ilopezluna (åˆ›å»ºäº: 2025-12-10 17:21 (UTC+8))
- [#30375](https://github.com/vllm-project/vllm/issues/30375) [Bug]: [TPU] ShapeDtypeStruct error when loading custom safetensors checkpoint on TPU v5litepod â€” bug â€” by Baltsat (åˆ›å»ºäº: 2025-12-10 16:12 (UTC+8))
- [#30372](https://github.com/vllm-project/vllm/issues/30372) [Bug]: vLLM (GPT-OSS) causes distorted tool argument names + infinite tool-call loop with Korean messenger tool â€” bug â€” by minmini2 (åˆ›å»ºäº: 2025-12-10 14:59 (UTC+8))
- [#30370](https://github.com/vllm-project/vllm/issues/30370) [Performance]: DeepSeek-V3.2 AWQ Performance is lower then i expected â€” performance â€” by yongho-chang (åˆ›å»ºäº: 2025-12-10 10:45 (UTC+8))
- [#30368](https://github.com/vllm-project/vllm/issues/30368) [CI] Test target determination using LLM â€” feature request,ci â€” by khluu (åˆ›å»ºäº: 2025-12-10 09:42 (UTC+8))

### å·²å…³é—­ Issue
- [#29453](https://github.com/vllm-project/vllm/issues/29453) [CI Failure]: mi325_1: Basic Correctness Test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:21 (UTC+8))
- [#29464](https://github.com/vllm-project/vllm/issues/29464) [CI Failure]: mi325_1: OpenAI API correctness â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:21 (UTC+8))
- [#29803](https://github.com/vllm-project/vllm/issues/29803) [CI Failure]: mi325_1: Cudagraph test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:20 (UTC+8))
- [#29465](https://github.com/vllm-project/vllm/issues/29465) [CI Failure]: mi325_2: Prime-RL Integration Test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:18 (UTC+8))
- [#29526](https://github.com/vllm-project/vllm/issues/29526) [CI Failure]: mi325_1: Entrypoints Integration Test (Pooling) â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:17 (UTC+8))
- [#29514](https://github.com/vllm-project/vllm/issues/29514) [CI Failure]: mi325_4: EPLB Execution Test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:17 (UTC+8))
- [#29443](https://github.com/vllm-project/vllm/issues/29443) [CI Failure]: mi325_1: Python-only Installation Test â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:16 (UTC+8))
- [#29537](https://github.com/vllm-project/vllm/issues/29537) [CI Failure]: mi325_2: Weight Loading Multiple GPU Test - Large Models â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:09 (UTC+8))
- [#29534](https://github.com/vllm-project/vllm/issues/29534) [CI Failure]: mi325_8: LoRA Test %N â€” ci-failure â€” by AndreasKaratzas (å…³é—­äº: 2025-12-11 09:01 (UTC+8))
- [#14083](https://github.com/vllm-project/vllm/issues/14083) [Feature]: Improve Logging for Error Messages â€” help wanted,good first issue,feature request,unstale â€” by robertgshaw2-redhat (å…³é—­äº: 2025-12-11 07:17 (UTC+8))
- [#30214](https://github.com/vllm-project/vllm/issues/30214) [Bug]: DeepSeek V3.2 on B200 fails with "CUTLASS_MLA is not valid... Reason: ['sparse not supported']" â€” bug â€” by hadikoub (å…³é—­äº: 2025-12-11 04:20 (UTC+8))
- [#30240](https://github.com/vllm-project/vllm/issues/30240) [Bug]: Lots of "Current vLLM config is not set." warnings when FlashInfer attention is used â€” bug â€” by nvpohanh (å…³é—­äº: 2025-12-11 03:18 (UTC+8))
- [#24601](https://github.com/vllm-project/vllm/issues/24601) [Bug]: Launching multiple vLLM processes at the same time doesn't work well with vLLM's compile cache â€” bug,torch.compile â€” by zou3519 (å…³é—­äº: 2025-12-11 02:53 (UTC+8))
- [#30342](https://github.com/vllm-project/vllm/issues/30342) [Bug]: HunyuanOCR batching problem with variable sized images in a batch. â€” bug â€” by anker-c2 (å…³é—­äº: 2025-12-11 02:09 (UTC+8))
- [#15636](https://github.com/vllm-project/vllm/issues/15636) [Bug]: Outlines broken on vLLM 0.8+ â€” bug,structured-output,unstale â€” by cpfiffer (å…³é—­äº: 2025-12-10 21:18 (UTC+8))
- [#30382](https://github.com/vllm-project/vllm/issues/30382) [Bug]: Issues with mistralai/Ministral-3-14B-Instruct-2512 â€” bug â€” by eltorre (å…³é—­äº: 2025-12-10 18:47 (UTC+8))
- [#30381](https://github.com/vllm-project/vllm/issues/30381) [Usage]: â€” usage â€” by tobeprozy (å…³é—­äº: 2025-12-10 17:28 (UTC+8))
- [#30379](https://github.com/vllm-project/vllm/issues/30379) [Usage]: how to use vllm/tests/ï¼Ÿ â€” usage â€” by tobeprozy (å…³é—­äº: 2025-12-10 17:26 (UTC+8))
- [#30311](https://github.com/vllm-project/vllm/issues/30311) [Bug]: deepseekv32.DeepseekV32Tokenizer Runtime causes model to crash â€” bug â€” by magician-xin (å…³é—­äº: 2025-12-10 16:30 (UTC+8))
- [#28314](https://github.com/vllm-project/vllm/issues/28314) [AMD][CI Failure]: Tracking failure for AMD CI Dependencies & Environments â€” rocm,ci-failure â€” by zhewenl (å…³é—­äº: 2025-12-10 13:32 (UTC+8))
- [#30343](https://github.com/vllm-project/vllm/issues/30343) [Bug]: In DeepSeek-V3.2 tokenizer mode, detokenization saturates the main thread, causing the server to hang â€” bug â€” by scratch-ml (å…³é—­äº: 2025-12-10 12:05 (UTC+8))
- [#20181](https://github.com/vllm-project/vllm/issues/20181) [Feature]: Batch inference for Multi-Modal Online Serving â€” feature request,stale â€” by eslambakr (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#21097](https://github.com/vllm-project/vllm/issues/21097) [Bug]: w8a8 quantization not supporting sm120 â€” bug,stale â€” by sarmiena (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21909](https://github.com/vllm-project/vllm/issues/21909) [Bug]: quant_method is not None â€” bug,stale â€” by maxin9966 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22325](https://github.com/vllm-project/vllm/issues/22325) [Bug]: gpt-oss model crashes on NVIDIA B200 with any OpenAI chat completion request â€” bug,stale â€” by teds-lin (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22422](https://github.com/vllm-project/vllm/issues/22422) [Feature]: mxfp4 support for 3090 â€” feature request,stale â€” by ehartford (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22501](https://github.com/vllm-project/vllm/issues/22501) [Usage]: Running a 300-400B Parameter Model on Multi-Node Setup (2x 8xA100) â€” usage,stale â€” by rangehow (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22575](https://github.com/vllm-project/vllm/issues/22575) [Bug]: Vllm hangs when I use the offline engine with dp = 2 or more â€” bug,stale â€” by Stealthwriter (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22623](https://github.com/vllm-project/vllm/issues/22623) [Usage]: if openai-mirror/gpt-oss-20b  can run in A100? â€” usage,stale â€” by neverstoplearn (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#22624](https://github.com/vllm-project/vllm/issues/22624) [Bug]: 1.7B fp16 + 0.6B draft OOM with gpu_memory_utilization=0.9, while 4B int8 + 0.6B works fine on A800 80 GB â€” bug,stale â€” by kiexu (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#22639](https://github.com/vllm-project/vllm/issues/22639) [Bug]: function convert_lark_to_gbnf interpreting '#' to parse as lark commentaries â€” bug,stale â€” by renout-nicolas (å…³é—­äº: 2025-12-10 10:23 (UTC+8))

### æ–°å¢ PR
- [#30431](https://github.com/vllm-project/vllm/pull/30431) Revert "[CI] Add Async Eplb nightly CI tests (#29385)" â€” ready,ci/build â€” by SageMoore (åˆ›å»ºäº: 2025-12-11 06:27 (UTC+8))
- [#30446](https://github.com/vllm-project/vllm/pull/30446) Added a test for invalid inputs for parse_raw_prompts â€” æ— æ ‡ç­¾ â€” by mivehk (åˆ›å»ºäº: 2025-12-11 09:23 (UTC+8))
- [#30444](https://github.com/vllm-project/vllm/pull/30444) [Fix] Update lazing loading of video loader backend â€” multi-modality â€” by jeremyteboul (åˆ›å»ºäº: 2025-12-11 08:49 (UTC+8))
- [#30442](https://github.com/vllm-project/vllm/pull/30442) [Feature] AWQ marlin quantization support for fused moe with lora â€” æ— æ ‡ç­¾ â€” by princepride (åˆ›å»ºäº: 2025-12-11 08:10 (UTC+8))
- [#30390](https://github.com/vllm-project/vllm/pull/30390) fix: Update json features supported by xGrammar â€” structured-output,ready,v1 â€” by johannesflommersfeld (åˆ›å»ºäº: 2025-12-10 20:51 (UTC+8))
- [#30432](https://github.com/vllm-project/vllm/pull/30432) [ROCm] Fix broken import in platform attention backend dispatching â€” rocm,ready â€” by AndreasKaratzas (åˆ›å»ºäº: 2025-12-11 06:40 (UTC+8))
- [#30440](https://github.com/vllm-project/vllm/pull/30440) [fix] Fix qwen3_coder tool call per parameter streaming â€” frontend,tool-calling,qwen â€” by koush (åˆ›å»ºäº: 2025-12-11 07:46 (UTC+8))
- [#30395](https://github.com/vllm-project/vllm/pull/30395) [ROCm] [CI] [Release] Add rocm wheel release pipeline â€” rocm,ci/build â€” by tjtanaa (åˆ›å»ºäº: 2025-12-10 22:28 (UTC+8))
- [#30443](https://github.com/vllm-project/vllm/pull/30443) [PT nightlies] Remove nightly_torch Docker image and use standard â€” ci/build â€” by orionr (åˆ›å»ºäº: 2025-12-11 08:47 (UTC+8))
- [#30418](https://github.com/vllm-project/vllm/pull/30418) LoRA Slab Optimization â€” v1 â€” by Majid-Taheri (åˆ›å»ºäº: 2025-12-11 02:47 (UTC+8))
- [#30437](https://github.com/vllm-project/vllm/pull/30437) [Bugfix] missing tokens occur in harmony streaming â€” frontend,gpt-oss â€” by Ri0S (åˆ›å»ºäº: 2025-12-11 07:36 (UTC+8))
- [#30438](https://github.com/vllm-project/vllm/pull/30438) [Feature][Observability] Fine-grained model runner timing metrics â€” v1 â€” by andylolu2 (åˆ›å»ºäº: 2025-12-11 07:38 (UTC+8))
- [#30433](https://github.com/vllm-project/vllm/pull/30433) [Bugfix] Qwen3-next with  --hf-overrides \{\"num_hidden_layers\":8\}  â€” qwen â€” by heheda12345 (åˆ›å»ºäº: 2025-12-11 07:19 (UTC+8))
- [#30434](https://github.com/vllm-project/vllm/pull/30434) fix(gguf): Use EOS token ID from GGUF metadata instead of HF tokenizer â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 07:20 (UTC+8))
- [#30407](https://github.com/vllm-project/vllm/pull/30407) fix(shm): Add memory barriers for cross-process shared memory visibility â€” ready â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:43 (UTC+8))
- [#30417](https://github.com/vllm-project/vllm/pull/30417) [CI/Build][AMD] Skip tests in test_fusions_e2e and test_dbo_dp_ep_gsm8k that require non-existing imports for ROCm  â€” rocm,v1 â€” by rasmith (åˆ›å»ºäº: 2025-12-11 02:15 (UTC+8))
- [#30399](https://github.com/vllm-project/vllm/pull/30399) [BugFix] Fix `AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight_scale'` â€” bug,ready,deepseek â€” by LucasWilkinson (åˆ›å»ºäº: 2025-12-10 23:14 (UTC+8))
- [#30430](https://github.com/vllm-project/vllm/pull/30430) [ROCm][Bugfix] Add MLACommonMetadata to allowed attention types for speculative decoding â€” rocm,speculative-decoding,v1 â€” by AndreasKaratzas (åˆ›å»ºäº: 2025-12-11 06:15 (UTC+8))
- [#30429](https://github.com/vllm-project/vllm/pull/30429) test branch - not for merge â€” needs-rebase,v1 â€” by debroy-rh (åˆ›å»ºäº: 2025-12-11 05:38 (UTC+8))
- [#30408](https://github.com/vllm-project/vllm/pull/30408) fix(gguf): Disable bfloat16 for GGUF on sm120 device â€” ready â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:45 (UTC+8))
- [#30389](https://github.com/vllm-project/vllm/pull/30389) Standardise `get_rope` to use `rope_parameters["partial_rotary_factor"]`, not `rotary_dim` â€” performance,ready,llama,qwen,deepseek,gpt-oss â€” by hmellor (åˆ›å»ºäº: 2025-12-10 20:33 (UTC+8))
- [#30423](https://github.com/vllm-project/vllm/pull/30423) fix(gguf): Make GGUFMoEMethod.apply() parameters optional â€” needs-rebase â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 03:42 (UTC+8))
- [#30413](https://github.com/vllm-project/vllm/pull/30413) fix(nemotron_h): Add missing rotary positional embeddings to attention layers â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:46 (UTC+8))
- [#30428](https://github.com/vllm-project/vllm/pull/30428) [Chore] Fix torch precision warning â€” ready,v1 â€” by yewentao256 (åˆ›å»ºäº: 2025-12-11 05:34 (UTC+8))
- [#30427](https://github.com/vllm-project/vllm/pull/30427) fix(gguf): Extract attn_logit_softcapping from GGUF metadata â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 05:27 (UTC+8))
- [#30422](https://github.com/vllm-project/vllm/pull/30422) [ROCm][CI][Bugfix] Fallback for grouped_topk when num_experts can't be grouped properly â€” rocm â€” by micah-wil (åˆ›å»ºäº: 2025-12-11 03:25 (UTC+8))
- [#30426](https://github.com/vllm-project/vllm/pull/30426) [Docs] Update EPLB docs â€” documentation,ready â€” by mgoin (åˆ›å»ºäº: 2025-12-11 04:45 (UTC+8))
- [#30425](https://github.com/vllm-project/vllm/pull/30425) [LMCache] Relax lmcache version requirement â€” ready,ci/build,kv-connector â€” by njhill (åˆ›å»ºäº: 2025-12-11 04:04 (UTC+8))
- [#30424](https://github.com/vllm-project/vllm/pull/30424) fix(gemma2): Add quant_config to embedding layer for GGUF support â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 03:46 (UTC+8))
- [#30400](https://github.com/vllm-project/vllm/pull/30400) {Deprecation] Remove tokenizer setter â€” frontend,ready,v1 â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-10 23:33 (UTC+8))
- [#30376](https://github.com/vllm-project/vllm/pull/30376) [Fix]fix import error from lmcache â€” ready,kv-connector â€” by wz1qqx (åˆ›å»ºäº: 2025-12-10 16:38 (UTC+8))
- [#30420](https://github.com/vllm-project/vllm/pull/30420) [NIXL][BUG FIX] Fix a bug for PD with host_buffer after merging 29665 â€” kv-connector â€” by xuechendi (åˆ›å»ºäº: 2025-12-11 03:08 (UTC+8))
- [#30419](https://github.com/vllm-project/vllm/pull/30419) [NIXL][BUG FIX] Fix both failing issue and accuracy issue with nixl + host_buffer on CUDA â€” v1,kv-connector,nvidia â€” by xuechendi (åˆ›å»ºäº: 2025-12-11 02:54 (UTC+8))
- [#30421](https://github.com/vllm-project/vllm/pull/30421) fix(gemma2): Skip missing parameters during GGUF weight loading â€” structured-output,v1 â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 03:24 (UTC+8))
- [#30391](https://github.com/vllm-project/vllm/pull/30391) [IMPROVEMENT] Change MistralReasoningParser behavior â€” ready â€” by juliendenize (åˆ›å»ºäº: 2025-12-10 21:02 (UTC+8))
- [#30415](https://github.com/vllm-project/vllm/pull/30415) [V0 Deprecation] Deprecate use_v1 â€” documentation â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-11 02:08 (UTC+8))
- [#30416](https://github.com/vllm-project/vllm/pull/30416) [Deprecation] Remove old `_Backend` enum â€” documentation,ready â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-11 02:11 (UTC+8))
- [#30414](https://github.com/vllm-project/vllm/pull/30414) [Doc] Add instructions for building docker image on GB300 with CUDA13 â€” documentation,aarch64-cuda,nvidia â€” by soodoshll (åˆ›å»ºäº: 2025-12-11 02:05 (UTC+8))
- [#30409](https://github.com/vllm-project/vllm/pull/30409) [BugFix] Lazy tokenizer init in StructuredOutputManager to prevent GGUF semaphore leak â€” structured-output,v1 â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:45 (UTC+8))
- [#30410](https://github.com/vllm-project/vllm/pull/30410) fix(gguf): Auto-select compatible dtype for GGUF models on Blackwell â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:45 (UTC+8))
- [#30373](https://github.com/vllm-project/vllm/pull/30373) Implement LMDB-based multi-modal cache â€” ci/build,v1,multi-modality â€” by petersalas (åˆ›å»ºäº: 2025-12-10 15:21 (UTC+8))
- [#30385](https://github.com/vllm-project/vllm/pull/30385) [Core] Whisper support `torch.compile` â€” v1 â€” by NickLucche (åˆ›å»ºäº: 2025-12-10 19:11 (UTC+8))
- [#30403](https://github.com/vllm-project/vllm/pull/30403) [Misc] Consistent case for `vllm bench serve` results â€” documentation,performance,structured-output â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-11 00:56 (UTC+8))
- [#30398](https://github.com/vllm-project/vllm/pull/30398) [Chore] Delay recent deprecations â€” ready,v1,multi-modality â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-10 23:04 (UTC+8))
- [#30411](https://github.com/vllm-project/vllm/pull/30411) fix(gguf): Ensure Gemma2 configs have hidden_act for backward compatibility â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:45 (UTC+8))
- [#30412](https://github.com/vllm-project/vllm/pull/30412) fix(gguf): Skip lm_head mapping for models with tied word embeddings â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:46 (UTC+8))
- [#30406](https://github.com/vllm-project/vllm/pull/30406) fix(nemotron_h): Add missing rotary positional embeddings to attention layers â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:42 (UTC+8))
- [#30404](https://github.com/vllm-project/vllm/pull/30404) fix(gguf): Ensure Gemma2 configs have hidden_act for backward compatibility â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:40 (UTC+8))
- [#30405](https://github.com/vllm-project/vllm/pull/30405) fix(gguf): Skip lm_head mapping for models with tied word embeddings â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-11 01:41 (UTC+8))
- [#30369](https://github.com/vllm-project/vllm/pull/30369) [Fix] Add default rope theta for qwen1 model â€” qwen â€” by iwzbi (åˆ›å»ºäº: 2025-12-10 10:36 (UTC+8))
- [#30397](https://github.com/vllm-project/vllm/pull/30397) [Deprecation] Remove deprecated task, seed and MM settings â€” documentation,performance,frontend,ready,qwen â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-10 22:54 (UTC+8))
- [#30396](https://github.com/vllm-project/vllm/pull/30396) [Deprecation] Remove deprecated plugin and compilation fields for v0.13 release â€” documentation,ready â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-10 22:45 (UTC+8))
- [#30402](https://github.com/vllm-project/vllm/pull/30402) [Docs][CPU Backend] Add nightly and per revision pre-built Arm CPU wheels â€” documentation â€” by ioghiban (åˆ›å»ºäº: 2025-12-11 00:47 (UTC+8))
- [#30388](https://github.com/vllm-project/vllm/pull/30388) [Docs] Generate full list of metrics in user docs â€” documentation,ready â€” by markmc (åˆ›å»ºäº: 2025-12-10 19:50 (UTC+8))
- [#30386](https://github.com/vllm-project/vllm/pull/30386) [v1] Add PrefixLM support to TritonAttention backend â€” v1 â€” by Isotr0py (åˆ›å»ºäº: 2025-12-10 19:32 (UTC+8))
- [#30393](https://github.com/vllm-project/vllm/pull/30393) qk-rmsnorm op â€” qwen â€” by ZYang6263 (åˆ›å»ºäº: 2025-12-10 22:00 (UTC+8))
- [#30384](https://github.com/vllm-project/vllm/pull/30384) [BugFix] Fix minimax m2 model rotary_dim â€” ready â€” by rogeryoungh (åˆ›å»ºäº: 2025-12-10 18:37 (UTC+8))
- [#30377](https://github.com/vllm-project/vllm/pull/30377) adding constraint updates of cos-sin to improve mrope performance â€” æ— æ ‡ç­¾ â€” by wujinyuan1 (åˆ›å»ºäº: 2025-12-10 16:48 (UTC+8))
- [#30371](https://github.com/vllm-project/vllm/pull/30371) [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output â€” structured-output,ready,v1,deepseek â€” by chaunceyjiang (åˆ›å»ºäº: 2025-12-10 12:26 (UTC+8))

### å·²åˆå¹¶ PR
- [#30431](https://github.com/vllm-project/vllm/pull/30431) Revert "[CI] Add Async Eplb nightly CI tests (#29385)" â€” ready,ci/build â€” by SageMoore (åˆå¹¶äº: 2025-12-11 08:48 (UTC+8))
- [#30432](https://github.com/vllm-project/vllm/pull/30432) [ROCm] Fix broken import in platform attention backend dispatching â€” rocm,ready â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-11 09:12 (UTC+8))
- [#30106](https://github.com/vllm-project/vllm/pull/30106) Add more docs for regex â€” documentation,structured-output,ready â€” by xu-song (åˆå¹¶äº: 2025-12-11 08:12 (UTC+8))
- [#28051](https://github.com/vllm-project/vllm/pull/28051) [Bugfix] fix confusing OOM errors during v1 init â€” ready,v1 â€” by shivampr (åˆå¹¶äº: 2025-12-11 07:17 (UTC+8))
- [#30407](https://github.com/vllm-project/vllm/pull/30407) fix(shm): Add memory barriers for cross-process shared memory visibility â€” ready â€” by kitaekatt (åˆå¹¶äº: 2025-12-11 07:01 (UTC+8))
- [#30399](https://github.com/vllm-project/vllm/pull/30399) [BugFix] Fix `AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight_scale'` â€” bug,ready,deepseek â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-10 23:59 (UTC+8))
- [#27933](https://github.com/vllm-project/vllm/pull/27933) [docs] Improve wide-EP performance + benchmarking documentation â€” documentation,ready â€” by eicherseiji (åˆå¹¶äº: 2025-12-11 06:15 (UTC+8))
- [#30426](https://github.com/vllm-project/vllm/pull/30426) [Docs] Update EPLB docs â€” documentation,ready â€” by mgoin (åˆå¹¶äº: 2025-12-11 04:56 (UTC+8))
- [#30216](https://github.com/vllm-project/vllm/pull/30216) [LMCache] Fix breakage due to new LMCache version â€” ready,ci/build,kv-connector â€” by njhill (åˆå¹¶äº: 2025-12-11 03:52 (UTC+8))
- [#30400](https://github.com/vllm-project/vllm/pull/30400) {Deprecation] Remove tokenizer setter â€” frontend,ready,v1 â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 03:10 (UTC+8))
- [#30241](https://github.com/vllm-project/vllm/pull/30241) [bug] Fix "Current vLLM config is not set." warnings when FlashInfer attention is used â€” bug,ready,v1,nvidia â€” by nvpohanh (åˆå¹¶äº: 2025-12-11 03:18 (UTC+8))
- [#29289](https://github.com/vllm-project/vllm/pull/29289) [Perf] Enable environment cache in EngineCore to enable the feature for UniProcExecutor as well â€” ready,v1 â€” by Jialin (åˆå¹¶äº: 2025-12-11 03:13 (UTC+8))
- [#26813](https://github.com/vllm-project/vllm/pull/26813) [P/D] KV Load Failure Recovery/Abort Configuration â€” frontend,ready,v1,kv-connector â€” by wseaton (åˆå¹¶äº: 2025-12-11 03:00 (UTC+8))
- [#30344](https://github.com/vllm-project/vllm/pull/30344) [Bugfix] Fix HunyuanOCR cross-image contamination in batch processing â€” ready â€” by anker-c2 (åˆå¹¶äº: 2025-12-11 02:09 (UTC+8))
- [#30403](https://github.com/vllm-project/vllm/pull/30403) [Misc] Consistent case for `vllm bench serve` results â€” documentation,performance,structured-output â€” by MatthewBonanni (åˆå¹¶äº: 2025-12-11 01:44 (UTC+8))
- [#30398](https://github.com/vllm-project/vllm/pull/30398) [Chore] Delay recent deprecations â€” ready,v1,multi-modality â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 01:48 (UTC+8))
- [#30388](https://github.com/vllm-project/vllm/pull/30388) [Docs] Generate full list of metrics in user docs â€” documentation,ready â€” by markmc (åˆå¹¶äº: 2025-12-11 00:09 (UTC+8))
- [#30331](https://github.com/vllm-project/vllm/pull/30331) [Bugfix] tpu_model_runner: set vllm config context when calling reset_dynamo_cache() â€” tpu,ready,v1 â€” by dtrifiro (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30072](https://github.com/vllm-project/vllm/pull/30072) [Core] Whisper enable `FULL_DECODE_ONLY` CudaGraph  â€” ready,v1,multi-modality,nvidia â€” by NickLucche (åˆå¹¶äº: 2025-12-10 22:14 (UTC+8))
- [#30384](https://github.com/vllm-project/vllm/pull/30384) [BugFix] Fix minimax m2 model rotary_dim â€” ready â€” by rogeryoungh (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30062](https://github.com/vllm-project/vllm/pull/30062) [CPU] Support for Whisper â€” ready,ci/build,v1,multi-modality â€” by aditew01 (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30351](https://github.com/vllm-project/vllm/pull/30351) [Bugfix] Cache added_vocab to avoid per-token overhead â€” ready â€” by scratch-ml (åˆå¹¶äº: 2025-12-10 12:05 (UTC+8))
- [#30371](https://github.com/vllm-project/vllm/pull/30371) [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output â€” structured-output,ready,v1,deepseek â€” by chaunceyjiang (åˆå¹¶äº: 2025-12-10 16:30 (UTC+8))
- [#30347](https://github.com/vllm-project/vllm/pull/30347) [cpu][ci] Add CPU Attention Tests for Neon Backend â€” ready â€” by fadara01 (åˆå¹¶äº: 2025-12-10 13:37 (UTC+8))
- [#29358](https://github.com/vllm-project/vllm/pull/29358) [ROCm][CI] Attempt to fix the failures under a subgroup of the e2e the test group â€” rocm,ready,ci/build,v1,multi-modality â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-10 13:33 (UTC+8))
- [#30339](https://github.com/vllm-project/vllm/pull/30339) [CMake][Build]: Remove unused ACL CMake env variables â€” ready,ci/build â€” by Radu2k (åˆå¹¶äº: 2025-12-10 12:27 (UTC+8))
- [#30345](https://github.com/vllm-project/vllm/pull/30345) Fix typos in comments across multiple files â€” documentation,ready,v1 â€” by wilsonwu (åˆå¹¶äº: 2025-12-10 12:05 (UTC+8))
- [#30308](https://github.com/vllm-project/vllm/pull/30308) [bugfix][quantization] fix quark qwen3 kv_cache quantization â€” ready,qwen â€” by haoyangli-amd (åˆå¹¶äº: 2025-12-10 11:24 (UTC+8))
- [#30367](https://github.com/vllm-project/vllm/pull/30367) [CI] Reduce Flakiness For test_spec_decode.py::test_suffix_decoding_acceptance â€” ready,v1 â€” by micah-wil (åˆå¹¶äº: 2025-12-10 10:35 (UTC+8))
- [#30230](https://github.com/vllm-project/vllm/pull/30230) [responsesAPI][6] Fix multi turn MCP tokenization â€” documentation,frontend,ready,gpt-oss â€” by qandrew (åˆå¹¶äº: 2025-12-10 10:13 (UTC+8))
- [#30020](https://github.com/vllm-project/vllm/pull/30020) [CI/Build][AMD] Skip quantization kernels tests that require CUTLASS or e4m3fn when not supported by platform â€” rocm,ready,nvidia â€” by rasmith (åˆå¹¶äº: 2025-12-10 10:28 (UTC+8))

### å…³é—­ä½†æœªåˆå¹¶çš„ PR
- [#30086](https://github.com/vllm-project/vllm/pull/30086) Revert "[CI] Add Async Eplb nightly CI tests" â€” ready,ci/build â€” by SageMoore (å…³é—­äº: 2025-12-11 06:18 (UTC+8))
- [#29357](https://github.com/vllm-project/vllm/pull/29357) Add request-ids to TranscriptionRequest, TranslationRequest â€” documentation,frontend â€” by eicherseiji (å…³é—­äº: 2025-12-11 06:10 (UTC+8))
- [#30429](https://github.com/vllm-project/vllm/pull/30429) test branch - not for merge â€” needs-rebase,v1 â€” by debroy-rh (å…³é—­äº: 2025-12-11 06:08 (UTC+8))
- [#29924](https://github.com/vllm-project/vllm/pull/29924) [WIP] Try fixing nightly build pipeline â€” ci/build â€” by atalman (å…³é—­äº: 2025-12-11 05:54 (UTC+8))
- [#30423](https://github.com/vllm-project/vllm/pull/30423) fix(gguf): Make GGUFMoEMethod.apply() parameters optional â€” needs-rebase â€” by kitaekatt (å…³é—­äº: 2025-12-11 05:44 (UTC+8))
- [#30415](https://github.com/vllm-project/vllm/pull/30415) [V0 Deprecation] Deprecate use_v1 â€” documentation â€” by MatthewBonanni (å…³é—­äº: 2025-12-11 02:25 (UTC+8))
- [#30416](https://github.com/vllm-project/vllm/pull/30416) [Deprecation] Remove old `_Backend` enum â€” documentation,ready â€” by MatthewBonanni (å…³é—­äº: 2025-12-11 02:26 (UTC+8))
- [#29198](https://github.com/vllm-project/vllm/pull/29198) [Model] Restore Gemma3 GGUF multimodal support with GGUF-only guards â€” ready,v1,multi-modality â€” by lucianommartins (å…³é—­äº: 2025-12-11 03:04 (UTC+8))
- [#29819](https://github.com/vllm-project/vllm/pull/29819) fix(shm): Add memory barriers for cross-process shared memory visibility â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30406](https://github.com/vllm-project/vllm/pull/30406) fix(nemotron_h): Add missing rotary positional embeddings to attention layers â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30404](https://github.com/vllm-project/vllm/pull/30404) fix(gguf): Ensure Gemma2 configs have hidden_act for backward compatibility â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:40 (UTC+8))
- [#30365](https://github.com/vllm-project/vllm/pull/30365) fix(gguf): Auto-select compatible dtype for GGUF models on Blackwell â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30284](https://github.com/vllm-project/vllm/pull/30284) [BugFix] Lazy tokenizer init in StructuredOutputManager to prevent GGUF semaphore leak â€” structured-output,v1 â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30090](https://github.com/vllm-project/vllm/pull/30090) fix: Force float16 dtype for GGUF models to fix incorrect output â€” ready â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:42 (UTC+8))
- [#30405](https://github.com/vllm-project/vllm/pull/30405) fix(gguf): Skip lm_head mapping for models with tied word embeddings â€” æ— æ ‡ç­¾ â€” by kitaekatt (å…³é—­äº: 2025-12-11 01:41 (UTC+8))
- [#28156](https://github.com/vllm-project/vllm/pull/28156) [CI/Build] Skip encoder-decoder models on AMD â€” rocm,ready,ci/build â€” by zhewenl (å…³é—­äº: 2025-12-11 01:22 (UTC+8))
- [#28353](https://github.com/vllm-project/vllm/pull/28353) try fix by record_stream() â€” needs-rebase â€” by zhewenl (å…³é—­äº: 2025-12-11 01:22 (UTC+8))
- [#28799](https://github.com/vllm-project/vllm/pull/28799) enable tests â€” needs-rebase,ci/build â€” by zhewenl (å…³é—­äº: 2025-12-11 01:22 (UTC+8))
- [#28523](https://github.com/vllm-project/vllm/pull/28523) debug tests/tool_use/test_tool_calls.py failure on AMD â€” documentation,performance,rocm,frontend,ci/build,tool-calling â€” by zhewenl (å…³é—­äº: 2025-12-11 01:22 (UTC+8))
- [#30258](https://github.com/vllm-project/vllm/pull/30258) [Feature]: OpenTelemetry Metrics Support â€” v1 â€” by mladjan-gadzic (å…³é—­äº: 2025-12-10 22:23 (UTC+8))
- [#23997](https://github.com/vllm-project/vllm/pull/23997) Feature/sampler benchmark #23977 â€” performance,unstale â€” by baonudesifeizhai (å…³é—­äº: 2025-12-10 21:14 (UTC+8))
- [#26701](https://github.com/vllm-project/vllm/pull/26701) [ROCm]: W8A8BlockFp8LinearOp should use AITER on MI355X â€” rocm,ready,needs-rebase â€” by gronsti-amd (å…³é—­äº: 2025-12-10 20:33 (UTC+8))
- [#30348](https://github.com/vllm-project/vllm/pull/30348) [Docs]: adds a new metric vllm:request_prefill_kv_computed_tokens in docs â€” documentation â€” by googs1025 (å…³é—­äº: 2025-12-10 19:59 (UTC+8))
- [#30349](https://github.com/vllm-project/vllm/pull/30349) [BugFix] Fix minimax m2 model rope_parameters â€” æ— æ ‡ç­¾ â€” by esmeetu (å…³é—­äº: 2025-12-10 18:47 (UTC+8))
- [#29653](https://github.com/vllm-project/vllm/pull/29653) fix potential object has no attribute 'bias' error â€” æ— æ ‡ç­¾ â€” by allerou4 (å…³é—­äº: 2025-12-10 15:16 (UTC+8))
- [#30297](https://github.com/vllm-project/vllm/pull/30297) [Core] Add SLA-tiered scheduling (opt-in) and docs â€” documentation,v1 â€” by ProdByBuddha (å…³é—­äº: 2025-12-10 13:13 (UTC+8))
- [#30327](https://github.com/vllm-project/vllm/pull/30327) [BugFix] Fix hang issue in LMCache mp mode â€” v1,kv-connector â€” by wz1qqx (å…³é—­äº: 2025-12-10 10:32 (UTC+8))
- [#17830](https://github.com/vllm-project/vllm/pull/17830) cmake: Get rid of VLLM_PYTHON_EXECUTABLE â€” needs-rebase,ci/build,stale â€” by seemethere (å…³é—­äº: 2025-12-10 10:26 (UTC+8))
- [#17872](https://github.com/vllm-project/vllm/pull/17872) measure peak memory correctly by removing already used memory â€” needs-rebase,stale,v1 â€” by MiladInk (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#17959](https://github.com/vllm-project/vllm/pull/17959) [Bugfix] fix check kv cache memory log info â€” needs-rebase,stale,v1 â€” by BoL0150 (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#21056](https://github.com/vllm-project/vllm/pull/21056) [Feature][EPLB] Add EPLB support for MiniMax-01 â€” stale â€” by haveheartt (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21413](https://github.com/vllm-project/vllm/pull/21413) Intentionally fail parallel sampling test â€” stale,v1 â€” by sethkimmel3 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21506](https://github.com/vllm-project/vllm/pull/21506) [V1][SpecDecode]Support relaxed acceptance for thinking tokens in speculative decoding in V1 â€” documentation,rocm,frontend,ci/build,stale,v1,multi-modality,tool-calling â€” by DW934 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22238](https://github.com/vllm-project/vllm/pull/22238) [V1][SpecDecode]Support Relaxed Acceptance for thinking tokens in speculative decoding when using greedy search, camp up by Nvidia. â€” stale,v1 â€” by DW934 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22488](https://github.com/vllm-project/vllm/pull/22488) Feat/sliding window metrics â€” Related to #22480 â€” needs-rebase,stale,v1 â€” by NumberWan (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22632](https://github.com/vllm-project/vllm/pull/22632) [Bugfix]fix deepseek_r1_reasoning bugs when <think> </think> in contents. â€” stale,deepseek â€” by z2415445508 (å…³é—­äº: 2025-12-10 10:23 (UTC+8))