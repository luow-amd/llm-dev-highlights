# vLLM å¼€å‘åŠ¨æ€æŠ¥å‘Š - 2025-12-09

> æ—¶é—´çª—å£: 2025-12-09 21:37 (UTC+8) ~ 2025-12-10 21:37 (UTC+8)
> æ•°æ®ç»Ÿè®¡: æ–° Issue 17 | å…³é—­ Issue 19 | æ–° PR 37 | åˆå¹¶ PR 37 | å…³é—­æœªåˆå¹¶ PR 23

---

### ğŸ“Š æ¯æ—¥å¼€å‘çŠ¶æ€æ‘˜è¦
åœ¨æœ¬æ¬¡è§‚å¯Ÿçª—å£å†…ï¼ŒvLLM é¡¹ç›®ä¿æŒäº†æé«˜çš„å¼€å‘æ´»è·ƒåº¦ï¼Œæ–°å¢ä¸åˆå¹¶çš„ PR æ•°é‡å‡ä¸º 37 ä¸ªï¼Œåæ˜ äº†å¿«é€Ÿçš„è¿­ä»£ä¸é—®é¢˜ä¿®å¤èŠ‚å¥ã€‚å¼€å‘ç„¦ç‚¹é›†ä¸­åœ¨ **AMD å¹³å°å…¼å®¹æ€§ä¼˜åŒ–ã€DeepSeek-V3.2 æ¨¡å‹çš„ç³»åˆ—é—®é¢˜ä¿®å¤ã€ä»¥åŠ CPU/TPU åç«¯çš„åŠŸèƒ½å¢å¼º** ä¸Šã€‚å¤šä¸ªæŠ€æœ¯è®¨è®ºï¼ˆRFCï¼‰çš„æå‡ºä¹Ÿè¡¨æ˜ç¤¾åŒºæ­£åœ¨å°±æ¶æ„æ¼”è¿›ï¼ˆå¦‚åœ¨çº¿é‡åŒ–ã€åŸºå‡†æµ‹è¯•å·¥å…·ï¼‰è¿›è¡Œæ·±å…¥æ¢è®¨ã€‚

### ğŸ¯ AMD/ROCm ç”Ÿæ€ç›¸å…³åŠ¨æ€
æœ¬å‘¨æœŸ AMD ç”Ÿæ€ç›¸å…³æ´»åŠ¨é¢‘ç¹ï¼Œæ¶‰åŠå…¼å®¹æ€§ä¿®å¤ã€æ€§èƒ½ä¼˜åŒ–å’Œ CI æ”¹è¿›ã€‚

| ç¼–å· | ç±»å‹ | æ ‡é¢˜ | å…³é”®è´¡çŒ®è€… | æ ¸å¿ƒå†…å®¹ä¸å½±å“åˆ†æ |
| :--- | :--- | :--- | :--- | :--- |
| **#30360** | **Issue** | [RFC]: Consolidate FP8 min/max values into somewhere reasonable (Python only) | `rasmith` | **ã€è®¾è®¡ææ¡ˆã€‘** æŒ‡å‡ºåœ¨ MI300ï¼ˆä½¿ç”¨ `torch.float8_e4m3fnuz`ï¼‰ç­‰å¹³å°ä¸Šï¼Œå»ºè®®çš„ FP8 æœ€å°/æœ€å¤§å€¼ï¼ˆÂ±224ï¼‰ä¸ PyTorch é»˜è®¤å€¼ï¼ˆÂ±240ï¼‰å­˜åœ¨å†²çªï¼Œå¯¼è‡´æµ‹è¯•å¤±è´¥ã€‚ææ¡ˆæ—¨åœ¨å°† FP8 çš„æ¨èå€¼ç»Ÿä¸€é›†ä¸­ç®¡ç†ï¼Œä»¥è§£å†³å¹³å°å·®å¼‚å¹¶é˜²æ­¢æœªæ¥é—®é¢˜å¤å‘ã€‚è¿™æ˜¯æå‡ AMD å¹³å° FP8 é‡åŒ–ç²¾åº¦å’Œä»£ç å¥å£®æ€§çš„é‡è¦ä¸€æ­¥ã€‚ |
| **#30361** | **PR** | [Attention][AMD] Make flash-attn optional | `mgehre-amd` | **ã€å…¼å®¹æ€§ä¿®å¤ã€‘** ä¿®å¤äº† ROCm å¹³å°ä¸Šä¸€ä¸ªé˜»å¡æ€§é—®é¢˜ï¼šç”±äº `vllm.v1.spec_decode.eagle` æ— æ¡ä»¶å¯¼å…¥ `flash-attn` å·¥å…·æ¨¡å—ï¼Œå¯¼è‡´å³ä½¿è¿è¡Œé Eagle æ¨ç†ä»»åŠ¡ï¼ˆä¸”ä½¿ç”¨ Triton ç­‰åç«¯ï¼‰ï¼Œåªè¦æœªå®‰è£… `flash-attn` å°±ä¼šå¯åŠ¨å¤±è´¥ã€‚æ­¤ PR ä½¿å¯¼å…¥å˜ä¸ºæ¡ä»¶æ€§ï¼Œå¢å¼ºäº† ROCm ç¯å¢ƒä¸‹ä¾èµ–ç®¡ç†çš„çµæ´»æ€§ã€‚ |
| **#30364** | **PR** | [Bugfix] awq_gemm: fix argument order swap | `mgehre-amd` | **ã€ä»£ç æ¸…ç†ã€‘** ä¿®æ­£äº† `_custom_ops.awq_gemm` å‡½æ•°å†…éƒ¨å‚æ•° `scales` å’Œ `zeros` çš„å£°æ˜ä¸ä¼ é€’é¡ºåºï¼Œä½¿å…¶ä¸è°ƒç”¨æ–¹åŠ CUDA å†…æ ¸çš„å®ç°ä¿æŒä¸€è‡´ã€‚æ­¤ä¿®å¤ä¸å½±å“åŠŸèƒ½ï¼Œä½†æé«˜äº†ä»£ç å¯è¯»æ€§å’Œç»´æŠ¤æ€§ï¼Œå¯¹ AMD å¹³å° AWQ é‡åŒ–çš„å¼€å‘æœ‰ç§¯ææ„ä¹‰ã€‚ |
| **#30308** | **PR (å·²åˆå¹¶)** | [bugfix][quantization] fix quark qwen3 kv_cache quantization | `haoyangli-amd` | **ã€Quark é‡åŒ–ä¿®å¤ã€‘** è§£å†³äº† Qwen3 MoE æ¨¡å‹ä½¿ç”¨ Quark é‡åŒ–æ—¶ï¼ŒKV Cache ç¼©æ”¾å› å­è¯†åˆ«é”™è¯¯çš„é—®é¢˜ã€‚é€šè¿‡è°ƒç”¨åŸºç±»çš„ `get_cache_scale` æ–¹æ³•è¿›è¡Œæ­£ç¡®è¯†åˆ«ï¼Œç¡®ä¿äº†æ¨¡å‹æ¨ç†çš„å‡†ç¡®æ€§ã€‚è¿™æ˜¯ **AMD Quark é‡åŒ–å·¥å…·** å¯¹å¤æ‚æ¨¡å‹æ”¯æŒçš„é‡è¦å®Œå–„ã€‚ |
| **#29937** | **PR (å·²åˆå¹¶)** | Improve wvsplitK tile and balance heuristics. | `amd-hhashemi` | **ã€æ€§èƒ½ä¼˜åŒ–ã€‘** æ”¹è¿›äº†é’ˆå¯¹â€œç˜¦â€GEMMï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰è¿ç®—çš„ tile å¤§å°é€‰æ‹©å’Œè´Ÿè½½å‡è¡¡å¯å‘å¼ç®—æ³•ã€‚è¯¥ä¼˜åŒ–åŸºäºå¯¹ Llama3.1/3.3 ç­‰å¤§å‹æ¨¡å‹çš„å®é™…æ€§èƒ½åˆ†æï¼Œæ—¨åœ¨æå‡ AMD ç¡¬ä»¶ä¸Šçš„è®¡ç®—å†…æ ¸æ•ˆç‡ã€‚ |
| **#25552** | **PR (å·²åˆå¹¶)** | [ROCm] Aiter Quant Kernels | `vllmellm` | **ã€æ€§èƒ½ä¼˜åŒ–ã€‘** é›†æˆäº† Aiter çš„ FP8 é‡åŒ–å†…æ ¸ï¼Œæ”¯æŒé™æ€/åŠ¨æ€å¼ é‡é‡åŒ–å’ŒåŠ¨æ€ token é‡åŒ–ã€‚åˆå¹¶åçš„æ€§èƒ½æ•°æ®æ˜¾ç¤ºï¼Œåœ¨ Llama-3.1-70B-Instruct-FP8-KV ç­‰æ¨¡å‹ä¸Šï¼Œæ€»ååé‡ï¼ˆtok/sï¼‰æœ‰æ˜¾è‘—æå‡ï¼ˆ~3%ï¼‰ï¼Œæ˜¯ AMD ROCm å¹³å°é‡åŒ–æ¨ç†æ€§èƒ½çš„å…³é”®å¢å¼ºã€‚ |
| **#25693** | **PR (å·²åˆå¹¶)** | [Rocm][torch.compile] Adding layernorm + fp8 block quant and silu + fp8 block quant for Aiter | `charlifu` | **ã€æ€§èƒ½ä¼˜åŒ–ã€‘** ä¸º Aiter æ·»åŠ äº† LayerNorm + FP8 å—é‡åŒ–ä»¥åŠ SiLU + FP8 å—é‡åŒ–çš„èåˆç®—å­ã€‚é€šè¿‡ç®—å­èåˆå‡å°‘å†…å­˜è®¿é—®å’Œå†…æ ¸å¯åŠ¨å¼€é”€ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æå‡ AMD å¹³å°ä¸Šé‡åŒ–æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚ |
| **#28314** | **Issue (å·²å…³é—­)** | [AMD][CI Failure]: Tracking failure for AMD CI Dependencies & Environments | `zhewenl` | **ã€CI ä¿®å¤ã€‘** æ­¤è·Ÿè¸ª AMD CI ç¯å¢ƒä¾èµ–é—®é¢˜çš„ Issue å·²è¢«å…³é—­ï¼Œè¡¨æ˜ UVã€torchaudioã€terratorchã€pqdm/num2words ç­‰ä¾èµ–é—®é¢˜å·²é€šè¿‡å¤šä¸ª PR å¾—åˆ°è§£å†³ï¼ŒAMD CI ç¯å¢ƒçš„ç¨³å®šæ€§å’Œå®Œæ•´æ€§å¾—åˆ°æ˜¾è‘—æ”¹å–„ã€‚ |
| **#30020** | **PR (å·²åˆå¹¶)** | [CI/Build][AMD] Skip quantization kernels tests that require CUTLASS or e4m3fn when not supported by platform | `rasmith` | **ã€CI ä¿®å¤ã€‘** ä½¿æµ‹è¯•æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å¹³å°èƒ½åŠ›ï¼Œè·³è¿‡ MI300 ç­‰ä¸æ”¯æŒçš„æµ‹è¯•ï¼ˆå¦‚éœ€è¦ CUTLASS æˆ– e4m3fn æ ¼å¼çš„æµ‹è¯•ï¼‰ï¼Œé™ä½äº† AMD CI çš„å™ªéŸ³å’Œå¤±è´¥ç‡ï¼Œæé«˜äº†æµ‹è¯•çš„é’ˆå¯¹æ€§ã€‚ |

### ğŸ’¬ é«˜çƒ­åº¦è®¨è®ºåˆ†æ
1.  **Issue #30358: [Bug]: Prefill scheduled num_block mismatch**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šåœ¨åˆ†å—é¢„å¡«å……ï¼ˆchunked prefillï¼‰åœºæ™¯ä¸‹ï¼Œè°ƒåº¦å™¨åˆ†é…ç»™è¯·æ±‚çš„ KV å—æ•°é‡åœ¨åˆå§‹åˆ†é… (`update_state_after_alloc`) å’Œæœ€ç»ˆå®Œæˆ (`request_finished`) ä¸¤ä¸ªé˜¶æ®µä¸ä¸€è‡´ï¼Œå¯èƒ½å¯¼è‡´åˆ†å¸ƒå¼ KV è¿æ¥å™¨å…ƒæ•°æ®ä¸åŒæ­¥ã€‚
    *   **è§‚ç‚¹ä¸è¿›å±•**ï¼š
        *   **æŠ¥å‘Šè€… (`xuechendi`)**ï¼šé€šè¿‡è¯¦å°½çš„æ—¥å¿—åˆ†æï¼Œå®šä½åˆ°é—®é¢˜æºäºè°ƒåº¦å™¨åœ¨åç»­å¾ªç¯ä¸­æ›´æ–°äº†è¯·æ±‚çš„å—IDåˆ—è¡¨ï¼Œä½†æœªåŒæ­¥é€šçŸ¥ KV è¿æ¥å™¨ã€‚
        *   **ç»“è®º**ï¼šæŠ¥å‘Šè€…å·²æ‰¾åˆ°æ ¹æœ¬åŸå› ï¼ˆä»£ç é“¾æ¥ï¼‰ï¼Œå¹¶è®¡åˆ’æäº¤ä¿®å¤ã€‚è¿™æ˜¯ä¸€ä¸ªæ·±å…¥çš„æŠ€æœ¯è°ƒè¯•æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†ç¤¾åŒºè´¡çŒ®è€…å¼ºå¤§çš„é—®é¢˜å®šä½èƒ½åŠ›ã€‚
    *   **å½“å‰çŠ¶æ€**ï¼š`open`ï¼Œç­‰å¾…ä¿®å¤ PRã€‚

2.  **Issue #15636: [Bug]: Outlines broken on vLLM 0.8+ (å·²å…³é—­)**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šV1 å¼•æ“é»˜è®¤å¯ç”¨åï¼Œä¸å†æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰çš„ logits processorsï¼ˆå¦‚ Outlines åº“æ‰€éœ€ï¼‰ï¼Œå¯¼è‡´å¤§é‡ç”¨æˆ·å·¥ä½œæµæ–­è£‚ã€‚
    *   **ä¸åŒè§‚ç‚¹**ï¼š
        *   **ç”¨æˆ·ç¾¤ä½“**ï¼šè¡¨è¾¾äº†å¼ºçƒˆä¸æ»¡ï¼Œè®¤ä¸ºåœ¨ V1 å¼•æ“åŠŸèƒ½æœªå®Œå…¨å¯¹é½ V0 æ—¶å°±é»˜è®¤åˆ‡æ¢æ˜¯â€œç³Ÿç³•çš„å†³ç­–â€ï¼Œç»™é›†æˆå¸¦æ¥äº†å›°éš¾å’Œä¸ä¾¿ã€‚
        *   **ç»´æŠ¤è€… (`simon-mo`)**ï¼šæ‰¿è®¤å†³ç­–å¤±è¯¯å¹¶é“æ­‰ï¼Œè§£é‡Šç”±äºæ— æ³•æŒ‰è¯·æ±‚ç²’åº¦è¿›è¡Œå›é€€ï¼Œå¯¼è‡´æ— æ³•ä¼˜é›…é™çº§ã€‚
        *   **ç»´æŠ¤è€… (`russellb, mgoin`)**ï¼šæä¾›äº†ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼ˆè®¾ç½® `VLLM_USE_V1=0` å›é€€åˆ° V0 å¼•æ“ï¼‰ï¼Œå¹¶æŒ‡å‡º V1 ä¸­åº”ä½¿ç”¨å†…ç½®çš„ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½ã€‚
    *   **äº‰è®®ç„¦ç‚¹**ï¼šåŠŸèƒ½è¿­ä»£çš„æ¿€è¿›ç¨‹åº¦ä¸å‘åå…¼å®¹æ€§ã€ç”¨æˆ·è¿ç§»æˆæœ¬ä¹‹é—´çš„å¹³è¡¡ã€‚
    *   **æœ€ç»ˆç»“è®º**ï¼šIssue åœ¨é•¿æœŸè®¨è®ºåè¢«å…³é—­ï¼Œä½†æ­ç¤ºäº†é¡¹ç›®åœ¨é‡å¤§æ¶æ„å‡çº§æ—¶é¢ä¸´çš„å…¼å®¹æ€§æŒ‘æˆ˜ã€‚

3.  **PR #30062: [CPU] Support for Whisper**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šä¸º CPU åç«¯æ·»åŠ  Whisper è¯­éŸ³æ¨¡å‹æ”¯æŒã€‚
    *   **å…³é”®è®¨è®º**ï¼š
        *   **Codex å®¡æŸ¥**ï¼šæŒ‡å‡ºä¸€ä¸ª **P1 çº§åˆ«ä¸¥é‡é—®é¢˜**â€”â€”CPU æ³¨æ„åŠ›åç«¯åœ¨å¤„ç†ç¼–ç å™¨-è§£ç å™¨ï¼ˆå¦‚ Whisperï¼‰çš„äº¤å‰æ³¨æ„åŠ›æ—¶ï¼Œé”™è¯¯åœ°ä½¿ç”¨äº†å› æœæ©ç ï¼ˆcausal maskï¼‰ï¼Œè¿™å°†å¯¼è‡´è§£ç å™¨åªèƒ½çœ‹åˆ°éƒ¨åˆ†ç¼–ç å™¨è®°å¿†ï¼Œäº§ç”Ÿé”™è¯¯è¾“å‡ºã€‚
        *   **ç»´æŠ¤è€…äº’åŠ¨**ï¼šå®¡æŸ¥æ„è§å¾—åˆ°äº†é‡è§†å’Œè®¨è®ºï¼ŒPR åœ¨ç»è¿‡å¤šè½® CI æµ‹è¯•å’Œä»£ç ä¿®æ­£åæœ€ç»ˆåˆå¹¶ã€‚
    *   **ç»“è®º**ï¼šæ­¤ PR çš„åˆå¹¶æ ‡å¿—ç€ CPU åç«¯å¤šæ¨¡æ€æ”¯æŒçš„é‡è¦æ‰©å±•ï¼ŒåŒæ—¶å±•ç¤ºäº†è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥å·¥å…·åœ¨æ•æ‰æ·±å±‚é€»è¾‘é”™è¯¯ä¸Šçš„ä»·å€¼ã€‚

4.  **PR #30346: [Core] Major fix catch backend grammar exceptions (xgrammar, outlines, etc) in scheduler**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šå½“ç”¨æˆ·ä¼ å…¥ç•¸å½¢æˆ–ä¸æ”¯æŒçš„ JSON schema æ—¶ï¼Œxgrammar ç­‰ç»“æ„åŒ–è¾“å‡ºåç«¯ä¼šæŠ›å‡ºæœªæ•è·çš„å¼‚å¸¸ï¼Œå¯¼è‡´æ•´ä¸ª vLLM æœåŠ¡è¿›ç¨‹å´©æºƒã€‚
    *   **ä¸åŒè§‚ç‚¹**ï¼š
        *   **è´¡çŒ®è€… (`blancsw`)**ï¼šæå‡ºåœ¨è°ƒåº¦å™¨å±‚é¢æ•è·è¿™äº›å¼‚å¸¸ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯¹å®¢æˆ·ç«¯çš„é”™è¯¯å“åº”ï¼Œé¿å…æœåŠ¡é‡å¯ï¼Œæå‡æœåŠ¡éŸ§æ€§ã€‚
        *   **æ ¸å¿ƒå¼€å‘è€… (`wseaton`)**ï¼šæŒ‡å‡ºâ€œabortâ€æœ¯è¯­åœ¨ vLLM ä¸­é€šå¸¸ç‰¹æŒ‡å®¢æˆ·ç«¯ä¸­æ­¢ï¼Œå»ºè®®è¯¥é”™è¯¯å¤„ç†æœºåˆ¶å¯ä»¥å‚è€ƒå…¶æ­£åœ¨å¼€å‘çš„ #26813 PR ä¸­çš„æ–°è°ƒåº¦å™¨/å¼•æ“å†…éƒ¨é”™è¯¯å¤„ç†æ¡†æ¶ï¼Œä»¥å®ç°æ›´ç»Ÿä¸€çš„è®¾è®¡ã€‚
    *   **å½“å‰çŠ¶æ€**ï¼šè®¨è®ºæŒ‡å‘ä¸€ä¸ªæ›´æ¶æ„åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥ PR å¯èƒ½éœ€è¦è¿›è¡Œè°ƒæ•´æˆ–ä¸å¦ä¸€ä¸ª PR åä½œã€‚

### ğŸ”¥ çƒ­é—¨è¯é¢˜ä¸è¶‹åŠ¿åˆ†æ
1.  **DeepSeek-V3.2 æ¨¡å‹é—®é¢˜é›†ä¸­æ¶Œç°**ï¼šæˆä¸ºè¿‘æœŸæœ€çƒ­é—¨çš„æ”¯æŒä¸»é¢˜ã€‚ç›¸å…³ Issue/PR æ¶‰åŠåˆ†è¯å™¨æ€§èƒ½ï¼ˆ#30351 ç¼“å­˜ä¼˜åŒ–ï¼‰ã€ç»“æ„åŒ–è¾“å‡ºæ”¯æŒï¼ˆ#30371 ä¿®å¤ï¼‰ã€AWQ æ€§èƒ½ï¼ˆ#30370ï¼‰ã€å·¥å…·è°ƒç”¨è§£æï¼ˆ#30311ï¼‰ç­‰ï¼Œåæ˜ äº†æ–°é”å¤§æ¨¡å‹ä¸æ¨ç†å¼•æ“å¿«é€Ÿé€‚é…è¿‡ç¨‹ä¸­çš„å…¸å‹é˜µç—›æœŸã€‚
2.  **é‡åŒ–æŠ€æœ¯æŒç»­æ·±åŒ–**ï¼šFP8 é‡åŒ–æ˜¯ç„¦ç‚¹ï¼Œæ—¢æœ‰é’ˆå¯¹ AMD å¹³å°çš„æ ‡å‡†å€¼ç»Ÿä¸€ï¼ˆ#30360ï¼‰ï¼Œä¹Ÿæœ‰ Quark å·¥å…·å¯¹ Qwen3 çš„ä¿®å¤ï¼ˆ#30308ï¼‰ï¼Œä»¥åŠåœ¨çº¿é‡åŒ–é‡è½½çš„æ¶æ„è®¾è®¡è®¨è®ºï¼ˆ#30359ï¼‰ï¼Œæ˜¾ç¤ºå‡ºé‡åŒ–åœ¨æå‡æ¨ç†æ•ˆç‡æ–¹é¢çš„æ ¸å¿ƒåœ°ä½ã€‚
3.  **CPU ä¸ TPU åç«¯ç¨³æ­¥æ¨è¿›**ï¼šCPU åç«¯æ–°å¢ Whisper æ”¯æŒï¼ˆ#30062ï¼‰å¹¶è¯·æ±‚æ·»åŠ  Attention åŸºå‡†æµ‹è¯•ï¼ˆ#30374ï¼‰ã€‚TPU åç«¯ä¿®å¤äº† PyTorch 2.9.1 å‡çº§å¼•å‘çš„ç¼–è¯‘é”™è¯¯ï¼ˆ#30331ï¼‰ã€‚è¿™è¡¨æ˜ vLLM åœ¨å¤šç¡¬ä»¶å¹³å°æ”¯æŒä¸Šçš„æˆ˜ç•¥å¸ƒå±€ã€‚
4.  **å¤šæ¨¡æ€ä¸è§†è§‰æ¨¡å‹æ”¯æŒ**ï¼šå‡ºç° HunyuanOCR æ‰¹å¤„ç†å›¾åƒæ±¡æŸ“é—®é¢˜ï¼ˆ#30342/30344ï¼‰å’Œ LMDB å¤šæ¨¡æ€ç¼“å­˜å®ç°ï¼ˆ#30373ï¼‰ï¼Œè¡¨æ˜è§†è§‰è¯­è¨€æ¨¡å‹çš„åº”ç”¨åœ¨å¢åŠ ï¼Œå¯¹æ¨ç†å¼•æ“çš„æ‰¹å¤„ç†å’Œç¼“å­˜æœºåˆ¶æå‡ºäº†æ–°éœ€æ±‚ã€‚

### ğŸ› ï¸ é‡ç‚¹æŠ€æœ¯å˜æ›´
1.  **PR #30351: [Bugfix] Cache added_vocab to avoid per-token overhead (å·²åˆå¹¶)**ï¼š
    *   **æŠ€æœ¯è§£è¯»**ï¼šDeepSeek-V3.2 åˆ†è¯å™¨çš„ `__len__` æ–¹æ³•æ¯æ¬¡è°ƒç”¨éƒ½ä¼šè®¡ç®—æ–°å¢è¯æ±‡è¡¨ï¼Œè¯¥æ“ä½œåœ¨é€ Token è§£ç çš„ä¸»çº¿ç¨‹ä¸­æˆä¸ºç“¶é¢ˆï¼Œå¯¼è‡´æœåŠ¡å»¶è¿Ÿç”šè‡³å¡æ­»ã€‚
    *   **å½±å“**ï¼šé€šè¿‡é¢„è®¡ç®—å¹¶ç¼“å­˜ `added_vocab`ï¼Œå½»åº•æ¶ˆé™¤äº†è¿™ä¸ªæ€§èƒ½çƒ­ç‚¹ï¼Œæ˜¾è‘—æå‡äº† DeepSeek-V3.2 æ¨¡å‹åœ¨é«˜å¹¶å‘ä¸‹çš„æœåŠ¡ç¨³å®šæ€§å’Œå“åº”èƒ½åŠ›ã€‚

2.  **PR #30371: [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output (å·²åˆå¹¶)**ï¼š
    *   **æŠ€æœ¯è§£è¯»**ï¼šä¿®å¤äº† DeepSeek-V3.2 æ¨¡å‹æ— æ³•å¯ç”¨ç»“æ„åŒ–è¾“å‡ºï¼ˆå¦‚ JSON Schemaã€Grammar çº¦æŸï¼‰çš„é—®é¢˜ã€‚
    *   **å½±å“**ï¼šè§£ç¦äº† DeepSeek-V3.2 æ¨¡å‹åœ¨éœ€è¦ä¸¥æ ¼è¾“å‡ºæ ¼å¼æ§åˆ¶åœºæ™¯ä¸‹çš„åº”ç”¨èƒ½åŠ›ï¼Œæ˜¯å…¶åŠŸèƒ½æ”¯æŒå®Œæ•´æ€§çš„å…³é”®è¡¥å…¨ã€‚

3.  **PR #30384/#30349: Fix minimax m2 model rotary_dim (å·²åˆå¹¶/å…³é—­)**ï¼š
    *   **æŠ€æœ¯è§£è¯»**ï¼šPR #29966 ç»Ÿä¸€ RoPE ç»´åº¦è®¡ç®—é€»è¾‘åï¼Œå¼•å‘äº† Minimax-M2 ç­‰æ¨¡å‹å›  `rotary_dim` è¯­ä¹‰æ··æ·†ï¼ˆå·²ç¼©æ”¾ vs å¾…ç¼©æ”¾ï¼‰è€Œå¯¼è‡´çš„è¾“å‡ºä¹±ç é—®é¢˜ã€‚PR #30384 é€šè¿‡å…è®¸ `get_rope` å‡½æ•°è¯†åˆ«å¹¶è·³è¿‡å·²ç¼©æ”¾çš„ç»´åº¦æ¥ä¿®å¤ã€‚
    *   **å½±å“**ï¼šæ¢å¤äº†å—å½±å“æ¨¡å‹çš„æ­£å¸¸æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼•å‘äº†å¯¹ `rotary_dim` å‚æ•°è¿›è¡Œå…¨å±€æ ‡å‡†åŒ–é‡æ„çš„è®¨è®ºï¼ˆè§ PR #30389ï¼‰ï¼Œæœ‰åŠ©äºç»Ÿä¸€ä»£ç é€»è¾‘ã€‚

4.  **PR #30361: [Attention][AMD] Make flash-attn optional (è¿›è¡Œä¸­)**ï¼š
    *   **æŠ€æœ¯è§£è¯»**ï¼šå°† ROCm å¹³å°å¯¹ä¸Šæ¸¸ `flash-attn` çš„å¼ºä¾èµ–æ”¹ä¸ºå¼±ä¾èµ–ï¼Œå…è®¸ç”¨æˆ·åœ¨æœªå®‰è£…è¯¥åŒ…æ—¶ä½¿ç”¨å…¶ä»–æ³¨æ„åŠ›åç«¯ã€‚
    *   **å½±å“**ï¼šæé«˜äº† AMD å¹³å°éƒ¨ç½²çš„çµæ´»æ€§ï¼Œé™ä½äº†ä¾èµ–ç®¡ç†çš„å¤æ‚åº¦å’Œæ½œåœ¨å†²çªã€‚

5.  **PR #30062: [CPU] Support for Whisper (å·²åˆå¹¶)**ï¼š
    *   **æŠ€æœ¯è§£è¯»**ï¼šä¸º CPU æ¨ç†åç«¯å®ç°äº† Whisper è¯­éŸ³ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„æ”¯æŒï¼ŒåŒ…æ‹¬å¤„ç†äº¤å‰æ³¨æ„åŠ›çš„æ­£ç¡®æ©ç é€»è¾‘ã€‚
    *   **å½±å“**ï¼šå¤§å¹…æ‰©å±•äº† vLLM åœ¨æ—  GPU ç¯å¢ƒä¸‹çš„åº”ç”¨åœºæ™¯ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è¯­éŸ³è½¬å½•ä»»åŠ¡ã€‚

### ğŸ“ˆ å¼€å‘æ´»è·ƒåº¦è§‚å¯Ÿ
*   **AMD è´¡çŒ®è€…é«˜åº¦æ´»è·ƒ**ï¼š`-amd` åç¼€çš„è´¡çŒ®è€…åœ¨æœ¬å‘¨æœŸæäº¤äº†å¤šä¸ªå…³é”®ä¿®å¤ï¼ˆQuarkã€FP8ã€CIï¼‰ï¼Œæ˜¾ç¤ºå‡º AMD å›¢é˜Ÿåœ¨ç§¯ææ¨åŠ¨å…¶ç¡¬ä»¶ç”Ÿæ€ä¸ vLLM çš„æ·±åº¦é›†æˆï¼Œä»åŠŸèƒ½ã€æ€§èƒ½åˆ°æµ‹è¯•çš„å…¨é¢è¦†ç›–ã€‚
*   **å¿«é€Ÿå“åº”ä¸ä¿®å¤**ï¼šé’ˆå¯¹ DeepSeek-V3.2 å’Œ Minimax-M2 ç­‰æ¨¡å‹çš„å…·ä½“é—®é¢˜ï¼Œç¤¾åŒºèƒ½åœ¨çŸ­æ—¶é—´å†…å®šä½æ ¹æºå¹¶æä¾›ä¿®å¤ï¼Œå±•ç°äº†é¡¹ç›®å¯¹ä¸»æµæ¨¡å‹é€‚é…çš„å¿«é€Ÿå“åº”èƒ½åŠ›ã€‚
*   **ç¤¾åŒºæ·±åº¦å‚ä¸**ï¼šå¦‚ Issue #30358 æ‰€ç¤ºï¼Œè´¡çŒ®è€…èƒ½å¤Ÿè¿›è¡Œå¤æ‚è°ƒåº¦é€»è¾‘çš„è°ƒè¯•å¹¶æå‡ºå…·ä½“ä¿®å¤æ–¹æ¡ˆï¼Œè¯´æ˜ vLLM ç¤¾åŒºå…·å¤‡å¼ºå¤§çš„æŠ€æœ¯ä¸“å®¶ç¾¤ä½“ã€‚

### ğŸ’¡ å€¼å¾—å…³æ³¨çš„é—®é¢˜
1.  **Issue #30359: [RFC] [QeRL]: Online Quantization and Model Reloading**ï¼š
    *   è¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ¶æ„è®¾è®¡ææ¡ˆï¼Œæ—¨åœ¨ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ç­‰åœºæ™¯ä¸­é‡åŒ–æ¨¡å‹çš„åœ¨çº¿é‡è½½æµç¨‹ï¼Œè§£å†³å½“å‰å®ç°ä¸­å†…å­˜å ç”¨é«˜ã€æ”¯æŒæœ‰é™çš„é—®é¢˜ã€‚è®¨è®ºå°†å½±å“ vLLM åœ¨è®­ç»ƒåå¤„ç†ï¼ˆRLHF/Pipelineï¼‰é¢†åŸŸçš„åº”ç”¨æ•ˆèƒ½ï¼Œå€¼å¾—é‡åŒ–åŠæ ¸å¿ƒæ¶æ„å¼€å‘è€…å…³æ³¨ã€‚

2.  **Issue #30358: Prefill scheduled num_block mismatch**ï¼š
    *   è™½ç„¶æ ¹å› å·²æ‰¾åˆ°ï¼Œä½†è¯¥ Bug æ­ç¤ºäº†åœ¨å¤æ‚è°ƒåº¦å’Œåˆ†å¸ƒå¼ KV ç®¡ç†åœºæ™¯ä¸‹ï¼ŒçŠ¶æ€åŒæ­¥çš„è„†å¼±æ€§ã€‚å…¶æœ€ç»ˆä¿®å¤æ–¹æ¡ˆéœ€è¦è°¨æ…è®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨æ‰€æœ‰è¾¹ç¼˜æƒ…å†µä¸‹çš„ä¸€è‡´æ€§ã€‚

3.  **Issue #30383: [RFC]: Multi-Process Benchmark Architecture**ï¼š
    *   æŒ‡å‡ºäº†å½“å‰ `vllm benchmark` å·¥å…·åœ¨å•è¿›ç¨‹è´Ÿè½½ç”Ÿæˆå™¨ä¸‹çš„æ€§èƒ½ç“¶é¢ˆï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°é«˜å¹¶å‘æœåŠ¡çš„çœŸå®æ€§èƒ½ã€‚è¯¥ææ¡ˆæ—¨åœ¨è®¾è®¡ä¸€ä¸ªå¤šè¿›ç¨‹æ¶æ„ï¼Œå¯¹äºæ€§èƒ½è¯„æµ‹çš„å…¬æ­£æ€§å’Œå¯é æ€§è‡³å…³é‡è¦ï¼Œå°¤å…¶å½±å“å¤§å‹æœåŠ¡ç³»ç»Ÿçš„é€‰å‹è¯„ä¼°ã€‚

---

## ğŸ“‹ é™„å½•ï¼šè¯¦ç»†æ•°æ®åˆ—è¡¨

### æ–°å¢ Issue
- [#30380](https://github.com/vllm-project/vllm/issues/30380) [Usage]: å¤§å®¶ä¸€èˆ¬æ€ä¹ˆä½¿ç”¨vllm/testsçš„ï¼Ÿ â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:27 (UTC+8))
- [#30359](https://github.com/vllm-project/vllm/issues/30359) [RFC] [QeRL]: Online Quantization and Model Reloading â€” RFC â€” by kylesayrs (åˆ›å»ºäº: 2025-12-10 05:24 (UTC+8))
- [#30387](https://github.com/vllm-project/vllm/issues/30387) [Bug]: illegal memory access countered when using kv-cache-type=fp8  loading  a weight-fp8 model for evaltest  in flash-attn backend â€” bug â€” by youngze0016 (åˆ›å»ºäº: 2025-12-10 19:43 (UTC+8))
- [#30382](https://github.com/vllm-project/vllm/issues/30382) [Bug]: Issues with mistralai/Ministral-3-14B-Instruct-2512 â€” bug â€” by eltorre (åˆ›å»ºäº: 2025-12-10 17:55 (UTC+8))
- [#30383](https://github.com/vllm-project/vllm/issues/30383) [RFC]: Multi-Process Benchmark Architecture for Scaling Beyond Single-Core Limits â€” RFC â€” by GaoHuaZhang (åˆ›å»ºäº: 2025-12-10 18:02 (UTC+8))
- [#30374](https://github.com/vllm-project/vllm/issues/30374) [Feature][CPU Backend]: Add Paged Attention Benchmarks for CPU backend â€” feature request,cpu â€” by fadara01 (åˆ›å»ºäº: 2025-12-10 15:53 (UTC+8))
- [#30381](https://github.com/vllm-project/vllm/issues/30381) [Usage]: â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:27 (UTC+8))
- [#30379](https://github.com/vllm-project/vllm/issues/30379) [Usage]: how to use vllm/tests/ï¼Ÿ â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:25 (UTC+8))
- [#30378](https://github.com/vllm-project/vllm/issues/30378) [Feature]: Automatically infer Qwen3 reranker settings (remove need for hf_overrides) â€” feature request â€” by ilopezluna (åˆ›å»ºäº: 2025-12-10 17:21 (UTC+8))
- [#30375](https://github.com/vllm-project/vllm/issues/30375) [Bug]: [TPU] ShapeDtypeStruct error when loading custom safetensors checkpoint on TPU v5litepod â€” bug â€” by Baltsat (åˆ›å»ºäº: 2025-12-10 16:12 (UTC+8))
- [#30372](https://github.com/vllm-project/vllm/issues/30372) [Bug]: vLLM (GPT-OSS) causes distorted tool argument names + infinite tool-call loop with Korean messenger tool â€” bug â€” by minmini2 (åˆ›å»ºäº: 2025-12-10 14:59 (UTC+8))
- [#30370](https://github.com/vllm-project/vllm/issues/30370) [Performance]: DeepSeek-V3.2 AWQ Performance is lower then i expected â€” performance â€” by yongho-chang (åˆ›å»ºäº: 2025-12-10 10:45 (UTC+8))
- [#30343](https://github.com/vllm-project/vllm/issues/30343) [Bug]: In DeepSeek-V3.2 tokenizer mode, detokenization saturates the main thread, causing the server to hang â€” bug â€” by scratch-ml (åˆ›å»ºäº: 2025-12-09 22:48 (UTC+8))
- [#30358](https://github.com/vllm-project/vllm/issues/30358) [Bug]: Prefill scheduled num_block mismatch at update_state_after_alloc and request_finished â€” bug â€” by xuechendi (åˆ›å»ºäº: 2025-12-10 04:15 (UTC+8))
- [#30368](https://github.com/vllm-project/vllm/issues/30368) [CI] Test target determination using LLM â€” feature request,ci â€” by khluu (åˆ›å»ºäº: 2025-12-10 09:42 (UTC+8))
- [#30360](https://github.com/vllm-project/vllm/issues/30360) [RFC]: Consolidate FP8 min/max values into somewhere reasonable (Python only) â€” rocm,RFC â€” by rasmith (åˆ›å»ºäº: 2025-12-10 05:44 (UTC+8))
- [#30342](https://github.com/vllm-project/vllm/issues/30342) [Bug]: HunyuanOCR batching problem with variable sized images in a batch. â€” bug â€” by anker-c2 (åˆ›å»ºäº: 2025-12-09 22:22 (UTC+8))

### å·²å…³é—­ Issue
- [#15636](https://github.com/vllm-project/vllm/issues/15636) [Bug]: Outlines broken on vLLM 0.8+ â€” bug,structured-output,unstale â€” by cpfiffer (å…³é—­äº: 2025-12-10 21:18 (UTC+8))
- [#30382](https://github.com/vllm-project/vllm/issues/30382) [Bug]: Issues with mistralai/Ministral-3-14B-Instruct-2512 â€” bug â€” by eltorre (å…³é—­äº: 2025-12-10 18:47 (UTC+8))
- [#30381](https://github.com/vllm-project/vllm/issues/30381) [Usage]: â€” usage â€” by tobeprozy (å…³é—­äº: 2025-12-10 17:28 (UTC+8))
- [#30379](https://github.com/vllm-project/vllm/issues/30379) [Usage]: how to use vllm/tests/ï¼Ÿ â€” usage â€” by tobeprozy (å…³é—­äº: 2025-12-10 17:26 (UTC+8))
- [#30311](https://github.com/vllm-project/vllm/issues/30311) [Bug]: deepseekv32.DeepseekV32Tokenizer Runtime causes model to crash â€” bug â€” by magician-xin (å…³é—­äº: 2025-12-10 16:30 (UTC+8))
- [#28314](https://github.com/vllm-project/vllm/issues/28314) [AMD][CI Failure]: Tracking failure for AMD CI Dependencies & Environments â€” rocm,ci-failure â€” by zhewenl (å…³é—­äº: 2025-12-10 13:32 (UTC+8))
- [#30343](https://github.com/vllm-project/vllm/issues/30343) [Bug]: In DeepSeek-V3.2 tokenizer mode, detokenization saturates the main thread, causing the server to hang â€” bug â€” by scratch-ml (å…³é—­äº: 2025-12-10 12:05 (UTC+8))
- [#20181](https://github.com/vllm-project/vllm/issues/20181) [Feature]: Batch inference for Multi-Modal Online Serving â€” feature request,stale â€” by eslambakr (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#21097](https://github.com/vllm-project/vllm/issues/21097) [Bug]: w8a8 quantization not supporting sm120 â€” bug,stale â€” by sarmiena (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21909](https://github.com/vllm-project/vllm/issues/21909) [Bug]: quant_method is not None â€” bug,stale â€” by maxin9966 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22325](https://github.com/vllm-project/vllm/issues/22325) [Bug]: gpt-oss model crashes on NVIDIA B200 with any OpenAI chat completion request â€” bug,stale â€” by teds-lin (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22422](https://github.com/vllm-project/vllm/issues/22422) [Feature]: mxfp4 support for 3090 â€” feature request,stale â€” by ehartford (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22501](https://github.com/vllm-project/vllm/issues/22501) [Usage]: Running a 300-400B Parameter Model on Multi-Node Setup (2x 8xA100) â€” usage,stale â€” by rangehow (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22575](https://github.com/vllm-project/vllm/issues/22575) [Bug]: Vllm hangs when I use the offline engine with dp = 2 or more â€” bug,stale â€” by Stealthwriter (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22623](https://github.com/vllm-project/vllm/issues/22623) [Usage]: if openai-mirror/gpt-oss-20b  can run in A100? â€” usage,stale â€” by neverstoplearn (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#22624](https://github.com/vllm-project/vllm/issues/22624) [Bug]: 1.7B fp16 + 0.6B draft OOM with gpu_memory_utilization=0.9, while 4B int8 + 0.6B works fine on A800 80 GB â€” bug,stale â€” by kiexu (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#22639](https://github.com/vllm-project/vllm/issues/22639) [Bug]: function convert_lark_to_gbnf interpreting '#' to parse as lark commentaries â€” bug,stale â€” by renout-nicolas (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#30206](https://github.com/vllm-project/vllm/issues/30206) [Bug]: DeepSeek-V3.2 DeepGEMM RuntimeError â€” bug â€” by coval3nte (å…³é—­äº: 2025-12-09 22:55 (UTC+8))
- [#29840](https://github.com/vllm-project/vllm/issues/29840) [Bug]: LMCacheConnectorV1Impl has no attribute 'layerwise_storers' on remote full cache hit with layerwise mode â€” bug â€” by XinyiQiao (å…³é—­äº: 2025-12-10 01:11 (UTC+8))

### æ–°å¢ PR
- [#30390](https://github.com/vllm-project/vllm/pull/30390) fix: Update json features supported by xGrammar â€” structured-output,v1 â€” by johannesflommersfeld (åˆ›å»ºäº: 2025-12-10 20:51 (UTC+8))
- [#30391](https://github.com/vllm-project/vllm/pull/30391) [IMPROVEMENT] Change MistralReasoningParser behavior â€” æ— æ ‡ç­¾ â€” by juliendenize (åˆ›å»ºäº: 2025-12-10 21:02 (UTC+8))
- [#30389](https://github.com/vllm-project/vllm/pull/30389) Standardise `get_rope` to use `rope_parameters["partial_rotary_factor"]`, not `rotary_dim` â€” performance,llama,qwen,deepseek,gpt-oss â€” by hmellor (åˆ›å»ºäº: 2025-12-10 20:33 (UTC+8))
- [#30384](https://github.com/vllm-project/vllm/pull/30384) [BugFix] Fix minimax m2 model rotary_dim â€” ready â€” by rogeryoungh (åˆ›å»ºäº: 2025-12-10 18:37 (UTC+8))
- [#30340](https://github.com/vllm-project/vllm/pull/30340) Add Eagle and Eagle3 support to Transformers modeling backend â€” æ— æ ‡ç­¾ â€” by hmellor (åˆ›å»ºäº: 2025-12-09 22:09 (UTC+8))
- [#30388](https://github.com/vllm-project/vllm/pull/30388) [Docs] Generate full list of metrics in user docs â€” documentation â€” by markmc (åˆ›å»ºäº: 2025-12-10 19:50 (UTC+8))
- [#30386](https://github.com/vllm-project/vllm/pull/30386) [v1] Add PrefixLM support to TritonAttention backend â€” v1 â€” by Isotr0py (åˆ›å»ºäº: 2025-12-10 19:32 (UTC+8))
- [#30385](https://github.com/vllm-project/vllm/pull/30385) [Core] Whisper support `torch.compile` â€” v1 â€” by NickLucche (åˆ›å»ºäº: 2025-12-10 19:11 (UTC+8))
- [#30376](https://github.com/vllm-project/vllm/pull/30376) [Fix]fix import error from lmcache â€” kv-connector â€” by wz1qqx (åˆ›å»ºäº: 2025-12-10 16:38 (UTC+8))
- [#30348](https://github.com/vllm-project/vllm/pull/30348) [Docs]: adds a new metric vllm:request_prefill_kv_computed_tokens in docs â€” documentation â€” by googs1025 (åˆ›å»ºäº: 2025-12-09 23:24 (UTC+8))
- [#30361](https://github.com/vllm-project/vllm/pull/30361) [Attention][AMD] Make flash-attn optional â€” rocm,speculative-decoding,v1 â€” by mgehre-amd (åˆ›å»ºäº: 2025-12-10 06:46 (UTC+8))
- [#30349](https://github.com/vllm-project/vllm/pull/30349) [BugFix] Fix minimax m2 model rope_parameters â€” æ— æ ‡ç­¾ â€” by esmeetu (åˆ›å»ºäº: 2025-12-09 23:42 (UTC+8))
- [#30351](https://github.com/vllm-project/vllm/pull/30351) [Bugfix] Cache added_vocab to avoid per-token overhead â€” ready â€” by scratch-ml (åˆ›å»ºäº: 2025-12-10 00:57 (UTC+8))
- [#30364](https://github.com/vllm-project/vllm/pull/30364) [Bugfix] awq_gemm: fix argument order swap â€” æ— æ ‡ç­¾ â€” by mgehre-amd (åˆ›å»ºäº: 2025-12-10 07:17 (UTC+8))
- [#30377](https://github.com/vllm-project/vllm/pull/30377) adding constraint updates of cos-sin to improve mrope performance â€” æ— æ ‡ç­¾ â€” by wujinyuan1 (åˆ›å»ºäº: 2025-12-10 16:48 (UTC+8))
- [#30373](https://github.com/vllm-project/vllm/pull/30373) Implement LMDB-based multi-modal cache â€” ci/build,v1,multi-modality â€” by petersalas (åˆ›å»ºäº: 2025-12-10 15:21 (UTC+8))
- [#30371](https://github.com/vllm-project/vllm/pull/30371) [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output â€” structured-output,ready,v1,deepseek â€” by chaunceyjiang (åˆ›å»ºäº: 2025-12-10 12:26 (UTC+8))
- [#30344](https://github.com/vllm-project/vllm/pull/30344) [Bugfix] Fix HunyuanOCR cross-image contamination in batch processing â€” æ— æ ‡ç­¾ â€” by anker-c2 (åˆ›å»ºäº: 2025-12-09 22:49 (UTC+8))
- [#30346](https://github.com/vllm-project/vllm/pull/30346) [Core] Major fix catch backend grammar exceptions (xgrammar, outlines, etc) in scheduler â€” v1 â€” by blancsw (åˆ›å»ºäº: 2025-12-09 22:58 (UTC+8))
- [#30347](https://github.com/vllm-project/vllm/pull/30347) [cpu][ci] Add CPU Attention Tests for Neon Backend â€” ready â€” by fadara01 (åˆ›å»ºäº: 2025-12-09 23:14 (UTC+8))
- [#30339](https://github.com/vllm-project/vllm/pull/30339) [CMake][Build]: Remove unused ACL CMake env variables â€” ready,ci/build â€” by Radu2k (åˆ›å»ºäº: 2025-12-09 22:09 (UTC+8))
- [#30369](https://github.com/vllm-project/vllm/pull/30369) [Fix] Add default rope theta for qwen1 model â€” qwen â€” by iwzbi (åˆ›å»ºäº: 2025-12-10 10:36 (UTC+8))
- [#30345](https://github.com/vllm-project/vllm/pull/30345) Fix typos in comments across multiple files â€” documentation,ready,v1 â€” by wilsonwu (åˆ›å»ºäº: 2025-12-09 22:56 (UTC+8))
- [#30367](https://github.com/vllm-project/vllm/pull/30367) [CI] Reduce Flakiness For test_spec_decode.py::test_suffix_decoding_acceptance â€” ready,v1 â€” by micah-wil (åˆ›å»ºäº: 2025-12-10 08:18 (UTC+8))
- [#30363](https://github.com/vllm-project/vllm/pull/30363) Remove all2all backend envvar â€” documentation,ci/build â€” by elizabetht (åˆ›å»ºäº: 2025-12-10 07:09 (UTC+8))
- [#30366](https://github.com/vllm-project/vllm/pull/30366) [Bug Fix] Fix Kimi-Linear model initialization crash due to missing 'indexer_rotary_emb' arg â€” æ— æ ‡ç­¾ â€” by yonasTMC (åˆ›å»ºäº: 2025-12-10 08:02 (UTC+8))
- [#30357](https://github.com/vllm-project/vllm/pull/30357) Upstream fp8 with static scales gpt oss â€” needs-rebase,gpt-oss â€” by maleksan85 (åˆ›å»ºäº: 2025-12-10 03:49 (UTC+8))
- [#30365](https://github.com/vllm-project/vllm/pull/30365) fix(gguf): Auto-select compatible dtype for GGUF models on Blackwell â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-10 07:59 (UTC+8))
- [#30362](https://github.com/vllm-project/vllm/pull/30362) [WIP] Bump dockerfile to cuda 13.0.2 (for testing) â€” ci/build,nvidia â€” by dougbtv (åˆ›å»ºäº: 2025-12-10 06:51 (UTC+8))
- [#30353](https://github.com/vllm-project/vllm/pull/30353) [Fix] Handle multiple tool calls in Qwen3-MTP tool parser â€” frontend,tool-calling,qwen â€” by ArkVex (åˆ›å»ºäº: 2025-12-10 01:48 (UTC+8))
- [#30356](https://github.com/vllm-project/vllm/pull/30356) [CI][DeepSeek] Add nightly DeepSeek R1 `lm_eval` tests on H200 â€” ready,ci/build,deepseek â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-10 02:05 (UTC+8))
- [#30352](https://github.com/vllm-project/vllm/pull/30352) [CI/Test] Fix FP8 per-tensor quant test reference scale shape â€” ready â€” by LucasWilkinson (åˆ›å»ºäº: 2025-12-10 01:30 (UTC+8))
- [#30354](https://github.com/vllm-project/vllm/pull/30354) [WIP][Core] Update PyTorch to 2.9.1 generally â€” rocm,ci/build,nvidia â€” by orionr (åˆ›å»ºäº: 2025-12-10 01:49 (UTC+8))
- [#30355](https://github.com/vllm-project/vllm/pull/30355) [Model Runner V2] Fix Triton warning on tl.where â€” v1 â€” by WoosukKwon (åˆ›å»ºäº: 2025-12-10 01:56 (UTC+8))
- [#30350](https://github.com/vllm-project/vllm/pull/30350) Remove virtual engine handling â€” tpu,needs-rebase,v1,codex,qwen,kv-connector â€” by WoosukKwon (åˆ›å»ºäº: 2025-12-10 00:34 (UTC+8))
- [#30341](https://github.com/vllm-project/vllm/pull/30341) [CI] refine more logic when generating and using nightly wheels & indices â€” ci/build â€” by Harry-Chen (åˆ›å»ºäº: 2025-12-09 22:17 (UTC+8))
- [#30338](https://github.com/vllm-project/vllm/pull/30338) Fix gigachat3 parser + update tests â€” frontend,tool-calling â€” by ajpqs (åˆ›å»ºäº: 2025-12-09 21:37 (UTC+8))

### å·²åˆå¹¶ PR
- [#30384](https://github.com/vllm-project/vllm/pull/30384) [BugFix] Fix minimax m2 model rotary_dim â€” ready â€” by rogeryoungh (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30062](https://github.com/vllm-project/vllm/pull/30062) [CPU] Support for Whisper â€” ready,ci/build,v1,multi-modality â€” by aditew01 (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30331](https://github.com/vllm-project/vllm/pull/30331) [Bugfix] tpu_model_runner: set vllm config context when calling reset_dynamo_cache() â€” tpu,ready,v1 â€” by dtrifiro (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30351](https://github.com/vllm-project/vllm/pull/30351) [Bugfix] Cache added_vocab to avoid per-token overhead â€” ready â€” by scratch-ml (åˆå¹¶äº: 2025-12-10 12:05 (UTC+8))
- [#30332](https://github.com/vllm-project/vllm/pull/30332) [BUGFIX] Mistral tool call parser v11+ â€” frontend,ready,tool-calling â€” by juliendenize (åˆå¹¶äº: 2025-12-09 22:55 (UTC+8))
- [#30371](https://github.com/vllm-project/vllm/pull/30371) [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output â€” structured-output,ready,v1,deepseek â€” by chaunceyjiang (åˆå¹¶äº: 2025-12-10 16:30 (UTC+8))
- [#30347](https://github.com/vllm-project/vllm/pull/30347) [cpu][ci] Add CPU Attention Tests for Neon Backend â€” ready â€” by fadara01 (åˆå¹¶äº: 2025-12-10 13:37 (UTC+8))
- [#29358](https://github.com/vllm-project/vllm/pull/29358) [ROCm][CI] Attempt to fix the failures under a subgroup of the e2e the test group â€” rocm,ready,ci/build,v1,multi-modality â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-10 13:33 (UTC+8))
- [#30339](https://github.com/vllm-project/vllm/pull/30339) [CMake][Build]: Remove unused ACL CMake env variables â€” ready,ci/build â€” by Radu2k (åˆå¹¶äº: 2025-12-10 12:27 (UTC+8))
- [#30345](https://github.com/vllm-project/vllm/pull/30345) Fix typos in comments across multiple files â€” documentation,ready,v1 â€” by wilsonwu (åˆå¹¶äº: 2025-12-10 12:05 (UTC+8))
- [#30308](https://github.com/vllm-project/vllm/pull/30308) [bugfix][quantization] fix quark qwen3 kv_cache quantization â€” ready,qwen â€” by haoyangli-amd (åˆå¹¶äº: 2025-12-10 11:24 (UTC+8))
- [#30367](https://github.com/vllm-project/vllm/pull/30367) [CI] Reduce Flakiness For test_spec_decode.py::test_suffix_decoding_acceptance â€” ready,v1 â€” by micah-wil (åˆå¹¶äº: 2025-12-10 10:35 (UTC+8))
- [#30230](https://github.com/vllm-project/vllm/pull/30230) [responsesAPI][6] Fix multi turn MCP tokenization â€” documentation,frontend,ready,gpt-oss â€” by qandrew (åˆå¹¶äº: 2025-12-10 10:13 (UTC+8))
- [#30020](https://github.com/vllm-project/vllm/pull/30020) [CI/Build][AMD] Skip quantization kernels tests that require CUTLASS or e4m3fn when not supported by platform â€” rocm,ready,nvidia â€” by rasmith (åˆå¹¶äº: 2025-12-10 10:28 (UTC+8))
- [#30336](https://github.com/vllm-project/vllm/pull/30336) [Bugfix] Fix fp8 DeepGemm compilation issues â€” bug,ready,ci-failure,deepseek â€” by ElizaWszola (åˆå¹¶äº: 2025-12-10 09:17 (UTC+8))
- [#29624](https://github.com/vllm-project/vllm/pull/29624) [Attention] Make seq_lens_cpu optional in CommonAttentionMetadata to enable true async spec-decode â€” speculative-decoding,ready,v1,ready-run-all-tests â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-10 09:18 (UTC+8))
- [#30330](https://github.com/vllm-project/vllm/pull/30330) [Bugfix] Fix cuda graph sizes when running with speculative decoding â€” ready,nvidia â€” by PatrykSaffer (åˆå¹¶äº: 2025-12-10 08:47 (UTC+8))
- [#29723](https://github.com/vllm-project/vllm/pull/29723) [V1][Spec Decode] Optimize Medusa proposer to avoid GPU-CPU sync â€” speculative-decoding,ready,v1 â€” by dongbo910220 (åˆå¹¶äº: 2025-12-10 08:15 (UTC+8))
- [#29937](https://github.com/vllm-project/vllm/pull/29937) Improve wvsplitK tile and balance heristics. â€” rocm,ready â€” by amd-hhashemi (åˆå¹¶äº: 2025-12-10 07:51 (UTC+8))
- [#25693](https://github.com/vllm-project/vllm/pull/25693) [Rocm][torch.compile] Adding layernorm + fp8 block quant and silu + fp8 block quant for Aiter â€” rocm,ready â€” by charlifu (åˆå¹¶äº: 2025-12-10 06:39 (UTC+8))
- [#30119](https://github.com/vllm-project/vllm/pull/30119) [BugFix] Fix DeepSeek-R1 hang with DP and MTP â€” ready,v1,deepseek â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-10 02:51 (UTC+8))
- [#29066](https://github.com/vllm-project/vllm/pull/29066) [MoE][Refactor] Remove most arguments to FusedMoEMethodBase.apply â€” moe,ready,nvidia,ready-run-all-tests â€” by bnellnm (åˆå¹¶äº: 2025-12-10 05:48 (UTC+8))
- [#28480](https://github.com/vllm-project/vllm/pull/28480) [Quantization] FP8 Weight Reloading for Quantized RL Rollout â€” quantization,ready,rl â€” by kylesayrs (åˆå¹¶äº: 2025-12-10 05:54 (UTC+8))
- [#30277](https://github.com/vllm-project/vllm/pull/30277) [BugFix] Fix non detected failing tests â€” ready,ci/build â€” by ilmarkov (åˆå¹¶äº: 2025-12-10 01:57 (UTC+8))
- [#29145](https://github.com/vllm-project/vllm/pull/29145) [CI/Build] Make test_mha_attn.py run on correct platform only and check for flash_attn_varlen_func in layer.py â€” rocm,ready,ci/build â€” by rasmith (åˆå¹¶äº: 2025-12-10 04:18 (UTC+8))
- [#30234](https://github.com/vllm-project/vllm/pull/30234) Bump actions/stale from 10.1.0 to 10.1.1 â€” ready,dependencies,ci/build,github_actions â€” by dependabot (åˆå¹¶äº: 2025-12-10 04:12 (UTC+8))
- [#30233](https://github.com/vllm-project/vllm/pull/30233) Bump actions/checkout from 6.0.0 to 6.0.1 â€” ready,dependencies,ci/build,github_actions â€” by dependabot (åˆå¹¶äº: 2025-12-10 04:03 (UTC+8))
- [#30307](https://github.com/vllm-project/vllm/pull/30307) [Model][Quantization] Fix / Add GGUF support for Qwen2 MoE models â€” ready,qwen â€” by a4lg (åˆå¹¶äº: 2025-12-10 03:13 (UTC+8))
- [#30352](https://github.com/vllm-project/vllm/pull/30352) [CI/Test] Fix FP8 per-tensor quant test reference scale shape â€” ready â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-10 02:52 (UTC+8))
- [#29912](https://github.com/vllm-project/vllm/pull/29912) [Cleanup] Refactor profiling env vars into a CLI config â€” documentation,performance,structured-output,frontend,tpu,ready,v1 â€” by benchislett (åˆå¹¶äº: 2025-12-10 02:29 (UTC+8))
- [#30187](https://github.com/vllm-project/vllm/pull/30187) [Model Runner V2] Support num NaNs in logits â€” v1 â€” by WoosukKwon (åˆå¹¶äº: 2025-12-10 02:00 (UTC+8))
- [#30355](https://github.com/vllm-project/vllm/pull/30355) [Model Runner V2] Fix Triton warning on tl.where â€” v1 â€” by WoosukKwon (åˆå¹¶äº: 2025-12-10 01:59 (UTC+8))
- [#29897](https://github.com/vllm-project/vllm/pull/29897) [Compile] Fix torch warning `TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled` â€” ready,v1 â€” by yewentao256 (åˆå¹¶äº: 2025-12-09 23:40 (UTC+8))
- [#30298](https://github.com/vllm-project/vllm/pull/30298) Update AMD test definitions (2025-12-08) â€” rocm,ready,ci/build,amd â€” by Alexei-V-Ivanov-AMD (åˆå¹¶äº: 2025-12-10 01:31 (UTC+8))
- [#30173](https://github.com/vllm-project/vllm/pull/30173) [BugFix] Fix `assert  batch_descriptor.num_tokens == num_tokens_padded` â€” speculative-decoding,ready,v1,nvidia â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-09 23:36 (UTC+8))
- [#30018](https://github.com/vllm-project/vllm/pull/30018) [Feature] Batch-Invariant Support for FA2 and LoRA â€” ready,v1 â€” by quanliu1991 (åˆå¹¶äº: 2025-12-09 23:01 (UTC+8))
- [#25552](https://github.com/vllm-project/vllm/pull/25552) [ROCm] Aiter Quant Kernels â€” rocm,ready,ci/build â€” by vllmellm (åˆå¹¶äº: 2025-12-09 22:27 (UTC+8))

### å…³é—­ä½†æœªåˆå¹¶çš„ PR
- [#23997](https://github.com/vllm-project/vllm/pull/23997) Feature/sampler benchmark #23977 â€” performance,unstale â€” by baonudesifeizhai (å…³é—­äº: 2025-12-10 21:14 (UTC+8))
- [#26701](https://github.com/vllm-project/vllm/pull/26701) [ROCm]: W8A8BlockFp8LinearOp should use AITER on MI355X â€” rocm,ready,needs-rebase â€” by gronsti-amd (å…³é—­äº: 2025-12-10 20:33 (UTC+8))
- [#30348](https://github.com/vllm-project/vllm/pull/30348) [Docs]: adds a new metric vllm:request_prefill_kv_computed_tokens in docs â€” documentation â€” by googs1025 (å…³é—­äº: 2025-12-10 19:59 (UTC+8))
- [#30349](https://github.com/vllm-project/vllm/pull/30349) [BugFix] Fix minimax m2 model rope_parameters â€” æ— æ ‡ç­¾ â€” by esmeetu (å…³é—­äº: 2025-12-10 18:47 (UTC+8))
- [#29653](https://github.com/vllm-project/vllm/pull/29653) fix potential object has no attribute 'bias' error â€” æ— æ ‡ç­¾ â€” by allerou4 (å…³é—­äº: 2025-12-10 15:16 (UTC+8))
- [#30297](https://github.com/vllm-project/vllm/pull/30297) [Core] Add SLA-tiered scheduling (opt-in) and docs â€” documentation,v1 â€” by ProdByBuddha (å…³é—­äº: 2025-12-10 13:13 (UTC+8))
- [#30327](https://github.com/vllm-project/vllm/pull/30327) [BugFix] Fix hang issue in LMCache mp mode â€” v1,kv-connector â€” by wz1qqx (å…³é—­äº: 2025-12-10 10:32 (UTC+8))
- [#17830](https://github.com/vllm-project/vllm/pull/17830) cmake: Get rid of VLLM_PYTHON_EXECUTABLE â€” needs-rebase,ci/build,stale â€” by seemethere (å…³é—­äº: 2025-12-10 10:26 (UTC+8))
- [#17872](https://github.com/vllm-project/vllm/pull/17872) measure peak memory correctly by removing already used memory â€” needs-rebase,stale,v1 â€” by MiladInk (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#17959](https://github.com/vllm-project/vllm/pull/17959) [Bugfix] fix check kv cache memory log info â€” needs-rebase,stale,v1 â€” by BoL0150 (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#21056](https://github.com/vllm-project/vllm/pull/21056) [Feature][EPLB] Add EPLB support for MiniMax-01 â€” stale â€” by haveheartt (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21413](https://github.com/vllm-project/vllm/pull/21413) Intentionally fail parallel sampling test â€” stale,v1 â€” by sethkimmel3 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21506](https://github.com/vllm-project/vllm/pull/21506) [V1][SpecDecode]Support relaxed acceptance for thinking tokens in speculative decoding in V1 â€” documentation,rocm,frontend,ci/build,stale,v1,multi-modality,tool-calling â€” by DW934 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22238](https://github.com/vllm-project/vllm/pull/22238) [V1][SpecDecode]Support Relaxed Acceptance for thinking tokens in speculative decoding when using greedy search, camp up by Nvidia. â€” stale,v1 â€” by DW934 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22488](https://github.com/vllm-project/vllm/pull/22488) Feat/sliding window metrics â€” Related to #22480 â€” needs-rebase,stale,v1 â€” by NumberWan (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22632](https://github.com/vllm-project/vllm/pull/22632) [Bugfix]fix deepseek_r1_reasoning bugs when <think> </think> in contents. â€” stale,deepseek â€” by z2415445508 (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#27594](https://github.com/vllm-project/vllm/pull/27594) Fix intermediatetensors spawn error #27591 â€” qwen â€” by baonudesifeizhai (å…³é—­äº: 2025-12-10 08:44 (UTC+8))
- [#28627](https://github.com/vllm-project/vllm/pull/28627) [Weight Loading] Expand quantized weight reloading support â€” needs-rebase,v1 â€” by kylesayrs (å…³é—­äº: 2025-12-10 05:48 (UTC+8))
- [#30354](https://github.com/vllm-project/vllm/pull/30354) [WIP][Core] Update PyTorch to 2.9.1 generally â€” rocm,ci/build,nvidia â€” by orionr (å…³é—­äº: 2025-12-10 02:46 (UTC+8))
- [#30063](https://github.com/vllm-project/vllm/pull/30063) Mistral tool parser â€” frontend,tool-calling â€” by graelo (å…³é—­äº: 2025-12-09 23:56 (UTC+8))
- [#27305](https://github.com/vllm-project/vllm/pull/27305) [ROCm][torch.compile] Adding MulAddFusionPass to enable AITER fused_mul_add â€” rocm,needs-rebase â€” by micah-wil (å…³é—­äº: 2025-12-09 23:49 (UTC+8))
- [#26257](https://github.com/vllm-project/vllm/pull/26257) [Feature][torch.compile] Add pass to rearrange AllGather for FP8 models in sequence parallel for better Async TP fusion â€” needs-rebase,ci/build â€” by jasonlizhengjian (å…³é—­äº: 2025-12-09 22:59 (UTC+8))
- [#25618](https://github.com/vllm-project/vllm/pull/25618) [ROCm][Allreduce] Add dispatch mechanism for choosing performant allreduce implementations for AMD platforms â€” rocm,needs-rebase,nvidia â€” by zejunchen-zejun (å…³é—­äº: 2025-12-09 21:45 (UTC+8))