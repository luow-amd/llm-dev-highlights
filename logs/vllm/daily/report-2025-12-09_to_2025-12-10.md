---
title: vLLM å¼€å‘åŠ¨æ€æŠ¥å‘Š - 2025-12-09
date: 2025-12-09
layout: default
---

[<< Back to vLLM Reports]({{ site.baseurl }}/logs/vllm/)

# vLLM å¼€å‘åŠ¨æ€æŠ¥å‘Š - 2025-12-09

> æ—¶é—´çª—å£: 2025-12-09 21:30 (UTC+8) ~ 2025-12-10 21:30 (UTC+8)
> æ•°æ®ç»Ÿè®¡: æ–° Issue 17 | å…³é—­ Issue 19 | æ–° PR 37 | åˆå¹¶ PR 37 | å…³é—­æœªåˆå¹¶ PR 23

---

### ğŸ“Š æ¯æ—¥å¼€å‘çŠ¶æ€æ‘˜è¦
æœ¬å‘¨æœŸï¼ˆ2025-12-09è‡³2025-12-10ï¼‰vLLMé¡¹ç›®ä¿æŒé«˜é€Ÿå¼€å‘è¿­ä»£ï¼Œå…±åˆå¹¶37ä¸ªPRï¼Œå¤„ç†äº†36ä¸ªIssueã€‚å¼€å‘ç„¦ç‚¹é›†ä¸­åœ¨**DeepSeek-V3.2æ¨¡å‹çš„æ€§èƒ½ä¼˜åŒ–ä¸é—®é¢˜ä¿®å¤**ã€**å¤šæ¨¡æ€ä¸CPUåç«¯èƒ½åŠ›æ‰©å±•**ï¼Œä»¥åŠ**AMDï¼ˆROCmï¼‰ç”Ÿæ€æ”¯æŒçš„æŒç»­å®Œå–„**ã€‚ç¤¾åŒºåŒæ—¶å°±å¤šé¡¹é‡è¦æ¶æ„æ”¹è¿›ï¼ˆå¦‚åœ¨çº¿é‡åŒ–é‡è½½ã€åŸºå‡†æµ‹è¯•å·¥å…·é‡æ„ï¼‰å±•å¼€äº†æ·±å…¥è®¨è®ºã€‚

### ğŸ¯ AMD/ROCm ç”Ÿæ€ç›¸å…³åŠ¨æ€
æœ¬å‘¨æœŸAMDç”Ÿæ€ç›¸å…³æ´»åŠ¨æ´»è·ƒï¼Œæ¶‰åŠå…¼å®¹æ€§ä¿®å¤ã€æ€§èƒ½ä¼˜åŒ–å’Œå·¥å…·é“¾å®Œå–„ã€‚

1.  **PR #30361 - [Attention][AMD] Make flash-attn optional** (`mgehre-amd`)
    *   **å†…å®¹**ï¼šä¿®å¤äº†ROCmå¹³å°ä¸Šæœªå®‰è£…`flash-attn`åŒ…æ—¶ï¼Œå› Eagleæ¨æµ‹è§£ç æ¨¡å—çš„æ— æ¡ä»¶å¯¼å…¥è€Œå¯¼è‡´çš„å¯åŠ¨å¤±è´¥é—®é¢˜ã€‚ä¿®æ”¹ä½¿å¯¼å…¥å˜ä¸ºæ¡ä»¶æ€§ï¼Œå…è®¸éEagleå·¥ä½œè´Ÿè½½åœ¨ä¸å®‰è£…`flash-attn`çš„æƒ…å†µä¸‹è¿è¡Œã€‚
    *   **å½±å“**ï¼šæå‡äº†vLLMåœ¨AMDå¹³å°ä¸Šçš„éƒ¨ç½²çµæ´»æ€§å’Œé²æ£’æ€§ï¼Œé™ä½äº†ä¾èµ–é¡¹é—¨æ§›ã€‚

2.  **Issue #30360 - [RFC]: Consolidate FP8 min/max values into somewhere reasonable (Python only)** (`rasmith`)
    *   **å†…å®¹**ï¼šé’ˆå¯¹MI300ç­‰ä½¿ç”¨`torch.float8_e4m3fnuz`æ•°æ®ç±»å‹çš„å¹³å°ï¼Œæå‡ºäº†ç»Ÿä¸€FP8æœ€å°/æœ€å¤§å€¼ï¼ˆ-224.0/224.0ï¼‰åˆ°`fp8_utils`ä¸­çš„ææ¡ˆã€‚æ—¨åœ¨è§£å†³å½“å‰ä»£ç ä¸­é»˜è®¤å€¼ï¼ˆÂ±240ï¼‰ä¸æ¨èå€¼ä¸ä¸€è‡´å¯¼è‡´çš„æµ‹è¯•å¤±è´¥å’Œæ½œåœ¨ç²¾åº¦é—®é¢˜ã€‚
    *   **è§‚ç‚¹ä¸çŠ¶æ€**ï¼šè¿™æ˜¯ä¸€ä¸ª**å¾æ±‚åé¦ˆçš„RFCï¼ˆRequest for Commentsï¼‰**ï¼Œæ—¨åœ¨é€šè¿‡ä»£ç é›†ä¸­åŒ–æ¥é˜²æ­¢æœªæ¥å‡ºç°åŒç±»é—®é¢˜ã€‚ç›®å‰å¤„äºå¼€æ”¾è®¨è®ºé˜¶æ®µã€‚

3.  **PR #30364 - [Bugfix] awq_gemm: fix argument order swap** (`mgehre-amd`)
    *   **å†…å®¹**ï¼šä¿®å¤äº†`_custom_ops.awq_gemm`å‡½æ•°ä¸­`scales`å’Œ`zeros`å‚æ•°é¡ºåºå£°æ˜ä¸è°ƒç”¨ä¸ä¸€è‡´çš„é—®é¢˜ï¼ˆè™½ç„¶ç”±äºåŒé‡çš„é¡ºåºäº¤æ¢ï¼ŒåŠŸèƒ½åŸæ˜¯æ­£ç¡®çš„ï¼‰ã€‚è¿™æé«˜äº†ä»£ç çš„å¯è¯»æ€§å’Œä¸CUDAå®ç°çš„ä¸€è‡´æ€§ã€‚
    *   **å½±å“**ï¼šä»£ç æ¸…ç†ï¼Œæå‡AMDå¹³å°AWQé‡åŒ–å†…æ ¸ä»£ç çš„ç»´æŠ¤æ€§ã€‚

4.  **å·²å…³é—­ Issue #28314 - [AMD][CI Failure]: Tracking failure for AMD CI Dependencies & Environments**
    *   **å†…å®¹**ï¼šè¯¥è·Ÿè¸ªæ€§Issueè®°å½•äº†AMD CIç¯å¢ƒåœ¨ä¾èµ–é¡¹ï¼ˆå¦‚UVã€torchaudioã€terratorchã€pqdmç­‰ï¼‰æ–¹é¢çš„å†å²é—®é¢˜ã€‚
    *   **ç»“è®º**ï¼šéšç€ç›¸å…³PRçš„åˆå¹¶ï¼Œåˆ—å‡ºçš„é—®é¢˜å‡å·²è§£å†³ï¼Œè¯¥Issueè¢«å…³é—­ï¼Œæ ‡å¿—ç€AMD CIç¯å¢ƒçš„ç¨³å®šæ€§å’Œå®Œæ•´æ€§å¾—åˆ°æ˜¾è‘—æå‡ã€‚

**å°ç»“**ï¼šAMDç”Ÿæ€çš„æ”¯æŒæ­£åœ¨ä»â€œå¯ç”¨â€å‘â€œå¥½ç”¨â€å’Œâ€œç¨³å®šâ€è¿ˆè¿›ï¼Œå…³æ³¨ç‚¹ä»åŸºç¡€åŠŸèƒ½è½¬å‘äº†ç»†èŠ‚ä¼˜åŒ–ã€ä»£ç è´¨é‡ä»¥åŠå¼€å‘ä½“éªŒï¼ˆå¦‚CIã€ä¾èµ–ç®¡ç†ï¼‰ã€‚

### ğŸ’¬ é«˜çƒ­åº¦è®¨è®ºåˆ†æ

1.  **å·²å…³é—­ Issue #15636 - [Bug]: Outlines broken on vLLM 0.8+** (12æ¡è¯„è®º)
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šç”¨æˆ·å‘ç°ä»vLLM 0.8ç‰ˆæœ¬å¼€å§‹ï¼ˆV1å¼•æ“æˆä¸ºé»˜è®¤ï¼‰ï¼Œä¾èµ–äºè‡ªå®šä¹‰`logits_processor`çš„ç¬¬ä¸‰æ–¹åº“ï¼ˆå¦‚Outlinesï¼‰æ— æ³•å·¥ä½œã€‚
    *   **ä¸åŒè§‚ç‚¹**ï¼š
        *   **ç”¨æˆ·/å¼€å‘è€…**ï¼šè®¤ä¸ºåœ¨V1å¼•æ“åŠŸèƒ½å°šæœªä¸V0å®Œå…¨å¯¹é½æ—¶å°±å°†å…¶è®¾ä¸ºé»˜è®¤ï¼Œå¯¼è‡´äº†ç”Ÿæ€ç ´åï¼Œç»™é›†æˆæ–¹å¸¦æ¥äº†éº»çƒ¦ã€‚
        *   **ç»´æŠ¤è€…**ï¼šæ‰¿è®¤è¿™æ˜¯ä¸€ä¸ªæ•™è®­ï¼Œå¹¶æä¾›äº†ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼ˆè®¾ç½®`VLLM_USE_V1=0`å›é€€åˆ°V0å¼•æ“ï¼‰ï¼ŒåŒæ—¶è§£é‡Šäº†V1å¼•æ“ä¸­æ–°çš„ç»“æ„åŒ–è¾“å‡ºä½¿ç”¨æ–¹å¼ã€‚
    *   **äº‰è®®ç„¦ç‚¹**ï¼šåŠŸèƒ½è¿­ä»£ä¸å‘åå…¼å®¹çš„å¹³è¡¡ã€‚ç”¨æˆ·æœŸå¾…æ›´å¹³æ»‘çš„è¿‡æ¸¡æˆ–æ›´æ˜ç¡®çš„æ²Ÿé€šã€‚
    *   **æœ€ç»ˆç»“è®º**ï¼šIssueåœ¨è¿‘9ä¸ªæœˆåè¢«å…³é—­ï¼ŒæœŸé—´V1å¼•æ“çš„ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½å·²é€æ­¥å®Œå–„ï¼Œå¯èƒ½ä»£è¡¨ç›¸å…³é—®é¢˜å·²å¾—åˆ°ç¼“è§£æˆ–ç”¨æˆ·å·²è¿ç§»è‡³æ–°APIã€‚

2.  **Issue #30358 - [Bug]: Prefill scheduled num_block mismatch at update_state_after_alloc and request_finished** (4æ¡è¯„è®º)
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šè´¡çŒ®è€…`xuechendi`åœ¨æµ‹è¯•ä¸­å‘ç°ï¼Œè°ƒåº¦å™¨åœ¨å¤„ç†åˆ†å—é¢„å¡«å……ï¼ˆchunked prefillï¼‰æ—¶ï¼Œåˆ†é…ç»™åŒä¸€è¯·æ±‚çš„block IDåˆ—è¡¨åœ¨`update_state_after_alloc`å’Œ`request_finished`ä¸¤ä¸ªé˜¶æ®µå¯èƒ½å‡ºç°ä¸ä¸€è‡´ï¼Œå¯¼è‡´KVè¿æ¥å™¨å…ƒæ•°æ®ä¸åŒæ­¥ã€‚
    *   **è®¨è®ºè¿‡ç¨‹**ï¼šè´¡çŒ®è€…é€šè¿‡è¯¦ç»†æ—¥å¿—å®šä½åˆ°é—®é¢˜æ ¹æºâ€”â€”åœ¨è°ƒåº¦å™¨å¾ªç¯ä¸­æ›´æ–°äº†è¯·æ±‚çš„blockåˆ—è¡¨ï¼Œä½†æœªåŒæ­¥è§¦å‘KVè¿æ¥å™¨çš„å…ƒæ•°æ®æ›´æ–°ã€‚
    *   **å½“å‰çŠ¶æ€**ï¼šé—®é¢˜å·²å®šä½ï¼Œè´¡çŒ®è€…è¡¨ç¤ºå°†éšåæäº¤ä¿®å¤PRã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„æŠ€æœ¯æ·±åº¦è°ƒè¯•æ¡ˆä¾‹ã€‚

3.  **Issue #30359 - [RFC] [QeRL]: Online Quantization and Model Reloading** (1æ¡è¯„è®º)
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šæå‡ºä¸€ä¸ªæ”¯æŒ**åœ¨çº¿é‡åŒ–ä¸æ¨¡å‹é‡è½½**çš„æ–°æ¶æ„ï¼Œæ—¨åœ¨é«˜æ•ˆæ”¯æŒå¼ºåŒ–å­¦ä¹ è®­ç»ƒç­‰éœ€è¦é¢‘ç¹åˆ‡æ¢é‡åŒ–æ¨¡å‹æƒé‡çš„åœºæ™¯ã€‚
    *   **è§‚ç‚¹**ï¼š
        *   **ææ¡ˆè€…**ï¼šç°æœ‰å®ç°å­˜åœ¨å†…å­˜å ç”¨é«˜ï¼ˆå¯èƒ½ç¿»å€ï¼‰ã€ä»…æ”¯æŒéƒ¨åˆ†é…ç½®ç­‰é—®é¢˜ã€‚æ–°è®¾è®¡è¿½æ±‚æ›´æ¸…æ™°çš„å†…å­˜å’Œæ ¼å¼ç®¡ç†ã€‚
        *   **å‚ä¸è€… `vkuzo`**ï¼šè¯¢é—®äº†â€œæ£€æŸ¥ç‚¹æ ¼å¼â€ä¸â€œæ¨¡å‹æ ¼å¼â€å·®å¼‚çš„å…·ä½“ä¾‹å­ï¼Œå¹¶è®¤ä¸ºå¦‚æœæ–°è®¾è®¡æ˜¾è‘—æ›´æ¸…æ™°ï¼Œè¦æ±‚ç”¨æˆ·è¿ç§»è„šæœ¬æ˜¯åˆç†çš„ã€‚
    *   **çŠ¶æ€**ï¼šRFCåˆšæå‡ºï¼Œå¤„äºæ—©æœŸè®¾è®¡è®¨è®ºé˜¶æ®µã€‚è®¨è®ºå°†å›´ç»•æ¶æ„å¤æ‚æ€§ä¸ç”¨æˆ·è¿ç§»æˆæœ¬å±•å¼€ã€‚

### ğŸ”¥ çƒ­é—¨è¯é¢˜ä¸è¶‹åŠ¿åˆ†æ
1.  **DeepSeek-V3.2 é›†æˆä¸ä¼˜åŒ–**ï¼šæ˜¯å½“å‰ç»å¯¹çš„ç„¦ç‚¹ã€‚å¤šä¸ªIssueå’ŒPRå›´ç»•å…¶å±•å¼€ï¼Œæ¶‰åŠ**æ€§èƒ½è°ƒä¼˜**ï¼ˆ#30370ï¼‰ã€**Tokenizeræ•ˆç‡**ï¼ˆ#30351 å·²ä¿®å¤ï¼‰ã€**å·¥å…·è°ƒç”¨ä¸ç»“æ„åŒ–è¾“å‡ºæ”¯æŒ**ï¼ˆ#30371 å·²ä¿®å¤ï¼‰ã€**å†…æ ¸å…¼å®¹æ€§**ï¼ˆ#30336 å·²ä¿®å¤ï¼‰ç­‰ï¼Œåæ˜ äº†ç¤¾åŒºå¯¹ä¸»æµæ–°æ¨¡å‹å¿«é€Ÿé€‚é…çš„é«˜åº¦é‡è§†ã€‚
2.  **å¤šæ¨¡æ€ä¸CPUåç«¯æ‰©å±•**ï¼š
    *   å¤šæ¨¡æ€æ–¹é¢ï¼Œæœ‰PRä¸º**Whisperæ¨¡å‹æ–°å¢CPUåç«¯æ”¯æŒ**ï¼ˆ#30062 å·²åˆå¹¶ï¼‰ï¼Œå¹¶æå‡ºäº†**åŸºäºLMDBçš„å¤šæ¨¡æ€ç¼“å­˜å®ç°**ï¼ˆ#30373ï¼‰ã€‚
    *   CPUæ–¹é¢ï¼Œæœ‰Issueè¯·æ±‚ä¸º**CPUåˆ†é¡µæ³¨æ„åŠ›æ·»åŠ ç»Ÿä¸€åŸºå‡†æµ‹è¯•è„šæœ¬**ï¼ˆ#30374ï¼‰ã€‚
3.  **æ€§èƒ½ä¸é‡åŒ–ä¼˜åŒ–**ï¼š
    *   æ€§èƒ½å·¥å…·é¢ä¸´é‡æ„ï¼Œæœ‰RFCæè®®**é‡æ„åŸºå‡†æµ‹è¯•å·¥å…·ä»¥æ”¯æŒå¤šè¿›ç¨‹ï¼Œçªç ´å•æ ¸ç“¶é¢ˆ**ï¼ˆ#30383ï¼‰ã€‚
    *   é‡åŒ–é¢†åŸŸï¼Œå›´ç»•**FP8æ•°å€¼èŒƒå›´ç»Ÿä¸€**ï¼ˆ#30360ï¼‰ã€**æƒé‡é‡è½½æ”¯æŒ**ï¼ˆ#28480 å·²åˆå¹¶ï¼‰ç­‰åº•å±‚è®¾æ–½åœ¨è¿›è¡Œæ·±åº¦æ”¹è¿›ã€‚
4.  **å¼€å‘è€…ä½“éªŒä¸åŸºç¡€è®¾æ–½**ï¼š
    *   åŒ…æ‹¬**æ–‡æ¡£è‡ªåŠ¨åŒ–ç”ŸæˆæŒ‡æ ‡åˆ—è¡¨**ï¼ˆ#30388ï¼‰ã€**ä¿®å¤æœªè¢«CIæ£€æµ‹åˆ°çš„æµ‹è¯•å¤±è´¥**ï¼ˆ#30277 å·²åˆå¹¶ï¼‰ã€**æè®®ç”¨LLMæ™ºèƒ½å†³å®šCIæµ‹è¯•èŒƒå›´**ï¼ˆ#30368ï¼‰ç­‰ï¼Œä½“ç°äº†å¯¹é¡¹ç›®é•¿æœŸå¥åº·å’Œå¼€å‘æ•ˆç‡çš„å…³æ³¨ã€‚

### ğŸ› ï¸ é‡ç‚¹æŠ€æœ¯å˜æ›´
1.  **PR #30351 - [Bugfix] Cache added_vocab to avoid per-token overhead** (å·²åˆå¹¶)
    *   **è§£è¯»**ï¼šä¿®å¤äº†DeepSeek-V3.2 Tokenizeråœ¨æµå¼è§£ç æ—¶ï¼Œæ¯ä¸ªtokenéƒ½ä¼šè°ƒç”¨`len(tokenizer)`è¿›è€Œè§¦å‘æ˜‚è´µçš„åŸç”Ÿ`get_added_vocab()`æ“ä½œï¼Œå¯¼è‡´ä¸»çº¿ç¨‹é˜»å¡çš„é—®é¢˜ã€‚é€šè¿‡**ç¼“å­˜é™„åŠ è¯æ±‡è¡¨å¤§å°**ï¼Œå½»åº•æ¶ˆé™¤äº†æ¯tokençš„å†—ä½™è®¡ç®—ã€‚
    *   **å½±å“**ï¼šæ˜¾è‘—æå‡äº†DeepSeek-V3.2æ¨¡å‹åœ¨é«˜å¹¶å‘æµå¼è¾“å‡ºæ—¶çš„æœåŠ¡ç¨³å®šæ€§å’Œå“åº”èƒ½åŠ›ã€‚

2.  **PR #30371 - [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output** (å·²åˆå¹¶)
    *   **è§£è¯»**ï¼šä¿®å¤äº†DeepSeek-V3.2æ¨¡å‹æ— æ³•ä½¿ç”¨JSON Schemaã€è¯­æ³•ç­‰ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½çš„Bugã€‚ç¡®ä¿äº†å…¶æ¨ç†ã€å·¥å…·è°ƒç”¨ç­‰é«˜çº§åŠŸèƒ½ä¸ç»“æ„åŒ–è¾“å‡ºæµç¨‹çš„å…¼å®¹æ€§ã€‚
    *   **å½±å“**ï¼šå®Œå–„äº†DeepSeek-V3.2æ¨¡å‹åœ¨vLLMä¸­çš„åŠŸèƒ½é›†ï¼Œä½¿å…¶èƒ½æ›´å¥½åœ°åº”ç”¨äºéœ€è¦çº¦æŸè¾“å‡ºçš„å¤æ‚åœºæ™¯ã€‚

3.  **PR #30384 - [BugFix] Fix minimax m2 model rotary_dim** (å·²åˆå¹¶)
    *   **è§£è¯»**ï¼šä¿®å¤äº†å› PR #29966ç»Ÿä¸€RoPEè·å–é€»è¾‘åï¼Œå¯¼è‡´Minimax-M2æ¨¡å‹`rotary_dim`è¢«é‡å¤ç¼©æ”¾çš„é—®é¢˜ã€‚é€šè¿‡è°ƒæ•´é€»è¾‘ï¼Œæ­£ç¡®å¤„ç†äº†å·²é¢„ç¼©æ”¾`rotary_dim`çš„æ¨¡å‹é…ç½®ã€‚
    *   **å½±å“**ï¼šæ¢å¤äº†Minimax-M2æ¨¡å‹çš„æ­£ç¡®æ¨ç†èƒ½åŠ›ï¼Œæ˜¯æ¨¡å‹å…¼å®¹æ€§ç»´æŠ¤çš„å…³é”®ä¿®å¤ã€‚

4.  **PR #30062 - [CPU] Support for Whisper** (å·²åˆå¹¶)
    *   **è§£è¯»**ï¼šä¸ºWhisperè¯­éŸ³è½¬å½•æ¨¡å‹å®ç°äº†CPUåç«¯æ”¯æŒï¼ŒåŒ…æ‹¬å¤„ç†ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„æ³¨æ„åŠ›é€»è¾‘ã€‚
    *   **å½±å“**ï¼šæ‰©å±•äº†vLLMå¤šæ¨¡æ€æ¨¡å‹åœ¨CPUä¸Šçš„æ”¯æŒèŒƒå›´ï¼Œä¸ºæ— GPUç¯å¢ƒçš„éŸ³é¢‘å¤„ç†ä»»åŠ¡æä¾›äº†å¯èƒ½ã€‚

### ğŸ“ˆ å¼€å‘æ´»è·ƒåº¦è§‚å¯Ÿ
*   **é«˜æ•ˆåˆå¹¶**ï¼š24å°æ—¶å†…åˆå¹¶37ä¸ªPRï¼Œæ˜¾ç¤ºä»£ç å®¡æŸ¥ä¸é›†æˆæµç¨‹éå¸¸é«˜æ•ˆã€‚
*   **è´¡çŒ®è€…å¤šæ ·æ€§**ï¼šé™¤äº†æ ¸å¿ƒå›¢é˜Ÿï¼ˆå¦‚`WoosukKwon`ã€`LucasWilkinson`ï¼‰ï¼ŒAMDå·¥ç¨‹å¸ˆï¼ˆ`mgehre-amd`ã€`rasmith`ï¼‰ã€æ¥è‡ªå¤§å‹ç§‘æŠ€å…¬å¸çš„å¼€å‘è€…ï¼ˆå¦‚Metaçš„`fadara01`ï¼‰ä»¥åŠä¼—å¤šç‹¬ç«‹è´¡çŒ®è€…éƒ½æ´»è·ƒå‚ä¸ã€‚
*   **é—®é¢˜é—­ç¯è¿…é€Ÿ**ï¼šå…³é—­äº†19ä¸ªIssueï¼Œå…¶ä¸­åŒ…å«å¤šä¸ªå†å²é—ç•™é—®é¢˜ï¼ˆå¦‚#15636, #28314ï¼‰ï¼Œè¡¨æ˜ç¤¾åŒºåœ¨æ¸…ç†é—®é¢˜ç§¯å‹æ–¹é¢æŠ•å…¥äº†ç²¾åŠ›ã€‚
*   **æ·±åº¦æŠ€æœ¯æ¢è®¨**ï¼šå¦‚#30358ä¸­è´¡çŒ®è€…å¯¹è°ƒåº¦å™¨Bugçš„æ ¹å› åˆ†æï¼Œå±•ç°äº†ç¤¾åŒºæˆå‘˜å…·å¤‡æ·±å…¥å†…æ ¸æ’æŸ¥é—®é¢˜çš„èƒ½åŠ›ã€‚

### ğŸ’¡ å€¼å¾—å…³æ³¨çš„é—®é¢˜
1.  **åœ¨çº¿é‡åŒ–ä¸æ¨¡å‹é‡è½½æ¶æ„ï¼ˆRFC #30359ï¼‰**ï¼šæ­¤ææ¡ˆæ—¨åœ¨ä¸ºå¼ºåŒ–å­¦ä¹ ç­‰å‰æ²¿ç ”ç©¶åœºæ™¯æä¾›åº•å±‚æ”¯æŒï¼Œå…¶è®¾è®¡å†³ç­–å°†å½±å“vLLMåœ¨åŠ¨æ€é‡åŒ–å·¥ä½œæµä¸­çš„å†…å­˜æ•ˆç‡å’Œæ˜“ç”¨æ€§ã€‚
2.  **DeepSeek-V3.2çš„AWQé‡åŒ–æ€§èƒ½ï¼ˆIssue #30370ï¼‰**ï¼šç”¨æˆ·æŠ¥å‘Šå…¶AWQé‡åŒ–ç‰ˆæœ¬æ€§èƒ½ä½äºé¢„æœŸï¼Œä¸”`VLLM_USE_DEEP_GEMM=0`ä¼šå¯¼è‡´å´©æºƒã€‚è¿™å…³ç³»åˆ°è¯¥çƒ­é—¨æ¨¡å‹é‡åŒ–éƒ¨ç½²çš„æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œéœ€è¦æ ¸å¿ƒå›¢é˜Ÿå…³æ³¨ã€‚
3.  **å¤šè¿›ç¨‹åŸºå‡†æµ‹è¯•æ¶æ„ï¼ˆRFC #30383ï¼‰**ï¼šå½“å‰åŸºå‡†æµ‹è¯•å·¥å…·å—é™äºå•æ ¸æ€§èƒ½ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°é«˜å¹¶å‘åœºæ™¯ã€‚æ­¤é‡æ„å¯¹å…¬æ­£è¯„ä¼°vLLMåœ¨é«˜è´Ÿè½½ä¸‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚
4.  **GPUæ¶æ„å…¼å®¹æ€§**ï¼šIssue #30387æŠ¥å‘Šäº†åœ¨FlashAttentionåç«¯ä½¿ç”¨FP8 KV Cacheæ—¶å‡ºç°éæ³•å†…å­˜è®¿é—®ã€‚è¿™ç±»ä¸ç‰¹å®šç¡¬ä»¶ã€å†…æ ¸åç«¯å’Œæ•°æ®ç±»å‹ç»„åˆç›¸å…³çš„æ·±å±‚æ¬¡Bugï¼Œæ˜¯é«˜æ€§èƒ½æ¨ç†å¼•æ“éœ€è¦æŒç»­åº”å¯¹çš„æŒ‘æˆ˜ã€‚

---

## ğŸ“‹ é™„å½•ï¼šè¯¦ç»†æ•°æ®åˆ—è¡¨

### æ–°å¢ Issue
- [#30380](https://github.com/vllm-project/vllm/issues/30380) [Usage]: å¤§å®¶ä¸€èˆ¬æ€ä¹ˆä½¿ç”¨vllm/testsçš„ï¼Ÿ â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:27 (UTC+8))
- [#30359](https://github.com/vllm-project/vllm/issues/30359) [RFC] [QeRL]: Online Quantization and Model Reloading â€” RFC â€” by kylesayrs (åˆ›å»ºäº: 2025-12-10 05:24 (UTC+8))
- [#30387](https://github.com/vllm-project/vllm/issues/30387) [Bug]: illegal memory access countered when using kv-cache-type=fp8  loading  a weight-fp8 model for evaltest  in flash-attn backend â€” bug â€” by youngze0016 (åˆ›å»ºäº: 2025-12-10 19:43 (UTC+8))
- [#30382](https://github.com/vllm-project/vllm/issues/30382) [Bug]: Issues with mistralai/Ministral-3-14B-Instruct-2512 â€” bug â€” by eltorre (åˆ›å»ºäº: 2025-12-10 17:55 (UTC+8))
- [#30383](https://github.com/vllm-project/vllm/issues/30383) [RFC]: Multi-Process Benchmark Architecture for Scaling Beyond Single-Core Limits â€” RFC â€” by GaoHuaZhang (åˆ›å»ºäº: 2025-12-10 18:02 (UTC+8))
- [#30374](https://github.com/vllm-project/vllm/issues/30374) [Feature][CPU Backend]: Add Paged Attention Benchmarks for CPU backend â€” feature request,cpu â€” by fadara01 (åˆ›å»ºäº: 2025-12-10 15:53 (UTC+8))
- [#30381](https://github.com/vllm-project/vllm/issues/30381) [Usage]: â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:27 (UTC+8))
- [#30379](https://github.com/vllm-project/vllm/issues/30379) [Usage]: how to use vllm/tests/ï¼Ÿ â€” usage â€” by tobeprozy (åˆ›å»ºäº: 2025-12-10 17:25 (UTC+8))
- [#30378](https://github.com/vllm-project/vllm/issues/30378) [Feature]: Automatically infer Qwen3 reranker settings (remove need for hf_overrides) â€” feature request â€” by ilopezluna (åˆ›å»ºäº: 2025-12-10 17:21 (UTC+8))
- [#30375](https://github.com/vllm-project/vllm/issues/30375) [Bug]: [TPU] ShapeDtypeStruct error when loading custom safetensors checkpoint on TPU v5litepod â€” bug â€” by Baltsat (åˆ›å»ºäº: 2025-12-10 16:12 (UTC+8))
- [#30372](https://github.com/vllm-project/vllm/issues/30372) [Bug]: vLLM (GPT-OSS) causes distorted tool argument names + infinite tool-call loop with Korean messenger tool â€” bug â€” by minmini2 (åˆ›å»ºäº: 2025-12-10 14:59 (UTC+8))
- [#30370](https://github.com/vllm-project/vllm/issues/30370) [Performance]: DeepSeek-V3.2 AWQ Performance is lower then i expected â€” performance â€” by yongho-chang (åˆ›å»ºäº: 2025-12-10 10:45 (UTC+8))
- [#30343](https://github.com/vllm-project/vllm/issues/30343) [Bug]: In DeepSeek-V3.2 tokenizer mode, detokenization saturates the main thread, causing the server to hang â€” bug â€” by scratch-ml (åˆ›å»ºäº: 2025-12-09 22:48 (UTC+8))
- [#30358](https://github.com/vllm-project/vllm/issues/30358) [Bug]: Prefill scheduled num_block mismatch at update_state_after_alloc and request_finished â€” bug â€” by xuechendi (åˆ›å»ºäº: 2025-12-10 04:15 (UTC+8))
- [#30368](https://github.com/vllm-project/vllm/issues/30368) [CI] Test target determination using LLM â€” feature request,ci â€” by khluu (åˆ›å»ºäº: 2025-12-10 09:42 (UTC+8))
- [#30360](https://github.com/vllm-project/vllm/issues/30360) [RFC]: Consolidate FP8 min/max values into somewhere reasonable (Python only) â€” rocm,RFC â€” by rasmith (åˆ›å»ºäº: 2025-12-10 05:44 (UTC+8))
- [#30342](https://github.com/vllm-project/vllm/issues/30342) [Bug]: HunyuanOCR batching problem with variable sized images in a batch. â€” bug â€” by anker-c2 (åˆ›å»ºäº: 2025-12-09 22:22 (UTC+8))

### å·²å…³é—­ Issue
- [#15636](https://github.com/vllm-project/vllm/issues/15636) [Bug]: Outlines broken on vLLM 0.8+ â€” bug,structured-output,unstale â€” by cpfiffer (å…³é—­äº: 2025-12-10 21:18 (UTC+8))
- [#30382](https://github.com/vllm-project/vllm/issues/30382) [Bug]: Issues with mistralai/Ministral-3-14B-Instruct-2512 â€” bug â€” by eltorre (å…³é—­äº: 2025-12-10 18:47 (UTC+8))
- [#30381](https://github.com/vllm-project/vllm/issues/30381) [Usage]: â€” usage â€” by tobeprozy (å…³é—­äº: 2025-12-10 17:28 (UTC+8))
- [#30379](https://github.com/vllm-project/vllm/issues/30379) [Usage]: how to use vllm/tests/ï¼Ÿ â€” usage â€” by tobeprozy (å…³é—­äº: 2025-12-10 17:26 (UTC+8))
- [#30311](https://github.com/vllm-project/vllm/issues/30311) [Bug]: deepseekv32.DeepseekV32Tokenizer Runtime causes model to crash â€” bug â€” by magician-xin (å…³é—­äº: 2025-12-10 16:30 (UTC+8))
- [#28314](https://github.com/vllm-project/vllm/issues/28314) [AMD][CI Failure]: Tracking failure for AMD CI Dependencies & Environments â€” rocm,ci-failure â€” by zhewenl (å…³é—­äº: 2025-12-10 13:32 (UTC+8))
- [#30343](https://github.com/vllm-project/vllm/issues/30343) [Bug]: In DeepSeek-V3.2 tokenizer mode, detokenization saturates the main thread, causing the server to hang â€” bug â€” by scratch-ml (å…³é—­äº: 2025-12-10 12:05 (UTC+8))
- [#20181](https://github.com/vllm-project/vllm/issues/20181) [Feature]: Batch inference for Multi-Modal Online Serving â€” feature request,stale â€” by eslambakr (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#21097](https://github.com/vllm-project/vllm/issues/21097) [Bug]: w8a8 quantization not supporting sm120 â€” bug,stale â€” by sarmiena (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21909](https://github.com/vllm-project/vllm/issues/21909) [Bug]: quant_method is not None â€” bug,stale â€” by maxin9966 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22325](https://github.com/vllm-project/vllm/issues/22325) [Bug]: gpt-oss model crashes on NVIDIA B200 with any OpenAI chat completion request â€” bug,stale â€” by teds-lin (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22422](https://github.com/vllm-project/vllm/issues/22422) [Feature]: mxfp4 support for 3090 â€” feature request,stale â€” by ehartford (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22501](https://github.com/vllm-project/vllm/issues/22501) [Usage]: Running a 300-400B Parameter Model on Multi-Node Setup (2x 8xA100) â€” usage,stale â€” by rangehow (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22575](https://github.com/vllm-project/vllm/issues/22575) [Bug]: Vllm hangs when I use the offline engine with dp = 2 or more â€” bug,stale â€” by Stealthwriter (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22623](https://github.com/vllm-project/vllm/issues/22623) [Usage]: if openai-mirror/gpt-oss-20b  can run in A100? â€” usage,stale â€” by neverstoplearn (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#22624](https://github.com/vllm-project/vllm/issues/22624) [Bug]: 1.7B fp16 + 0.6B draft OOM with gpu_memory_utilization=0.9, while 4B int8 + 0.6B works fine on A800 80 GB â€” bug,stale â€” by kiexu (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#22639](https://github.com/vllm-project/vllm/issues/22639) [Bug]: function convert_lark_to_gbnf interpreting '#' to parse as lark commentaries â€” bug,stale â€” by renout-nicolas (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#30206](https://github.com/vllm-project/vllm/issues/30206) [Bug]: DeepSeek-V3.2 DeepGEMM RuntimeError â€” bug â€” by coval3nte (å…³é—­äº: 2025-12-09 22:55 (UTC+8))
- [#29840](https://github.com/vllm-project/vllm/issues/29840) [Bug]: LMCacheConnectorV1Impl has no attribute 'layerwise_storers' on remote full cache hit with layerwise mode â€” bug â€” by XinyiQiao (å…³é—­äº: 2025-12-10 01:11 (UTC+8))

### æ–°å¢ PR
- [#30390](https://github.com/vllm-project/vllm/pull/30390) fix: Update json features supported by xGrammar â€” structured-output,v1 â€” by johannesflommersfeld (åˆ›å»ºäº: 2025-12-10 20:51 (UTC+8))
- [#30391](https://github.com/vllm-project/vllm/pull/30391) [IMPROVEMENT] Change MistralReasoningParser behavior â€” æ— æ ‡ç­¾ â€” by juliendenize (åˆ›å»ºäº: 2025-12-10 21:02 (UTC+8))
- [#30389](https://github.com/vllm-project/vllm/pull/30389) Standardise `get_rope` to use `rope_parameters["partial_rotary_factor"]`, not `rotary_dim` â€” performance,llama,qwen,deepseek,gpt-oss â€” by hmellor (åˆ›å»ºäº: 2025-12-10 20:33 (UTC+8))
- [#30384](https://github.com/vllm-project/vllm/pull/30384) [BugFix] Fix minimax m2 model rotary_dim â€” ready â€” by rogeryoungh (åˆ›å»ºäº: 2025-12-10 18:37 (UTC+8))
- [#30340](https://github.com/vllm-project/vllm/pull/30340) Add Eagle and Eagle3 support to Transformers modeling backend â€” æ— æ ‡ç­¾ â€” by hmellor (åˆ›å»ºäº: 2025-12-09 22:09 (UTC+8))
- [#30388](https://github.com/vllm-project/vllm/pull/30388) [Docs] Generate full list of metrics in user docs â€” documentation â€” by markmc (åˆ›å»ºäº: 2025-12-10 19:50 (UTC+8))
- [#30386](https://github.com/vllm-project/vllm/pull/30386) [v1] Add PrefixLM support to TritonAttention backend â€” v1 â€” by Isotr0py (åˆ›å»ºäº: 2025-12-10 19:32 (UTC+8))
- [#30385](https://github.com/vllm-project/vllm/pull/30385) [Core] Whisper support `torch.compile` â€” v1 â€” by NickLucche (åˆ›å»ºäº: 2025-12-10 19:11 (UTC+8))
- [#30376](https://github.com/vllm-project/vllm/pull/30376) [Fix]fix import error from lmcache â€” kv-connector â€” by wz1qqx (åˆ›å»ºäº: 2025-12-10 16:38 (UTC+8))
- [#30348](https://github.com/vllm-project/vllm/pull/30348) [Docs]: adds a new metric vllm:request_prefill_kv_computed_tokens in docs â€” documentation â€” by googs1025 (åˆ›å»ºäº: 2025-12-09 23:24 (UTC+8))
- [#30361](https://github.com/vllm-project/vllm/pull/30361) [Attention][AMD] Make flash-attn optional â€” rocm,speculative-decoding,v1 â€” by mgehre-amd (åˆ›å»ºäº: 2025-12-10 06:46 (UTC+8))
- [#30349](https://github.com/vllm-project/vllm/pull/30349) [BugFix] Fix minimax m2 model rope_parameters â€” æ— æ ‡ç­¾ â€” by esmeetu (åˆ›å»ºäº: 2025-12-09 23:42 (UTC+8))
- [#30351](https://github.com/vllm-project/vllm/pull/30351) [Bugfix] Cache added_vocab to avoid per-token overhead â€” ready â€” by scratch-ml (åˆ›å»ºäº: 2025-12-10 00:57 (UTC+8))
- [#30364](https://github.com/vllm-project/vllm/pull/30364) [Bugfix] awq_gemm: fix argument order swap â€” æ— æ ‡ç­¾ â€” by mgehre-amd (åˆ›å»ºäº: 2025-12-10 07:17 (UTC+8))
- [#30377](https://github.com/vllm-project/vllm/pull/30377) adding constraint updates of cos-sin to improve mrope performance â€” æ— æ ‡ç­¾ â€” by wujinyuan1 (åˆ›å»ºäº: 2025-12-10 16:48 (UTC+8))
- [#30373](https://github.com/vllm-project/vllm/pull/30373) Implement LMDB-based multi-modal cache â€” ci/build,v1,multi-modality â€” by petersalas (åˆ›å»ºäº: 2025-12-10 15:21 (UTC+8))
- [#30371](https://github.com/vllm-project/vllm/pull/30371) [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output â€” structured-output,ready,v1,deepseek â€” by chaunceyjiang (åˆ›å»ºäº: 2025-12-10 12:26 (UTC+8))
- [#30344](https://github.com/vllm-project/vllm/pull/30344) [Bugfix] Fix HunyuanOCR cross-image contamination in batch processing â€” æ— æ ‡ç­¾ â€” by anker-c2 (åˆ›å»ºäº: 2025-12-09 22:49 (UTC+8))
- [#30346](https://github.com/vllm-project/vllm/pull/30346) [Core] Major fix catch backend grammar exceptions (xgrammar, outlines, etc) in scheduler â€” v1 â€” by blancsw (åˆ›å»ºäº: 2025-12-09 22:58 (UTC+8))
- [#30347](https://github.com/vllm-project/vllm/pull/30347) [cpu][ci] Add CPU Attention Tests for Neon Backend â€” ready â€” by fadara01 (åˆ›å»ºäº: 2025-12-09 23:14 (UTC+8))
- [#30339](https://github.com/vllm-project/vllm/pull/30339) [CMake][Build]: Remove unused ACL CMake env variables â€” ready,ci/build â€” by Radu2k (åˆ›å»ºäº: 2025-12-09 22:09 (UTC+8))
- [#30369](https://github.com/vllm-project/vllm/pull/30369) [Fix] Add default rope theta for qwen1 model â€” qwen â€” by iwzbi (åˆ›å»ºäº: 2025-12-10 10:36 (UTC+8))
- [#30345](https://github.com/vllm-project/vllm/pull/30345) Fix typos in comments across multiple files â€” documentation,ready,v1 â€” by wilsonwu (åˆ›å»ºäº: 2025-12-09 22:56 (UTC+8))
- [#30367](https://github.com/vllm-project/vllm/pull/30367) [CI] Reduce Flakiness For test_spec_decode.py::test_suffix_decoding_acceptance â€” ready,v1 â€” by micah-wil (åˆ›å»ºäº: 2025-12-10 08:18 (UTC+8))
- [#30363](https://github.com/vllm-project/vllm/pull/30363) Remove all2all backend envvar â€” documentation,ci/build â€” by elizabetht (åˆ›å»ºäº: 2025-12-10 07:09 (UTC+8))
- [#30366](https://github.com/vllm-project/vllm/pull/30366) [Bug Fix] Fix Kimi-Linear model initialization crash due to missing 'indexer_rotary_emb' arg â€” æ— æ ‡ç­¾ â€” by yonasTMC (åˆ›å»ºäº: 2025-12-10 08:02 (UTC+8))
- [#30357](https://github.com/vllm-project/vllm/pull/30357) Upstream fp8 with static scales gpt oss â€” needs-rebase,gpt-oss â€” by maleksan85 (åˆ›å»ºäº: 2025-12-10 03:49 (UTC+8))
- [#30365](https://github.com/vllm-project/vllm/pull/30365) fix(gguf): Auto-select compatible dtype for GGUF models on Blackwell â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-10 07:59 (UTC+8))
- [#30362](https://github.com/vllm-project/vllm/pull/30362) [WIP] Bump dockerfile to cuda 13.0.2 (for testing) â€” ci/build,nvidia â€” by dougbtv (åˆ›å»ºäº: 2025-12-10 06:51 (UTC+8))
- [#30353](https://github.com/vllm-project/vllm/pull/30353) [Fix] Handle multiple tool calls in Qwen3-MTP tool parser â€” frontend,tool-calling,qwen â€” by ArkVex (åˆ›å»ºäº: 2025-12-10 01:48 (UTC+8))
- [#30356](https://github.com/vllm-project/vllm/pull/30356) [CI][DeepSeek] Add nightly DeepSeek R1 `lm_eval` tests on H200 â€” ready,ci/build,deepseek â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-10 02:05 (UTC+8))
- [#30352](https://github.com/vllm-project/vllm/pull/30352) [CI/Test] Fix FP8 per-tensor quant test reference scale shape â€” ready â€” by LucasWilkinson (åˆ›å»ºäº: 2025-12-10 01:30 (UTC+8))
- [#30354](https://github.com/vllm-project/vllm/pull/30354) [WIP][Core] Update PyTorch to 2.9.1 generally â€” rocm,ci/build,nvidia â€” by orionr (åˆ›å»ºäº: 2025-12-10 01:49 (UTC+8))
- [#30355](https://github.com/vllm-project/vllm/pull/30355) [Model Runner V2] Fix Triton warning on tl.where â€” v1 â€” by WoosukKwon (åˆ›å»ºäº: 2025-12-10 01:56 (UTC+8))
- [#30350](https://github.com/vllm-project/vllm/pull/30350) Remove virtual engine handling â€” tpu,needs-rebase,v1,codex,qwen,kv-connector â€” by WoosukKwon (åˆ›å»ºäº: 2025-12-10 00:34 (UTC+8))
- [#30341](https://github.com/vllm-project/vllm/pull/30341) [CI] refine more logic when generating and using nightly wheels & indices â€” ci/build â€” by Harry-Chen (åˆ›å»ºäº: 2025-12-09 22:17 (UTC+8))
- [#30338](https://github.com/vllm-project/vllm/pull/30338) Fix gigachat3 parser + update tests â€” frontend,tool-calling â€” by ajpqs (åˆ›å»ºäº: 2025-12-09 21:37 (UTC+8))

### å·²åˆå¹¶ PR
- [#30384](https://github.com/vllm-project/vllm/pull/30384) [BugFix] Fix minimax m2 model rotary_dim â€” ready â€” by rogeryoungh (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30062](https://github.com/vllm-project/vllm/pull/30062) [CPU] Support for Whisper â€” ready,ci/build,v1,multi-modality â€” by aditew01 (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30331](https://github.com/vllm-project/vllm/pull/30331) [Bugfix] tpu_model_runner: set vllm config context when calling reset_dynamo_cache() â€” tpu,ready,v1 â€” by dtrifiro (åˆå¹¶äº: 2025-12-10 20:58 (UTC+8))
- [#30351](https://github.com/vllm-project/vllm/pull/30351) [Bugfix] Cache added_vocab to avoid per-token overhead â€” ready â€” by scratch-ml (åˆå¹¶äº: 2025-12-10 12:05 (UTC+8))
- [#30332](https://github.com/vllm-project/vllm/pull/30332) [BUGFIX] Mistral tool call parser v11+ â€” frontend,ready,tool-calling â€” by juliendenize (åˆå¹¶äº: 2025-12-09 22:55 (UTC+8))
- [#30371](https://github.com/vllm-project/vllm/pull/30371) [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output â€” structured-output,ready,v1,deepseek â€” by chaunceyjiang (åˆå¹¶äº: 2025-12-10 16:30 (UTC+8))
- [#30347](https://github.com/vllm-project/vllm/pull/30347) [cpu][ci] Add CPU Attention Tests for Neon Backend â€” ready â€” by fadara01 (åˆå¹¶äº: 2025-12-10 13:37 (UTC+8))
- [#29358](https://github.com/vllm-project/vllm/pull/29358) [ROCm][CI] Attempt to fix the failures under a subgroup of the e2e the test group â€” rocm,ready,ci/build,v1,multi-modality â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-10 13:33 (UTC+8))
- [#30339](https://github.com/vllm-project/vllm/pull/30339) [CMake][Build]: Remove unused ACL CMake env variables â€” ready,ci/build â€” by Radu2k (åˆå¹¶äº: 2025-12-10 12:27 (UTC+8))
- [#30345](https://github.com/vllm-project/vllm/pull/30345) Fix typos in comments across multiple files â€” documentation,ready,v1 â€” by wilsonwu (åˆå¹¶äº: 2025-12-10 12:05 (UTC+8))
- [#30308](https://github.com/vllm-project/vllm/pull/30308) [bugfix][quantization] fix quark qwen3 kv_cache quantization â€” ready,qwen â€” by haoyangli-amd (åˆå¹¶äº: 2025-12-10 11:24 (UTC+8))
- [#30367](https://github.com/vllm-project/vllm/pull/30367) [CI] Reduce Flakiness For test_spec_decode.py::test_suffix_decoding_acceptance â€” ready,v1 â€” by micah-wil (åˆå¹¶äº: 2025-12-10 10:35 (UTC+8))
- [#30230](https://github.com/vllm-project/vllm/pull/30230) [responsesAPI][6] Fix multi turn MCP tokenization â€” documentation,frontend,ready,gpt-oss â€” by qandrew (åˆå¹¶äº: 2025-12-10 10:13 (UTC+8))
- [#30020](https://github.com/vllm-project/vllm/pull/30020) [CI/Build][AMD] Skip quantization kernels tests that require CUTLASS or e4m3fn when not supported by platform â€” rocm,ready,nvidia â€” by rasmith (åˆå¹¶äº: 2025-12-10 10:28 (UTC+8))
- [#30336](https://github.com/vllm-project/vllm/pull/30336) [Bugfix] Fix fp8 DeepGemm compilation issues â€” bug,ready,ci-failure,deepseek â€” by ElizaWszola (åˆå¹¶äº: 2025-12-10 09:17 (UTC+8))
- [#29624](https://github.com/vllm-project/vllm/pull/29624) [Attention] Make seq_lens_cpu optional in CommonAttentionMetadata to enable true async spec-decode â€” speculative-decoding,ready,v1,ready-run-all-tests â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-10 09:18 (UTC+8))
- [#30330](https://github.com/vllm-project/vllm/pull/30330) [Bugfix] Fix cuda graph sizes when running with speculative decoding â€” ready,nvidia â€” by PatrykSaffer (åˆå¹¶äº: 2025-12-10 08:47 (UTC+8))
- [#29723](https://github.com/vllm-project/vllm/pull/29723) [V1][Spec Decode] Optimize Medusa proposer to avoid GPU-CPU sync â€” speculative-decoding,ready,v1 â€” by dongbo910220 (åˆå¹¶äº: 2025-12-10 08:15 (UTC+8))
- [#29937](https://github.com/vllm-project/vllm/pull/29937) Improve wvsplitK tile and balance heristics. â€” rocm,ready â€” by amd-hhashemi (åˆå¹¶äº: 2025-12-10 07:51 (UTC+8))
- [#25693](https://github.com/vllm-project/vllm/pull/25693) [Rocm][torch.compile] Adding layernorm + fp8 block quant and silu + fp8 block quant for Aiter â€” rocm,ready â€” by charlifu (åˆå¹¶äº: 2025-12-10 06:39 (UTC+8))
- [#30119](https://github.com/vllm-project/vllm/pull/30119) [BugFix] Fix DeepSeek-R1 hang with DP and MTP â€” ready,v1,deepseek â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-10 02:51 (UTC+8))
- [#29066](https://github.com/vllm-project/vllm/pull/29066) [MoE][Refactor] Remove most arguments to FusedMoEMethodBase.apply â€” moe,ready,nvidia,ready-run-all-tests â€” by bnellnm (åˆå¹¶äº: 2025-12-10 05:48 (UTC+8))
- [#28480](https://github.com/vllm-project/vllm/pull/28480) [Quantization] FP8 Weight Reloading for Quantized RL Rollout â€” quantization,ready,rl â€” by kylesayrs (åˆå¹¶äº: 2025-12-10 05:54 (UTC+8))
- [#30277](https://github.com/vllm-project/vllm/pull/30277) [BugFix] Fix non detected failing tests â€” ready,ci/build â€” by ilmarkov (åˆå¹¶äº: 2025-12-10 01:57 (UTC+8))
- [#29145](https://github.com/vllm-project/vllm/pull/29145) [CI/Build] Make test_mha_attn.py run on correct platform only and check for flash_attn_varlen_func in layer.py â€” rocm,ready,ci/build â€” by rasmith (åˆå¹¶äº: 2025-12-10 04:18 (UTC+8))
- [#30234](https://github.com/vllm-project/vllm/pull/30234) Bump actions/stale from 10.1.0 to 10.1.1 â€” ready,dependencies,ci/build,github_actions â€” by dependabot (åˆå¹¶äº: 2025-12-10 04:12 (UTC+8))
- [#30233](https://github.com/vllm-project/vllm/pull/30233) Bump actions/checkout from 6.0.0 to 6.0.1 â€” ready,dependencies,ci/build,github_actions â€” by dependabot (åˆå¹¶äº: 2025-12-10 04:03 (UTC+8))
- [#30307](https://github.com/vllm-project/vllm/pull/30307) [Model][Quantization] Fix / Add GGUF support for Qwen2 MoE models â€” ready,qwen â€” by a4lg (åˆå¹¶äº: 2025-12-10 03:13 (UTC+8))
- [#30352](https://github.com/vllm-project/vllm/pull/30352) [CI/Test] Fix FP8 per-tensor quant test reference scale shape â€” ready â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-10 02:52 (UTC+8))
- [#29912](https://github.com/vllm-project/vllm/pull/29912) [Cleanup] Refactor profiling env vars into a CLI config â€” documentation,performance,structured-output,frontend,tpu,ready,v1 â€” by benchislett (åˆå¹¶äº: 2025-12-10 02:29 (UTC+8))
- [#30187](https://github.com/vllm-project/vllm/pull/30187) [Model Runner V2] Support num NaNs in logits â€” v1 â€” by WoosukKwon (åˆå¹¶äº: 2025-12-10 02:00 (UTC+8))
- [#30355](https://github.com/vllm-project/vllm/pull/30355) [Model Runner V2] Fix Triton warning on tl.where â€” v1 â€” by WoosukKwon (åˆå¹¶äº: 2025-12-10 01:59 (UTC+8))
- [#29897](https://github.com/vllm-project/vllm/pull/29897) [Compile] Fix torch warning `TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled` â€” ready,v1 â€” by yewentao256 (åˆå¹¶äº: 2025-12-09 23:40 (UTC+8))
- [#30298](https://github.com/vllm-project/vllm/pull/30298) Update AMD test definitions (2025-12-08) â€” rocm,ready,ci/build,amd â€” by Alexei-V-Ivanov-AMD (åˆå¹¶äº: 2025-12-10 01:31 (UTC+8))
- [#30173](https://github.com/vllm-project/vllm/pull/30173) [BugFix] Fix `assert  batch_descriptor.num_tokens == num_tokens_padded` â€” speculative-decoding,ready,v1,nvidia â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-09 23:36 (UTC+8))
- [#30018](https://github.com/vllm-project/vllm/pull/30018) [Feature] Batch-Invariant Support for FA2 and LoRA â€” ready,v1 â€” by quanliu1991 (åˆå¹¶äº: 2025-12-09 23:01 (UTC+8))
- [#25552](https://github.com/vllm-project/vllm/pull/25552) [ROCm] Aiter Quant Kernels â€” rocm,ready,ci/build â€” by vllmellm (åˆå¹¶äº: 2025-12-09 22:27 (UTC+8))

### å…³é—­ä½†æœªåˆå¹¶çš„ PR
- [#23997](https://github.com/vllm-project/vllm/pull/23997) Feature/sampler benchmark #23977 â€” performance,unstale â€” by baonudesifeizhai (å…³é—­äº: 2025-12-10 21:14 (UTC+8))
- [#26701](https://github.com/vllm-project/vllm/pull/26701) [ROCm]: W8A8BlockFp8LinearOp should use AITER on MI355X â€” rocm,ready,needs-rebase â€” by gronsti-amd (å…³é—­äº: 2025-12-10 20:33 (UTC+8))
- [#30348](https://github.com/vllm-project/vllm/pull/30348) [Docs]: adds a new metric vllm:request_prefill_kv_computed_tokens in docs â€” documentation â€” by googs1025 (å…³é—­äº: 2025-12-10 19:59 (UTC+8))
- [#30349](https://github.com/vllm-project/vllm/pull/30349) [BugFix] Fix minimax m2 model rope_parameters â€” æ— æ ‡ç­¾ â€” by esmeetu (å…³é—­äº: 2025-12-10 18:47 (UTC+8))
- [#29653](https://github.com/vllm-project/vllm/pull/29653) fix potential object has no attribute 'bias' error â€” æ— æ ‡ç­¾ â€” by allerou4 (å…³é—­äº: 2025-12-10 15:16 (UTC+8))
- [#30297](https://github.com/vllm-project/vllm/pull/30297) [Core] Add SLA-tiered scheduling (opt-in) and docs â€” documentation,v1 â€” by ProdByBuddha (å…³é—­äº: 2025-12-10 13:13 (UTC+8))
- [#30327](https://github.com/vllm-project/vllm/pull/30327) [BugFix] Fix hang issue in LMCache mp mode â€” v1,kv-connector â€” by wz1qqx (å…³é—­äº: 2025-12-10 10:32 (UTC+8))
- [#17830](https://github.com/vllm-project/vllm/pull/17830) cmake: Get rid of VLLM_PYTHON_EXECUTABLE â€” needs-rebase,ci/build,stale â€” by seemethere (å…³é—­äº: 2025-12-10 10:26 (UTC+8))
- [#17872](https://github.com/vllm-project/vllm/pull/17872) measure peak memory correctly by removing already used memory â€” needs-rebase,stale,v1 â€” by MiladInk (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#17959](https://github.com/vllm-project/vllm/pull/17959) [Bugfix] fix check kv cache memory log info â€” needs-rebase,stale,v1 â€” by BoL0150 (å…³é—­äº: 2025-12-10 10:25 (UTC+8))
- [#21056](https://github.com/vllm-project/vllm/pull/21056) [Feature][EPLB] Add EPLB support for MiniMax-01 â€” stale â€” by haveheartt (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21413](https://github.com/vllm-project/vllm/pull/21413) Intentionally fail parallel sampling test â€” stale,v1 â€” by sethkimmel3 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#21506](https://github.com/vllm-project/vllm/pull/21506) [V1][SpecDecode]Support relaxed acceptance for thinking tokens in speculative decoding in V1 â€” documentation,rocm,frontend,ci/build,stale,v1,multi-modality,tool-calling â€” by DW934 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22238](https://github.com/vllm-project/vllm/pull/22238) [V1][SpecDecode]Support Relaxed Acceptance for thinking tokens in speculative decoding when using greedy search, camp up by Nvidia. â€” stale,v1 â€” by DW934 (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22488](https://github.com/vllm-project/vllm/pull/22488) Feat/sliding window metrics â€” Related to #22480 â€” needs-rebase,stale,v1 â€” by NumberWan (å…³é—­äº: 2025-12-10 10:24 (UTC+8))
- [#22632](https://github.com/vllm-project/vllm/pull/22632) [Bugfix]fix deepseek_r1_reasoning bugs when <think> </think> in contents. â€” stale,deepseek â€” by z2415445508 (å…³é—­äº: 2025-12-10 10:23 (UTC+8))
- [#27594](https://github.com/vllm-project/vllm/pull/27594) Fix intermediatetensors spawn error #27591 â€” qwen â€” by baonudesifeizhai (å…³é—­äº: 2025-12-10 08:44 (UTC+8))
- [#28627](https://github.com/vllm-project/vllm/pull/28627) [Weight Loading] Expand quantized weight reloading support â€” needs-rebase,v1 â€” by kylesayrs (å…³é—­äº: 2025-12-10 05:48 (UTC+8))
- [#30354](https://github.com/vllm-project/vllm/pull/30354) [WIP][Core] Update PyTorch to 2.9.1 generally â€” rocm,ci/build,nvidia â€” by orionr (å…³é—­äº: 2025-12-10 02:46 (UTC+8))
- [#30063](https://github.com/vllm-project/vllm/pull/30063) Mistral tool parser â€” frontend,tool-calling â€” by graelo (å…³é—­äº: 2025-12-09 23:56 (UTC+8))
- [#27305](https://github.com/vllm-project/vllm/pull/27305) [ROCm][torch.compile] Adding MulAddFusionPass to enable AITER fused_mul_add â€” rocm,needs-rebase â€” by micah-wil (å…³é—­äº: 2025-12-09 23:49 (UTC+8))
- [#26257](https://github.com/vllm-project/vllm/pull/26257) [Feature][torch.compile] Add pass to rearrange AllGather for FP8 models in sequence parallel for better Async TP fusion â€” needs-rebase,ci/build â€” by jasonlizhengjian (å…³é—­äº: 2025-12-09 22:59 (UTC+8))
- [#25618](https://github.com/vllm-project/vllm/pull/25618) [ROCm][Allreduce] Add dispatch mechanism for choosing performant allreduce implementations for AMD platforms â€” rocm,needs-rebase,nvidia â€” by zejunchen-zejun (å…³é—­äº: 2025-12-09 21:45 (UTC+8))