# vLLM å¼€å‘åŠ¨æ€æŠ¥å‘Š - 2025-12-11

> æ—¶é—´çª—å£: 2025-12-11 10:48 (UTC+8) ~ 2025-12-12 10:48 (UTC+8)
> æ•°æ®ç»Ÿè®¡: æ–° Issue 15 | å…³é—­ Issue 23 | æ–° PR 68 | åˆå¹¶ PR 45 | å…³é—­æœªåˆå¹¶ PR 27

---

### ğŸ“Š æ¯æ—¥å¼€å‘çŠ¶æ€æ‘˜è¦
åœ¨2025å¹´12æœˆ11æ—¥è‡³12æ—¥çš„24å°æ—¶å†…ï¼ŒvLLMé¡¹ç›®ä¿æŒäº†æé«˜çš„å¼€å‘æ´»è·ƒåº¦ï¼Œåˆå¹¶äº†45ä¸ªPRå¹¶å¤„ç†äº†38ä¸ªIssueã€‚å¼€å‘ç„¦ç‚¹é›†ä¸­åœ¨æ€§èƒ½ä¼˜åŒ–ã€å†…å­˜ç®¡ç†ã€å¯¹æ–°ç¡¬ä»¶ï¼ˆå¦‚Blackwell Ultraï¼‰çš„æ”¯æŒä»¥åŠå¯¹è¿‘æœŸçƒ­é—¨æ¨¡å‹ï¼ˆå¦‚DeepSeek-V3.2ã€GPT-OSSï¼‰çš„é—®é¢˜ä¿®å¤ä¸Šã€‚AMD/ROCmå¹³å°çš„æ”¯æŒå·¥ä½œä¸»è¦ä½“ç°åœ¨æŒç»­é›†æˆï¼ˆCIï¼‰çš„æµ‹è¯•å®Œå–„å’Œç‰¹å®šå†…æ ¸çš„é€‚é…ä¸Šã€‚

### ğŸ¯ AMD/ROCm ç”Ÿæ€ç›¸å…³åŠ¨æ€
æœ¬å‘¨æœŸå†…ï¼Œç›´æ¥çš„AMDåŠŸèƒ½å¼€å‘PRè¾ƒå°‘ï¼Œä¸»è¦å·¥ä½œé›†ä¸­åœ¨CIæµ‹è¯•çš„å®Œå–„å’Œé—®é¢˜ä¿®å¤ä¸Šï¼š
1.  **CIä¸æµ‹è¯•é€‚é…**ï¼š
    *   **PR #30508 (å·²åˆå¹¶)** ä¸ **PR #30417 (å·²åˆå¹¶)**ï¼šç”±AMDè´¡çŒ®è€…ï¼ˆ`rasmith`ï¼‰æäº¤ï¼Œåˆ†åˆ«è·³è¿‡äº†ROCmä¸æ”¯æŒçš„`cutlass_w4a8_moe`æµ‹è¯•å’Œç¼ºå°‘ç‰¹å®šæ¨¡å—ï¼ˆå¦‚`deep_ep`ï¼‰çš„èåˆæµ‹è¯•ï¼Œç¡®ä¿ROCmæµ‹è¯•æµæ°´çº¿èƒ½å¤Ÿé¡ºåˆ©é€šè¿‡ã€‚
    *   **PR #30526 (å·²åˆå¹¶)**ï¼šå°†V1 e2eæµ‹è¯•çš„ä»£ç†æ± ä»`mi325_1`è°ƒæ•´ä¸º`mi325_4`ï¼Œä»¥ç¡®ä¿å¤šGPUæµ‹è¯•æœ‰è¶³å¤Ÿçš„ç¡¬ä»¶èµ„æºã€‚
    *   **PR #30527 (å¼€æ”¾ä¸­)**ï¼šè®¡åˆ’ä¸ºROCmç¯å¢ƒä¸‹çš„æ¨æµ‹è§£ç å¤šGPUæµ‹è¯•æ·»åŠ GPUå¯ç”¨æ€§æ£€æŸ¥ï¼Œé˜²æ­¢å› èµ„æºä¸è¶³å¯¼è‡´çš„æµ‹è¯•å¤±è´¥ã€‚
2.  **Issue è¿½è¸ªå…³é—­**ï¼š
    *   **Issue #14964 (å·²å…³é—­)**ï¼šè¿½è¸ªAMD AITERå†…æ ¸é›†æˆçš„æ€»è§ˆæ€§Issueè¢«ç»´æŠ¤è€…ä¸»åŠ¨å…³é—­ï¼Œå› å…¶â€œè¿‡äºé™ˆæ—§â€ï¼Œå¹¶å»ºè®®ç›¸å…³æ–¹å¼€æ–°çš„è¿½è¸ªå™¨ã€‚
    *   **Issue #22698 (å·²å…³é—­)**ï¼šå…³äºMI300/MI308ä¸ŠCUDAå›¾è¿è¡Œæœ«å°¾å­˜åœ¨â€œæ°”æ³¡â€çš„æ€§èƒ½é—®é¢˜ã€‚ç»AMDå·¥ç¨‹å¸ˆï¼ˆ`vllmellm`ï¼‰ç¡®è®¤ï¼Œåœ¨æœ€æ–°çš„`rocm/vllm-dev:nightly`é•œåƒä¸­æ­¤é—®é¢˜å·²ä¸å¤ç°ï¼Œæ•…å…³é—­ã€‚
3.  **Bugä¿®å¤ä¸æ”¯æŒç¡®è®¤**ï¼š
    *   **Issue #28184 (å·²å…³é—­)**ï¼šå…³äºWhisperç¼–ç å™¨-è§£ç å™¨æ¨¡å‹åœ¨vLLM 0.11ä¸ROCmä¸Šä¸å·¥ä½œçš„é—®é¢˜ã€‚AMDå·¥ç¨‹å¸ˆç¡®è®¤ç›¸å…³PRå·²åˆå¹¶ï¼Œåœ¨æœ€æ–°nightlyé•œåƒä¸­é—®é¢˜å·²è§£å†³ã€‚

**åˆ†æ**ï¼šæœ¬å‘¨æœŸAMDç”Ÿæ€çš„åŠ¨æ€ä»¥â€œç¨³â€ä¸ºä¸»ï¼Œä¾§é‡äºç¡®ä¿ç°æœ‰åŠŸèƒ½åœ¨CIä¸­çš„ç¨³å®šæ€§å’Œä¿®å¤å·²çŸ¥é—®é¢˜ï¼Œè€Œéå¼•å…¥æ–°åŠŸèƒ½ã€‚å…³é—­å†å²é—ç•™çš„è¿½è¸ªIssueä¹Ÿè¡¨æ˜é¡¹ç›®åœ¨æ¢³ç†å’Œä¼˜åŒ–é—®é¢˜ç®¡ç†æµç¨‹ã€‚

### ğŸ’¬ é«˜çƒ­åº¦è®¨è®ºåˆ†æ
1.  **Issue #30453: DeepSeek-V3.2 é¢‘ç¹é‡å¤è§£ç é—®é¢˜**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šç”¨æˆ·æŠ¥å‘Šåœ¨H200ä¸Šéƒ¨ç½²DeepSeek-V3.2æ—¶ï¼Œé‡åˆ°ä¸¥é‡çš„é‡å¤è§£ç ï¼ˆRepetitive Decodingï¼‰é—®é¢˜ã€‚
    *   **è§‚ç‚¹ä¸è¿›å±•**ï¼š
        *   æé—®è€… `makabaka6338` æä¾›äº†æå…¶è¯¦å°½çš„ç¯å¢ƒä¿¡æ¯å’Œè¯·æ±‚è„šæœ¬ã€‚
        *   å…¶ä»–ç”¨æˆ· `mondaylord` å’Œ `niceallen` è¡¨ç¤ºé‡åˆ°ç±»ä¼¼é—®é¢˜ï¼ˆæµå“åº”é¦–tokenè¢«æˆªæ–­æˆ–â€œ+1â€ï¼‰ã€‚
        *   è®¨è®ºç„¦ç‚¹è¿…é€Ÿè½¬å‘ç¯å¢ƒç‰ˆæœ¬æ’æŸ¥ã€‚ç”¨æˆ· `h1248759474` åå¤è¯¢é—®å…·ä½“çš„ä¾èµ–åŒ…ç‰ˆæœ¬ï¼Œæé—®è€…æœ€ç»ˆåˆ—å‡ºäº†å®Œæ•´çš„`pip list`è¾“å‡ºã€‚
    *   **å½“å‰çŠ¶æ€**ï¼šé—®é¢˜ä»ä¸º **Open**ã€‚è®¨è®ºå°šæœªæ¶‰åŠæ ¹æœ¬åŸå› åˆ†æï¼Œç›®å‰åœç•™åœ¨ä¿¡æ¯æ”¶é›†å’Œç¡®è®¤å…±æ€§ç°è±¡é˜¶æ®µã€‚

2.  **Issue #30498: GPT-OSS-120B åœ¨è¾¾åˆ° max_tokens æ—¶è¿”å›ç©ºå†…å®¹**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šç”¨æˆ·å‘ç°GPT-OSS-120Bæ¨¡å‹åœ¨é¢„çƒ­åï¼ˆå³å‘½ä¸­å‰ç¼€ç¼“å­˜åï¼‰ï¼Œå³ä½¿`max_tokens`è®¾ç½®å¾—è¶³å¤Ÿå¤§ï¼Œè¿”å›çš„`content`å­—æ®µä¹Ÿä¸º`null`ï¼Œè€Œä½¿ç”¨`--enforce-eager`æ ‡å¿—å¯ä»¥è§„é¿ã€‚
    *   **è§‚ç‚¹åˆ†æ**ï¼š
        *   **ç”¨æˆ·è§†è§’ (`ashtarkb`)**: æä¾›å®Œæ•´å¤ç°æ­¥éª¤ï¼Œè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªBugã€‚
        *   **åˆæ­¥è§£é‡Š (`alew3`)**: è®¤ä¸ºå¦‚æœæ¨ç†ï¼ˆreasoningï¼‰ç”¨å®Œäº†æ‰€æœ‰ä»¤ç‰Œé…é¢ï¼Œ`content`ä¸ºç©ºæ˜¯é¢„æœŸè¡Œä¸ºï¼Œå¹¶å»ºè®®æ”¹è¿›æ£€æŸ¥ä»£ç ã€‚
        *   **æ·±åº¦åˆ†æ (`bbrowning`)**: æŒ‡å‡ºè¿™å¾ˆå¯èƒ½ä¸Issue #26480æ˜¯åŒä¸€ä¸ªBugâ€”â€”å³å‘½ä¸­å‰ç¼€ç¼“å­˜åï¼ŒCUDAå›¾å¯èƒ½å¯¼è‡´æ— é™ç”Ÿæˆ`token id 0`ã€‚`--enforce-eager`æœ‰æ•ˆæ˜¯å› ä¸ºå®ƒç¦ç”¨äº†CUDAå›¾ã€‚è¿™ä¸ªè§‚ç‚¹å¾—åˆ°äº†æé—®è€…çš„è®¤å¯ã€‚
    *   **å½“å‰çŠ¶æ€**ï¼šé—®é¢˜ä»ä¸º **Open**ã€‚è®¨è®ºæŒ‡å‘ä¸€ä¸ªå·²çŸ¥çš„ã€ä¸ç¼“å­˜å’ŒCUDAå›¾ç›¸å…³çš„æ ¸å¿ƒBugï¼Œéœ€è¦æ›´æ·±å±‚çš„ä¿®å¤ã€‚

3.  **Issue #30521: Flashinfer åç«¯å†…å­˜åˆ†æå›å½’ (DP > 1)**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šåœ¨æ•°æ®å¹¶è¡Œï¼ˆDP>1ï¼‰æ—¶ï¼Œä½¿ç”¨Flashinferåç«¯çš„vLLM 0.12.0åœ¨å†…å­˜åˆ†æï¼ˆprofilingï¼‰é˜¶æ®µå­˜åœ¨å›å½’ï¼Œå¯èƒ½å¯¼è‡´OOMã€‚
    *   **è§‚ç‚¹åˆ†æ**ï¼š
        *   æŠ¥å‘Šè€… `nrghosh` è‡ªå·±è¿›è¡Œäº†æ·±å…¥åˆ†æï¼ŒæŒ‡å‡ºé—®é¢˜å…³é”®åœ¨äºï¼šå†…å­˜åˆ†æåœ¨CUDAå›¾æ•è·å’Œé‡‡æ ·å™¨é¢„çƒ­**ä¹‹å‰**è¿è¡Œï¼Œå› æ­¤æœªè®¡å…¥è¿™éƒ¨åˆ†å†…å­˜ï¼ˆç‰¹åˆ«æ˜¯Flashinferè¾ƒå¤§çš„å·¥ä½œåŒºç¼“å†²åŒºï¼‰ï¼Œå¯¼è‡´KVç¼“å­˜åˆ†é…è¿‡å¤§ã€‚
        *   ä»–æå‡ºäº†å‡ ä¸ªæ€è€ƒæ–¹å‘ï¼š1) åœ¨è®¡ç®—KVç¼“å­˜å‰ä¸ºé‡‡æ ·å™¨é¢„çƒ­é¢„ç•™å†…å­˜ï¼›2) ä½¿ç”¨æ›´å°çš„é¢„çƒ­æ‰¹æ¬¡ï¼›3) æ¢ç©¶Flashinfer CUDAå›¾å†…å­˜ä½¿ç”¨æ˜¯FlexAttentionä¸¤å€çš„åŸå› ã€‚
    *   **å½“å‰çŠ¶æ€**ï¼šé—®é¢˜ä»ä¸º **Open**ã€‚è¿™æ˜¯ä¸€ä¸ªç”±ç»éªŒä¸°å¯Œçš„è´¡çŒ®è€…æå‡ºçš„ã€å¸¦æœ‰è‡ªæˆ‘åˆ†æçš„æ·±åº¦æŠ€æœ¯è®¨è®ºï¼Œç›´æ¥æŒ‡å‘å†…å­˜ç®¡ç†æµç¨‹çš„æ ¸å¿ƒé€»è¾‘ã€‚

4.  **Issue #30139 (å·²å…³é—­): æ¨ç†æ¨¡å‹ä¸è¿›è¡Œæ¨ç†ï¼Œå°†ç»“æœå…¨æ”¾å…¥ reasoning_content**
    *   **æ ¸å¿ƒè®®é¢˜**ï¼šå¤šä¸ªç”¨æˆ·æŠ¥å‘ŠMistral Ministralç­‰æ¨ç†æ¨¡å‹åœ¨vLLMä¸­æœªæ­£ç¡®åˆ†ç¦»æ¨ç†å’Œå›ç­”å†…å®¹ã€‚
    *   **è§‚ç‚¹ä¸ç»“è®º**ï¼š
        *   **ç”¨æˆ·å›°æƒ‘**ï¼šå¤šä¸ªç”¨æˆ·ç¡®è®¤äº†æ­¤é—®é¢˜ï¼Œå½±å“Ministralã€MiniMax-M2ç­‰æ¨¡å‹ã€‚
        *   **æ ¹æœ¬åŸå› **ï¼šMistralçš„æ¨ç†è§£æå™¨ï¼ˆ`MistralReasoningParser`ï¼‰æœ€åˆå€Ÿé‰´äº†DeepSeek V1çš„å®ç°ï¼ŒåŒ…å«äº†ä¸€äº›å¯å‘å¼è§„åˆ™ï¼ˆå¦‚æ²¡æœ‰å¼€å§‹æ ‡ç­¾åˆ™å…¨è§†ä¸ºæ¨ç†ï¼‰ï¼Œè¿™ä¸é€‚åˆMistralæ¨¡å‹å¯åˆ‡æ¢â€œèŠå¤©/æ¨ç†â€æ¨¡å¼çš„ç‰¹æ€§ã€‚
        *   **è§£å†³æ–¹æ¡ˆ**ï¼šMistralå‘˜å·¥ `juliendenize` æäº¤äº† **PR #30391**ï¼Œæ”¹è¿›äº†è§£æå™¨çš„é€»è¾‘ï¼Œä½¿å…¶æ›´ç¬¦åˆMistralæ¨¡å‹çš„è¡Œä¸ºã€‚è¯¥PRå·²åˆå¹¶å¹¶è§£å†³äº†æ­¤é—®é¢˜ã€‚
    *   **æœ€ç»ˆç»“è®º**ï¼šé€šè¿‡ä¸Šæ¸¸æ¨¡å‹æä¾›æ–¹çš„ç›´æ¥è´¡çŒ®ï¼Œé—®é¢˜å¾—åˆ°ä¿®å¤ã€‚è¿™ä½“ç°äº†ç¤¾åŒºä¸æ¨¡å‹å¼€å‘å•†åˆä½œçš„é«˜æ•ˆæ€§ã€‚

### ğŸ”¥ çƒ­é—¨è¯é¢˜ä¸è¶‹åŠ¿åˆ†æ
1.  **æ–°æ¨¡å‹æ”¯æŒä¸é—®é¢˜**ï¼š**DeepSeek-V3.2** å’Œ **GPT-OSS-120B** æ˜¯ç»å¯¹çš„ç„¦ç‚¹ï¼Œç›¸å…³Issueè®¨è®ºçƒ­çƒˆã€‚è¿™åæ˜ å‡ºç¤¾åŒºå¿«é€Ÿé‡‡ç”¨å‰æ²¿å¤§æ¨¡å‹ï¼ŒåŒæ—¶vLLMåœ¨é€‚é…è¿™äº›æ¶æ„å¤æ‚çš„æ–°æ¨¡å‹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼ˆå¦‚é‡å¤è§£ç ã€ç¼“å­˜Bugï¼‰ã€‚
2.  **éƒ¨ç½²ä¸ç¡¬ä»¶é€‚é…**ï¼š
    *   **æ–°ç¡¬ä»¶**ï¼šPR #30484 å¼€å§‹ä¸º **NVIDIA Blackwell Ultra (SM103)** æ·»åŠ åˆå§‹æ”¯æŒã€‚
    *   **CPUåç«¯**ï¼šé’ˆå¯¹ **Arm CPU** çš„æ„å»ºé—®é¢˜ï¼ˆIssue #30470ï¼‰å’Œæ€§èƒ½ä¼˜åŒ–ï¼ˆIssue #30487ï¼ŒL2ç¼“å­˜æ£€æµ‹ï¼‰å—åˆ°å…³æ³¨ï¼Œæ˜¾ç¤ºç§»åŠ¨ç«¯å’Œè¾¹ç¼˜è®¡ç®—åœºæ™¯çš„é‡è¦æ€§ã€‚
    *   **å®‰è£…é—®é¢˜**ï¼šå¦‚ä½•ç¦»çº¿æˆ–é«˜æ•ˆä½¿ç”¨é¢„ç¼–è¯‘wheelï¼ˆIssue #30464ï¼‰æ˜¯å®é™…éƒ¨ç½²ä¸­çš„å¸¸è§ç—›ç‚¹ã€‚
3.  **æ€§èƒ½ä¸å†…å­˜ä¼˜åŒ–**ï¼šè¿™å§‹ç»ˆæ˜¯vLLMçš„æ ¸å¿ƒè®®é¢˜ã€‚æœ¬æœŸå›´ç»•**CUDAå›¾å†…å­˜ä¼°ç®—**ï¼ˆPR #30515ï¼‰ã€**ç¼–ç å™¨ç¼“å­˜å†…å­˜å ç”¨**ï¼ˆPR #30452, #30475ï¼‰ã€**Tritonå†…æ ¸split_kè®¾ç½®**ï¼ˆPR #30528ï¼‰çš„è®¨è®ºï¼Œéƒ½æ˜¯å¯¹æè‡´æ€§èƒ½çš„è¿½æ±‚ã€‚
4.  **æ¨ç†ä¸å·¥å…·è°ƒç”¨**ï¼šå…³äºå¦‚ä½•ç¦ç”¨Qwenæ¨¡å‹çš„â€œæ€è€ƒâ€ï¼ˆIssue #30477ï¼‰ï¼Œä»¥åŠæ¨ç†æ¨¡å‹è¾“å‡ºè§£æï¼ˆIssue #30139, #28852ï¼‰çš„è®¨è®ºï¼Œè¯´æ˜â€œæ¨ç†â€å’Œâ€œå·¥å…·è°ƒç”¨â€å·²æˆä¸ºLLMåº”ç”¨çš„é«˜çº§ç‰¹æ€§ï¼Œå¯¹å…¶å¯é æ§åˆ¶çš„éœ€æ±‚å¼ºçƒˆã€‚

### ğŸ› ï¸ é‡ç‚¹æŠ€æœ¯å˜æ›´
1.  **PR #30515: åœ¨å†…å­˜åˆ†æä¸­è®¡å…¥CUDAå›¾å ç”¨**ï¼šè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æ”¹è¿›ã€‚é€šè¿‡åœ¨å†…å­˜åˆ†æé˜¶æ®µè¿›è¡Œä¸€æ¬¡å°çš„ã€å‡çš„CUDAå›¾æ•è·ï¼Œæ¥é¢„ä¼°å…¶çœŸå®å†…å­˜å ç”¨ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è®¡ç®—å¯ç”¨çš„KVç¼“å­˜ç©ºé—´ã€‚è¿™å°†æœ‰æ•ˆå‡å°‘å› ä½ä¼°CUDAå›¾å†…å­˜è€Œå¯¼è‡´çš„å¯åŠ¨åOOMé—®é¢˜ï¼Œå¯¹äºDeepSeek-R1ç­‰å¤§æ¨¡å‹åœ¨æé™é…ç½®ä¸‹ç¨³å®šè¿è¡Œè‡³å…³é‡è¦ã€‚
2.  **PR #30452 / #30475: ä¼˜åŒ–ç¼–ç å™¨ç¼“å­˜å†…å­˜ä½¿ç”¨**ï¼šè¿™ä¸¤ä¸ªå…³è”PRé’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹çš„ç¼–ç å™¨ç¼“å­˜è¿›è¡Œäº†é‡æ„ã€‚é€šè¿‡ä»…å­˜å‚¨å®é™…çš„åµŒå…¥å‘é‡è€Œéå®Œæ•´çš„å ä½ç¬¦å¼ é‡ï¼Œå®ç°äº†é«˜è¾¾12.5å€çš„å†…å­˜èŠ‚çœã€‚è¿™ä½¿å¾—å¦‚Qwen3-VLç­‰å¤§å‹è§†è§‰æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„GPUå†…å­˜ä¸‹æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡æˆ–æ›´å¤šè§†é¢‘å¸§ï¼Œæ˜¯å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å…³é”®æå‡ã€‚
3.  **PR #30484: æ·»åŠ SM103 (Blackwell Ultra) æ”¯æŒ**ï¼šä¸ºæœªæ¥çš„NVIDIA B300/GB300ç³»åˆ—GPUæ·»åŠ äº†åˆå§‹æ”¯æŒã€‚è™½ç„¶éœ€è¦é…åˆTritonè¡¥ä¸ï¼Œä½†è¿™æ˜¯ä¿æŒvLLMåœ¨æœ€æ–°ç¡¬ä»¶ä¸Šé¢†å…ˆåœ°ä½çš„å¿…è¦æ­¥éª¤ã€‚
4.  **PR #30528: ä¸ºTritonå†…æ ¸è®¾ç½® split_k=1**ï¼šé’ˆå¯¹Hopperæ¶æ„ï¼ˆSM90ï¼‰çš„æ€§èƒ½è°ƒä¼˜ã€‚é€šè¿‡å¼ºåˆ¶è®¾ç½®`split_k=1`ï¼Œé¿å…å°æ‰¹æ¬¡æ¨ç†æ—¶Tritonå†…æ ¸å›é€€åˆ°float32ç²¾åº¦ï¼Œä»è€Œæå‡å°æ‰¹æ¬¡åœºæ™¯ä¸‹çš„æ¨ç†é€Ÿåº¦ã€‚è¿™ä½“ç°äº†å¯¹çœŸå®ç”Ÿäº§åœºæ™¯ä¸­å¤šæ ·åŒ–è´Ÿè½½çš„ç²¾ç»†åŒ–ä¼˜åŒ–ã€‚
5.  **PR #30389: æ ‡å‡†åŒ– RoPE partial_rotary_factor è·å–**ï¼šä¸€ä¸ªé‡è¦çš„åº•å±‚é‡æ„ï¼Œç»Ÿä¸€äº†ä»é…ç½®ä¸­è·å–æ—‹è½¬ä½ç½®ç¼–ç ç»´åº¦çš„æ–¹å¼ï¼Œå…¨éƒ¨æ”¹ä¸ºé€šè¿‡`rope_parameters[â€œpartial_rotary_factorâ€]`ã€‚è¿™è§£å†³äº†å› ç¤¾åŒºé‡åŒ–æ¨¡å‹é…ç½®ä¸ä¸€è‡´ï¼ˆå¦‚MiniMax-M2ï¼‰å¯¼è‡´çš„åŠ è½½å¤±è´¥é—®é¢˜ï¼Œæå‡äº†æ¨¡å‹çš„å…¼å®¹æ€§å’Œé²æ£’æ€§ã€‚

### ğŸ“ˆ å¼€å‘æ´»è·ƒåº¦è§‚å¯Ÿ
*   **é«˜æ•ˆåˆå¹¶**ï¼š24å°æ—¶å†…åˆå¹¶45ä¸ªPRï¼Œè¡¨æ˜ä»£ç å®¡æŸ¥å’Œåˆå¹¶æµç¨‹éå¸¸é«˜æ•ˆã€‚è®¸å¤šPRåœ¨åˆ›å»ºå½“å¤©å³è¢«åˆå¹¶ï¼ˆå¦‚ #30481, #30526ï¼‰ã€‚
*   **è´¡çŒ®è€…å¤šå…ƒåŒ–**ï¼šé™¤æ ¸å¿ƒç»´æŠ¤è€…ï¼ˆå¦‚ `DarkLight1337`, `yewentao256`, `hmellor`ï¼‰å¤–ï¼Œå¤§é‡æ¥è‡ªAMD (`rasmith`, `AndreasKaratzas`)ã€ NVIDIA (`LopezCastroRoberto`)ã€ ä»¥åŠä¼ä¸šç”¨æˆ·ï¼ˆå¦‚ `nrghosh`ï¼‰çš„è´¡çŒ®éå¸¸æ´»è·ƒã€‚ç”¨æˆ· `fadara01` åœ¨æŠ¥å‘ŠArm CPUæ„å»ºIssueåè¿…é€Ÿè‡ªè¡Œæäº¤äº†ä¿®å¤PRï¼Œå±•ç°äº†ç§¯æçš„ç¤¾åŒºå‚ä¸ã€‚
*   **é—®é¢˜è§£å†³æ¨¡å¼**ï¼šå¸¸è§æ¨¡å¼æ˜¯ç”¨æˆ·æäº¤è¯¦ç»†çš„BugæŠ¥å‘Šåï¼Œå¾ˆå¿«æœ‰ç»´æŠ¤è€…æˆ–ç¤¾åŒºæˆå‘˜è¿›è¡Œæ ¹å› åˆ†æï¼Œç”šè‡³ç›´æ¥å…³è”åˆ°å·²çŸ¥çš„Issueæˆ–æäº¤ä¿®å¤ä»£ç ã€‚è¿™ä½“ç°äº†ç¤¾åŒºæ‹¥æœ‰å¼ºå¤§çš„é›†ä½“æŠ€æœ¯èƒ½åŠ›å’ŒçŸ¥è¯†æ²‰æ·€ã€‚

### ğŸ’¡ å€¼å¾—å…³æ³¨çš„é—®é¢˜
1.  **DeepSeek-V3.2é‡å¤è§£ç  (Issue #30453)**ï¼šä½œä¸ºæœ€æ–°å‘å¸ƒçš„çƒ­é—¨æ¨¡å‹ï¼Œå…¶éƒ¨ç½²ç¨³å®šæ€§è‡³å…³é‡è¦ã€‚æ­¤é—®é¢˜å½±å“å¹¿æ³›ï¼Œéœ€è¦ä¼˜å…ˆè°ƒæŸ¥æ˜¯å¦ä¸vLLMçš„è°ƒåº¦æˆ–ç¼“å­˜é€»è¾‘å­˜åœ¨ç‰¹å®šäº¤äº’é—®é¢˜ã€‚
2.  **GPT-OSS-120Bç¼“å­˜ä¸CUDAå›¾Bug (Issue #30498)**ï¼šè¢«å…³è”åˆ°ä¸€ä¸ªå·²çŸ¥çš„æ·±å±‚Bugï¼ˆ#26480ï¼‰ã€‚è¿™ä¸ªé—®é¢˜ä¼šå½±å“æ‰€æœ‰å‘½ä¸­å‰ç¼€ç¼“å­˜çš„è¯·æ±‚ï¼Œå¯¼è‡´è¾“å‡ºè´¨é‡ä¸¥é‡ä¸‹é™ï¼Œæ˜¯éœ€è¦å½»åº•è§£å†³çš„æ ¸å¿ƒç¨³å®šæ€§é—®é¢˜ã€‚
3.  **NIXLè§£è€¦æœåŠ¡åœ¨æµæ°´çº¿å¹¶è¡Œä¸‹çš„æ¡æ‰‹é”™è¯¯ (Issue #30501)**ï¼šç”¨æˆ·å°è¯•åœ¨å¤æ‚é…ç½®ï¼ˆTP=8, PP=2ï¼‰ä¸‹è¿è¡ŒDeepSeek-R1çš„åˆ†è§£å¼æœåŠ¡é­é‡å¤±è´¥ã€‚è¿™æ­ç¤ºäº†é«˜çº§åˆ†å¸ƒå¼ç‰¹æ€§åœ¨è¾¹ç•Œç”¨ä¾‹ä¸‹çš„å…¼å®¹æ€§é—®é¢˜ï¼Œå¯¹äºæ¨åŠ¨å¤§è§„æ¨¡æ¨¡å‹æœåŠ¡æŠ€æœ¯æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚
4.  **Flashinferåç«¯å†…å­˜åˆ†æ (Issue #30521)**ï¼šè¯¥è®¨è®ºæ­ç¤ºäº†å½“å‰å†…å­˜åˆ†ææµç¨‹çš„ä¸€ä¸ªæ½œåœ¨ç¼ºé™·â€”â€”æœªèƒ½å……åˆ†è€ƒè™‘ä¸åŒåç«¯ï¼ˆFlashinfer vs FlexAttentionï¼‰åœ¨CUDAå›¾å’Œå·¥ä½œåŒºå†…å­˜ä¸Šçš„å·®å¼‚ã€‚è¿™å¯èƒ½éœ€è¦ä¸€ä¸ªæ›´é€šç”¨ã€æ›´ç²¾å‡†çš„å†…å­˜é¢„æµ‹æ¡†æ¶ã€‚

---

## ğŸ“‹ é™„å½•ï¼šè¯¦ç»†æ•°æ®åˆ—è¡¨

### æ–°å¢ Issue
- [#30453](https://github.com/vllm-project/vllm/issues/30453) [Bug]: Frequent Repetitive Decoding Problems in DeepSeek-V3.2 â€” bug â€” by makabaka6338 (åˆ›å»ºäº: 2025-12-11 11:19 (UTC+8))
- [#30470](https://github.com/vllm-project/vllm/issues/30470) [Bug] [CPU Backend]: vLLM build on Arm CPU fails with pytorch nightly â€” bug,cpu â€” by fadara01 (åˆ›å»ºäº: 2025-12-11 15:51 (UTC+8))
- [#30530](https://github.com/vllm-project/vllm/issues/30530) [Installation]: åœ¨h200éƒ¨ç½²deepseekv3.2ç‰ˆæœ¬åº“çš„é—®é¢˜ â€” installation â€” by h1248759474 (åˆ›å»ºäº: 2025-12-12 10:07 (UTC+8))
- [#30477](https://github.com/vllm-project/vllm/issues/30477) [Usage]: How to disable thinking for Qwen-8B â€” usage â€” by fancyerii (åˆ›å»ºäº: 2025-12-11 17:28 (UTC+8))
- [#30501](https://github.com/vllm-project/vllm/issues/30501) [Bug]/[Feature]:NIXL Disaggregated Serving Fails with Pipeline Parallelism (PP > 1) â€” bug â€” by nskpro-cmd (åˆ›å»ºäº: 2025-12-12 01:16 (UTC+8))
- [#30464](https://github.com/vllm-project/vllm/issues/30464) [Usage]: How can I use the local pre-compiled wheel of vllm â€” usage â€” by gcanlin (åˆ›å»ºäº: 2025-12-11 14:22 (UTC+8))
- [#30498](https://github.com/vllm-project/vllm/issues/30498) [Bug]: GPT-OSS-120B returns null content when hitting max_tokens without --enforce-eager â€” bug â€” by ashtarkb (åˆ›å»ºäº: 2025-12-12 00:40 (UTC+8))
- [#30521](https://github.com/vllm-project/vllm/issues/30521) [Bug]: vLLM 0.12.0 / Flashinfer Backend memory profiling regression (DP > 1) â€” bug â€” by nrghosh (åˆ›å»ºäº: 2025-12-12 06:05 (UTC+8))
- [#30511](https://github.com/vllm-project/vllm/issues/30511) Potential Deadlock? â€” æ— æ ‡ç­¾ â€” by ChuanLi1101 (åˆ›å»ºäº: 2025-12-12 03:57 (UTC+8))
- [#30493](https://github.com/vllm-project/vllm/issues/30493) [Bug]: 5090 RTX seems to be broken â€” bug â€” by mobicham (åˆ›å»ºäº: 2025-12-11 23:56 (UTC+8))
- [#30461](https://github.com/vllm-project/vllm/issues/30461) [Bug]: Lora support don't work on 0.12.0 with Qwen3-30B-A30B-Instuct-2507 â€” bug â€” by bash99 (åˆ›å»ºäº: 2025-12-11 13:49 (UTC+8))
- [#30487](https://github.com/vllm-project/vllm/issues/30487) [Bug] [CPU Backend]: Wrong L2 cache size on Arm CPU for Attention tiles â€” bug,cpu â€” by Radu2k (åˆ›å»ºäº: 2025-12-11 20:58 (UTC+8))
- [#30485](https://github.com/vllm-project/vllm/issues/30485) [Feature]: Support for GGUF qwen3vl models â€” feature request â€” by arno4000 (åˆ›å»ºäº: 2025-12-11 20:17 (UTC+8))
- [#30466](https://github.com/vllm-project/vllm/issues/30466) [Feature]: Support transformers>=5 â€” feature request â€” by Whadup (åˆ›å»ºäº: 2025-12-11 15:05 (UTC+8))
- [#30465](https://github.com/vllm-project/vllm/issues/30465) [Bug]: reranker benchmarking calculate input_token=0 â€” bug â€” by hz0ne (åˆ›å»ºäº: 2025-12-11 14:38 (UTC+8))

### å·²å…³é—­ Issue
- [#18467](https://github.com/vllm-project/vllm/issues/18467) [Usage]: Can vllm distributed load model? â€” usage,stale â€” by ASTLY123 (å…³é—­äº: 2025-12-12 10:16 (UTC+8))
- [#21689](https://github.com/vllm-project/vllm/issues/21689) [Bug]: VLLM 0.10.0 breaks quantized models batch inference speed for Qwen2.5-VL-7B (tested multiple quantization types) â€” bug,stale â€” by Magmanat (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#21799](https://github.com/vllm-project/vllm/issues/21799) [Bug]: Performance Regression in Eagle3: vLLM 0.9.0 vs. vLLM 0.9. â€” bug,performance,stale â€” by ggg-s (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22004](https://github.com/vllm-project/vllm/issues/22004) [Bug]: AWQ fails on MoE models â€” bug,stale â€” by someone132s (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22334](https://github.com/vllm-project/vllm/issues/22334) [Feature]: add --reasoning_parser flag for gpt-oss â€” feature request,stale â€” by lorentzbao (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22568](https://github.com/vllm-project/vllm/issues/22568) [Usage]: VRAM spike while loading gemma3-12b bnb on vllm-0.10 â€” usage,stale â€” by ivoras (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22718](https://github.com/vllm-project/vllm/issues/22718) [Bug]: kimi_k2_tool_parser.py  has bug with `tool_call_regex` â€” bug,stale â€” by Jarlene (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22719](https://github.com/vllm-project/vllm/issues/22719) [Bug]: Qwen2.5vl-3b OOM but 7b works fine â€” bug,stale â€” by Elenore1997 (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22766](https://github.com/vllm-project/vllm/issues/22766) [Usage]: Prevent prefill from being split across different batches â€” usage,stale â€” by randombk (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22784](https://github.com/vllm-project/vllm/issues/22784) [Bug]: AttributeError when loading Step3 model â€” bug,stale â€” by Joy666-Li (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22816](https://github.com/vllm-project/vllm/issues/22816) [Bug]: vllm serve does not bind to all available ipv4 and ipv6 addresses when --host is empty â€” bug,stale â€” by smarterclayton (å…³é—­äº: 2025-12-12 10:14 (UTC+8))
- [#30470](https://github.com/vllm-project/vllm/issues/30470) [Bug] [CPU Backend]: vLLM build on Arm CPU fails with pytorch nightly â€” bug,cpu â€” by fadara01 (å…³é—­äº: 2025-12-12 10:09 (UTC+8))
- [#14964](https://github.com/vllm-project/vllm/issues/14964) [Feature] [ROCm]: AITER Kernel Integration â€” feature request,rocm,stale â€” by tjtanaa (å…³é—­äº: 2025-12-12 09:37 (UTC+8))
- [#22698](https://github.com/vllm-project/vllm/issues/22698) [Performance]: Big bubble at the end of cudagraph run in MI300/MI308 â€” performance,rocm,unstale â€” by hoangvictor (å…³é—­äº: 2025-12-12 09:36 (UTC+8))
- [#30477](https://github.com/vllm-project/vllm/issues/30477) [Usage]: How to disable thinking for Qwen-8B â€” usage â€” by fancyerii (å…³é—­äº: 2025-12-12 09:17 (UTC+8))
- [#30445](https://github.com/vllm-project/vllm/issues/30445) [Bug]: QuantTrio/MiniMax-M2-AWQ produces garbage in 12/10/2025 build â€” bug â€” by eugr (å…³é—­äº: 2025-12-12 04:45 (UTC+8))
- [#30139](https://github.com/vllm-project/vllm/issues/30139) [Bug]: Reasoning models does not reason and put completion in reasoning_content â€” bug â€” by Rictus (å…³é—­äº: 2025-12-12 00:53 (UTC+8))
- [#29259](https://github.com/vllm-project/vllm/issues/29259) [Bug]: `simple_profiling.py` fails on CPU target â€” bug â€” by NobuoTsukamoto (å…³é—­äº: 2025-12-12 00:49 (UTC+8))
- [#30269](https://github.com/vllm-project/vllm/issues/30269) [Bug]: Multi-node deployment fails with TP=1 and PP=2 â€” bug â€” by doss22 (å…³é—­äº: 2025-12-11 18:41 (UTC+8))
- [#29861](https://github.com/vllm-project/vllm/issues/29861) [Feature]: [CPU Backend] Enable support for Whisper â€” feature request,cpu â€” by aditew01 (å…³é—­äº: 2025-12-11 18:09 (UTC+8))
- [#28852](https://github.com/vllm-project/vllm/issues/28852) [Bug]: When infer MiniMax-m2, during streaming returns, the think are contained in the 'content' field and cannot be separated. â€” bug â€” by gallery2016 (å…³é—­äº: 2025-12-11 17:05 (UTC+8))
- [#28184](https://github.com/vllm-project/vllm/issues/28184) [Bug]: encoder_decoder models (e.g. Whisper) is not working in vLLM 0.11 with ROCm â€” bug,rocm â€” by lucaschr21 (å…³é—­äº: 2025-12-11 14:01 (UTC+8))
- [#29587](https://github.com/vllm-project/vllm/issues/29587) [Installation]: PaddleOCR-VL integration with vLLM â€” installation â€” by ssuncheol (å…³é—­äº: 2025-12-11 10:55 (UTC+8))

### æ–°å¢ PR
- [#30532](https://github.com/vllm-project/vllm/pull/30532) [responsesAPI]add extra body parameters â€” frontend â€” by Ri0S (åˆ›å»ºäº: 2025-12-12 10:27 (UTC+8))
- [#30484](https://github.com/vllm-project/vllm/pull/30484) [Feature] Add SM103 (Blackwell Ultra) Support to vLLM â€” ready,needs-rebase,v1,nvidia â€” by LopezCastroRoberto (åˆ›å»ºäº: 2025-12-11 20:09 (UTC+8))
- [#30531](https://github.com/vllm-project/vllm/pull/30531) [CPU] Refactor CPU fused MOE â€” ci/build â€” by bigPYJ1151 (åˆ›å»ºäº: 2025-12-12 10:08 (UTC+8))
- [#30471](https://github.com/vllm-project/vllm/pull/30471) [Optimization]: Add fused router for GPTOSS â€” gpt-oss â€” by ijpq (åˆ›å»ºäº: 2025-12-11 15:55 (UTC+8))
- [#30524](https://github.com/vllm-project/vllm/pull/30524) [Optimization] Pad the number of tokens to a multiple of 4 to improve FP8 performance â€” v1 â€” by 0xjunhao (åˆ›å»ºäº: 2025-12-12 07:20 (UTC+8))
- [#30481](https://github.com/vllm-project/vllm/pull/30481) [CPU][FIX] Fix build failures on Arm CPUs with torch nightly â€” ready,ci/build â€” by fadara01 (åˆ›å»ºäº: 2025-12-11 17:57 (UTC+8))
- [#30529](https://github.com/vllm-project/vllm/pull/30529) [Benchmarks] `auto_tune.sh`: Use hostname variable for server requests â€” performance â€” by KevinMusgrave (åˆ›å»ºäº: 2025-12-12 10:01 (UTC+8))
- [#30515](https://github.com/vllm-project/vllm/pull/30515) [UX][Startup] Account for CUDA graphs during memory profiling â€” v1,nvidia â€” by MatthewBonanni (åˆ›å»ºäº: 2025-12-12 05:12 (UTC+8))
- [#30490](https://github.com/vllm-project/vllm/pull/30490) [DeepSeek V3.2] Proper drop_thinking logic â€” ready,deepseek â€” by vladnosiv (åˆ›å»ºäº: 2025-12-11 23:02 (UTC+8))
- [#30527](https://github.com/vllm-project/vllm/pull/30527) [ROCm][CI] Skip multi-GPU speculative decoding tests when insufficient GPUs available â€” rocm,ready,v1 â€” by AndreasKaratzas (åˆ›å»ºäº: 2025-12-12 08:59 (UTC+8))
- [#30528](https://github.com/vllm-project/vllm/pull/30528) [Perf] Set split_k to 1 for triton_kernels â€” æ— æ ‡ç­¾ â€” by xyang16 (åˆ›å»ºäº: 2025-12-12 09:15 (UTC+8))
- [#30526](https://github.com/vllm-project/vllm/pull/30526) [ROCm][CI] Use mi325_4 agent pool for V1 e2e tests â€” rocm,ready,ci/build â€” by AndreasKaratzas (åˆ›å»ºäº: 2025-12-12 08:16 (UTC+8))
- [#30520](https://github.com/vllm-project/vllm/pull/30520) [BugFix] Use late binding to avoid zmq port conflict race conditions â€” v1 â€” by njhill (åˆ›å»ºäº: 2025-12-12 05:57 (UTC+8))
- [#30452](https://github.com/vllm-project/vllm/pull/30452) [Core] Optimize encoder cache with mask-based storage â€” v1,multi-modality â€” by sunYtokki (åˆ›å»ºäº: 2025-12-11 11:09 (UTC+8))
- [#30516](https://github.com/vllm-project/vllm/pull/30516) [compile] Parse compile range cache keys as Range during cache loading. â€” ready â€” by zhxchen17 (åˆ›å»ºäº: 2025-12-12 05:30 (UTC+8))
- [#30508](https://github.com/vllm-project/vllm/pull/30508) [CI/Build][AMD] Skip test_cutlass_w4a8_moe tests on ROCm sine they require cutlass_pack_scale_fp8 â€” rocm,ready,nvidia â€” by rasmith (åˆ›å»ºäº: 2025-12-12 02:45 (UTC+8))
- [#30525](https://github.com/vllm-project/vllm/pull/30525) [Release 2.10] Test Torch 2.10 RC â€” rocm,ci/build,nvidia â€” by atalman (åˆ›å»ºäº: 2025-12-12 07:52 (UTC+8))
- [#30513](https://github.com/vllm-project/vllm/pull/30513) [WIP][CI] Speed up fusion tests â€” ready,needs-rebase,ci/build â€” by LucasWilkinson (åˆ›å»ºäº: 2025-12-12 04:19 (UTC+8))
- [#30499](https://github.com/vllm-project/vllm/pull/30499) [CI] Breakup h200 tests â€” ready,needs-rebase,ci/build â€” by LucasWilkinson (åˆ›å»ºäº: 2025-12-12 00:45 (UTC+8))
- [#30519](https://github.com/vllm-project/vllm/pull/30519) [Kernels][MoE] Add FusedMoERouter object â€” æ— æ ‡ç­¾ â€” by bnellnm (åˆ›å»ºäº: 2025-12-12 05:54 (UTC+8))
- [#30512](https://github.com/vllm-project/vllm/pull/30512) Improve parse_raw_prompt test cases for invalid input .v2 â€” æ— æ ‡ç­¾ â€” by mivehk (åˆ›å»ºäº: 2025-12-12 04:05 (UTC+8))
- [#30523](https://github.com/vllm-project/vllm/pull/30523) Fix Kimi K2 thinking model nvfp4 vocab size â€” v1 â€” by kjiang249 (åˆ›å»ºäº: 2025-12-12 06:43 (UTC+8))
- [#30510](https://github.com/vllm-project/vllm/pull/30510) [Refactor] Remove useless syncwarp â€” ready â€” by yewentao256 (åˆ›å»ºäº: 2025-12-12 03:35 (UTC+8))
- [#30494](https://github.com/vllm-project/vllm/pull/30494) [Perf] Optimize deepgemm experts initialization, 3.9% TTFT improvement â€” performance,ready,deepseek â€” by yewentao256 (åˆ›å»ºäº: 2025-12-12 00:15 (UTC+8))
- [#30496](https://github.com/vllm-project/vllm/pull/30496) [Refactor] Reduce duplicate code in `per_token_group_quant` cuda kernels â€” ready,nvidia â€” by yewentao256 (åˆ›å»ºäº: 2025-12-12 00:19 (UTC+8))
- [#30522](https://github.com/vllm-project/vllm/pull/30522) [KV Connector][Metrics] Do not count prefix cache hits in connector queries â€” ready,v1 â€” by markmc (åˆ›å»ºäº: 2025-12-12 06:11 (UTC+8))
- [#30509](https://github.com/vllm-project/vllm/pull/30509) [Doc] Add documents for multi-node distributed serving with MP backend â€” documentation,v1 â€” by Isotr0py (åˆ›å»ºäº: 2025-12-12 03:18 (UTC+8))
- [#30491](https://github.com/vllm-project/vllm/pull/30491) [Docs][CPU backend] Add pre-built Arm CPU Docker images â€” documentation,ready â€” by ioghiban (åˆ›å»ºäº: 2025-12-11 23:40 (UTC+8))
- [#30514](https://github.com/vllm-project/vllm/pull/30514) [CI] Update several models in registry that are available online now â€” ready,ci/build â€” by mgoin (åˆ›å»ºäº: 2025-12-12 04:53 (UTC+8))
- [#30518](https://github.com/vllm-project/vllm/pull/30518) Don't compile vision encoder for Transformers backend â€” æ— æ ‡ç­¾ â€” by hmellor (åˆ›å»ºäº: 2025-12-12 05:52 (UTC+8))
- [#30517](https://github.com/vllm-project/vllm/pull/30517) [CI] Fix mypy for vllm/v1/executor â€” v1 â€” by yewentao256 (åˆ›å»ºäº: 2025-12-12 05:33 (UTC+8))
- [#30505](https://github.com/vllm-project/vllm/pull/30505) [Bugfix][Model] Fix Afmoe rope_parameters issue â€” bug,ready â€” by mgoin (åˆ›å»ºäº: 2025-12-12 02:36 (UTC+8))
- [#30472](https://github.com/vllm-project/vllm/pull/30472) [BugFix][MM]support VLLM_RANDOMIZE_DP_DUMMY_INPUTS â€” ready,v1 â€” by charlotte12l (åˆ›å»ºäº: 2025-12-11 16:02 (UTC+8))
- [#30506](https://github.com/vllm-project/vllm/pull/30506) simplify the return value from generate_beam_search â€” æ— æ ‡ç­¾ â€” by nwaughachukwuma (åˆ›å»ºäº: 2025-12-12 02:37 (UTC+8))
- [#30503](https://github.com/vllm-project/vllm/pull/30503) [compile] Stop one-off setting enable_aot_compile and use context manager instead. â€” ready â€” by zhxchen17 (åˆ›å»ºäº: 2025-12-12 01:48 (UTC+8))
- [#30474](https://github.com/vllm-project/vllm/pull/30474) [Misc] Add mcp to requirements â€” ready,ci/build â€” by yeqcharlotte (åˆ›å»ºäº: 2025-12-11 17:03 (UTC+8))
- [#30507](https://github.com/vllm-project/vllm/pull/30507) [Bugfix] Dictionary MM embeddings for online chat â€” frontend,v1 â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-12 02:41 (UTC+8))
- [#30495](https://github.com/vllm-project/vllm/pull/30495) [Async][Feat] support apply penalty or bad_words for async + spec â€” v1 â€” by izhuhaoran (åˆ›å»ºäº: 2025-12-12 00:18 (UTC+8))
- [#30500](https://github.com/vllm-project/vllm/pull/30500) feat(gguf): Extract HF config from GGUF metadata for repos without config.json â€” æ— æ ‡ç­¾ â€” by kitaekatt (åˆ›å»ºäº: 2025-12-12 00:51 (UTC+8))
- [#30504](https://github.com/vllm-project/vllm/pull/30504) [CI] Whisper logprobs tests â€” ready,multi-modality â€” by NickLucche (åˆ›å»ºäº: 2025-12-12 02:19 (UTC+8))
- [#30462](https://github.com/vllm-project/vllm/pull/30462) enable unbacked with aot_compile â€” ready â€” by laithsakka (åˆ›å»ºäº: 2025-12-11 13:50 (UTC+8))
- [#30502](https://github.com/vllm-project/vllm/pull/30502) [Kernel] add H100 triton fused moe config for FP8 Qwen3MoE â€” qwen â€” by cjackal (åˆ›å»ºäº: 2025-12-12 01:18 (UTC+8))
- [#30497](https://github.com/vllm-project/vllm/pull/30497) fix(gguf): GGUF model support fixes for Blackwell GPUs â€” structured-output,v1 â€” by kitaekatt (åˆ›å»ºäº: 2025-12-12 00:37 (UTC+8))
- [#30488](https://github.com/vllm-project/vllm/pull/30488) Give pooling examples better names â€” documentation,ready â€” by hmellor (åˆ›å»ºäº: 2025-12-11 21:21 (UTC+8))
- [#30492](https://github.com/vllm-project/vllm/pull/30492) [WIP] add manual numa binding â€” v1 â€” by jasonlizhengjian (åˆ›å»ºäº: 2025-12-11 23:44 (UTC+8))
- [#30459](https://github.com/vllm-project/vllm/pull/30459) set assume_32bit_indexing and pass unbacked hints â€” ready â€” by laithsakka (åˆ›å»ºäº: 2025-12-11 12:58 (UTC+8))
- [#30480](https://github.com/vllm-project/vllm/pull/30480) Make the `httpx` logger less annoying when Transformers v5 is installed â€” ready â€” by hmellor (åˆ›å»ºäº: 2025-12-11 17:51 (UTC+8))
- [#30483](https://github.com/vllm-project/vllm/pull/30483) [Misc] Improve error message for `is_multimodal` â€” ready,qwen â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-11 19:01 (UTC+8))
- [#30489](https://github.com/vllm-project/vllm/pull/30489) Add encoder tag for compilation â€” qwen â€” by ilmarkov (åˆ›å»ºäº: 2025-12-11 21:23 (UTC+8))
- [#30473](https://github.com/vllm-project/vllm/pull/30473) Fix typo of endpoint name in CLI args docs â€” frontend,ready â€” by kmaehashi (åˆ›å»ºäº: 2025-12-11 16:33 (UTC+8))
- [#30486](https://github.com/vllm-project/vllm/pull/30486) [BugFix] Fix minimax m2 model partial_rotary_factor â€” æ— æ ‡ç­¾ â€” by rogeryoungh (åˆ›å»ºäº: 2025-12-11 20:25 (UTC+8))
- [#30482](https://github.com/vllm-project/vllm/pull/30482) [Frontend] Honor chat template for gpt-oss harmony (#23015) â€” frontend,gpt-oss â€” by ajayanto (åˆ›å»ºäº: 2025-12-11 18:23 (UTC+8))
- [#30458](https://github.com/vllm-project/vllm/pull/30458) [Deprecation] Remove fallbacks for `embed_input_ids` and `embed_multimodal` â€” speculative-decoding,ready,qwen â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-11 12:33 (UTC+8))
- [#30469](https://github.com/vllm-project/vllm/pull/30469) [Deprecation] Remove missed fallback for `embed_input_ids` â€” ready â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-11 15:34 (UTC+8))
- [#30476](https://github.com/vllm-project/vllm/pull/30476) [Bugfix] Fix `task` still being passed in tests/benchmarks â€” performance,ready,v1 â€” by DarkLight1337 (åˆ›å»ºäº: 2025-12-11 17:16 (UTC+8))
- [#30478](https://github.com/vllm-project/vllm/pull/30478) fix gme model do not use mrope â€” qwen â€” by zhuikefeng986285005-byte (åˆ›å»ºäº: 2025-12-11 17:40 (UTC+8))
- [#30463](https://github.com/vllm-project/vllm/pull/30463) [Deprecation] Deprecation `--convert reward`, use `--convert embed` instead. â€” documentation,ready â€” by noooop (åˆ›å»ºäº: 2025-12-11 14:01 (UTC+8))
- [#30475](https://github.com/vllm-project/vllm/pull/30475) [Core][MM] Optimize encoder cache manager by operating with embeddings only â€” v1,multi-modality,llama,qwen â€” by ywang96 (åˆ›å»ºäº: 2025-12-11 17:06 (UTC+8))
- [#30479](https://github.com/vllm-project/vllm/pull/30479) [BugFix] Fix unmap_and_release by tag not done correctly â€” æ— æ ‡ç­¾ â€” by Crispig (åˆ›å»ºäº: 2025-12-11 17:44 (UTC+8))
- [#30468](https://github.com/vllm-project/vllm/pull/30468) [Feat] EPD Mooncake store â€” documentation,v1,kv-connector â€” by khuonglmhw (åˆ›å»ºäº: 2025-12-11 15:33 (UTC+8))
- [#30467](https://github.com/vllm-project/vllm/pull/30467) [Optimization]: Add fused router for GPTOSS â€” gpt-oss â€” by ijpq (åˆ›å»ºäº: 2025-12-11 15:30 (UTC+8))
- [#30457](https://github.com/vllm-project/vllm/pull/30457) [Feat] Mooncake storage connector for E-PD disaggregate â€” documentation,v1,kv-connector â€” by khuonglmhw (åˆ›å»ºäº: 2025-12-11 12:28 (UTC+8))
- [#30456](https://github.com/vllm-project/vllm/pull/30456) feat: support video list inference â€” frontend,multi-modality â€” by LiuLi1998 (åˆ›å»ºäº: 2025-12-11 11:47 (UTC+8))
- [#30454](https://github.com/vllm-project/vllm/pull/30454) [Optimization]: Add fused router for GPTOSS â€” gpt-oss â€” by ijpq (åˆ›å»ºäº: 2025-12-11 11:31 (UTC+8))
- [#30460](https://github.com/vllm-project/vllm/pull/30460) [chore] Update FA commit â€” ready,ci/build â€” by LucasWilkinson (åˆ›å»ºäº: 2025-12-11 13:29 (UTC+8))
- [#30455](https://github.com/vllm-project/vllm/pull/30455) [Doc] Add Baidu Kunlun XPU support â€” documentation,ready â€” by xyDong0223 (åˆ›å»ºäº: 2025-12-11 11:38 (UTC+8))
- [#30451](https://github.com/vllm-project/vllm/pull/30451) [Core] Optimize encoder cache with mask-based storage â€” v1,multi-modality â€” by sunYtokki (åˆ›å»ºäº: 2025-12-11 11:03 (UTC+8))
- [#30450](https://github.com/vllm-project/vllm/pull/30450) [Core] Optimize encoder cache with mask-based storage â€” v1,multi-modality â€” by sunYtokki (åˆ›å»ºäº: 2025-12-11 10:50 (UTC+8))

### å·²åˆå¹¶ PR
- [#29421](https://github.com/vllm-project/vllm/pull/29421) [Core] Whisper Enable Encoder Batching â€” ready,v1 â€” by NickLucche (åˆå¹¶äº: 2025-12-12 05:06 (UTC+8))
- [#30254](https://github.com/vllm-project/vllm/pull/30254) gptq marlin quantization support for fused moe with lora â€” ready â€” by Bhanu068 (åˆå¹¶äº: 2025-12-12 10:27 (UTC+8))
- [#30481](https://github.com/vllm-project/vllm/pull/30481) [CPU][FIX] Fix build failures on Arm CPUs with torch nightly â€” ready,ci/build â€” by fadara01 (åˆå¹¶äº: 2025-12-12 10:09 (UTC+8))
- [#29628](https://github.com/vllm-project/vllm/pull/29628) [Core] Refactor `_build_attention_metadata` â€” ready,v1,ready-run-all-tests â€” by LucasWilkinson (åˆå¹¶äº: 2025-12-12 09:54 (UTC+8))
- [#30526](https://github.com/vllm-project/vllm/pull/30526) [ROCm][CI] Use mi325_4 agent pool for V1 e2e tests â€” rocm,ready,ci/build â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-12 09:37 (UTC+8))
- [#30508](https://github.com/vllm-project/vllm/pull/30508) [CI/Build][AMD] Skip test_cutlass_w4a8_moe tests on ROCm sine they require cutlass_pack_scale_fp8 â€” rocm,ready,nvidia â€” by rasmith (åˆå¹¶äº: 2025-12-12 09:02 (UTC+8))
- [#30314](https://github.com/vllm-project/vllm/pull/30314) [fix] fix SM check for Flashinfer TRTLLM MOE â€” ready,nvidia â€” by jiahanc (åˆå¹¶äº: 2025-12-12 09:00 (UTC+8))
- [#30417](https://github.com/vllm-project/vllm/pull/30417) [CI/Build][AMD] Skip tests in test_fusions_e2e and test_dbo_dp_ep_gsm8k that require non-existing imports for ROCm  â€” rocm,ready,v1 â€” by rasmith (åˆå¹¶äº: 2025-12-12 08:24 (UTC+8))
- [#30002](https://github.com/vllm-project/vllm/pull/30002) [FIX]Patch run-cluster.sh (fix for #28328) â€” documentation,ready â€” by evberrypi (åˆå¹¶äº: 2025-12-12 07:36 (UTC+8))
- [#30276](https://github.com/vllm-project/vllm/pull/30276) [ROCM][CI] Fix AMD Examples Test Group â€” documentation,rocm,ready,ci/build â€” by Concurrensee (åˆå¹¶äº: 2025-12-12 07:03 (UTC+8))
- [#29804](https://github.com/vllm-project/vllm/pull/29804) [EPLB] Support EPLB w/ NVFP4 â€” ready,nvidia â€” by andrewbriand (åˆå¹¶äº: 2025-12-12 06:59 (UTC+8))
- [#30510](https://github.com/vllm-project/vllm/pull/30510) [Refactor] Remove useless syncwarp â€” ready â€” by yewentao256 (åˆå¹¶äº: 2025-12-12 06:43 (UTC+8))
- [#30494](https://github.com/vllm-project/vllm/pull/30494) [Perf] Optimize deepgemm experts initialization, 3.9% TTFT improvement â€” performance,ready,deepseek â€” by yewentao256 (åˆå¹¶äº: 2025-12-12 06:28 (UTC+8))
- [#30491](https://github.com/vllm-project/vllm/pull/30491) [Docs][CPU backend] Add pre-built Arm CPU Docker images â€” documentation,ready â€” by ioghiban (åˆå¹¶äº: 2025-12-12 06:03 (UTC+8))
- [#30472](https://github.com/vllm-project/vllm/pull/30472) [BugFix][MM]support VLLM_RANDOMIZE_DP_DUMMY_INPUTS â€” ready,v1 â€” by charlotte12l (åˆå¹¶äº: 2025-12-12 05:00 (UTC+8))
- [#30389](https://github.com/vllm-project/vllm/pull/30389) Standardise `get_rope` to use `rope_parameters["partial_rotary_factor"]`, not `rotary_dim` â€” performance,ready,llama,qwen,deepseek,gpt-oss â€” by hmellor (åˆå¹¶äº: 2025-12-12 04:45 (UTC+8))
- [#30503](https://github.com/vllm-project/vllm/pull/30503) [compile] Stop one-off setting enable_aot_compile and use context manager instead. â€” ready â€” by zhxchen17 (åˆå¹¶äº: 2025-12-12 04:28 (UTC+8))
- [#30474](https://github.com/vllm-project/vllm/pull/30474) [Misc] Add mcp to requirements â€” ready,ci/build â€” by yeqcharlotte (åˆå¹¶äº: 2025-12-12 04:06 (UTC+8))
- [#30430](https://github.com/vllm-project/vllm/pull/30430) [ROCm][Bugfix] Add MLACommonMetadata to allowed attention types for speculative decoding â€” rocm,speculative-decoding,ready,v1 â€” by AndreasKaratzas (åˆå¹¶äº: 2025-12-12 03:25 (UTC+8))
- [#30442](https://github.com/vllm-project/vllm/pull/30442) [Feature] AWQ marlin quantization support for fused moe with lora â€” ready â€” by princepride (åˆå¹¶äº: 2025-12-12 02:03 (UTC+8))
- [#30340](https://github.com/vllm-project/vllm/pull/30340) Add Eagle and Eagle3 support to Transformers modeling backend â€” ready,v1 â€” by hmellor (åˆå¹¶äº: 2025-12-12 01:02 (UTC+8))
- [#30391](https://github.com/vllm-project/vllm/pull/30391) [IMPROVEMENT] Change MistralReasoningParser behavior â€” ready â€” by juliendenize (åˆå¹¶äº: 2025-12-12 00:53 (UTC+8))
- [#30341](https://github.com/vllm-project/vllm/pull/30341) [CI] refine more logic when generating and using nightly wheels & indices, add cuda130 build for aarch64, specify correct manylinux version â€” ready,ci/build,nvidia â€” by Harry-Chen (åˆå¹¶äº: 2025-12-12 00:42 (UTC+8))
- [#30488](https://github.com/vllm-project/vllm/pull/30488) Give pooling examples better names â€” documentation,ready â€” by hmellor (åˆå¹¶äº: 2025-12-12 00:22 (UTC+8))
- [#30402](https://github.com/vllm-project/vllm/pull/30402) [Docs][CPU Backend] Add nightly and per revision pre-built Arm CPU wheels â€” documentation,ready â€” by ioghiban (åˆå¹¶äº: 2025-12-11 23:57 (UTC+8))
- [#30480](https://github.com/vllm-project/vllm/pull/30480) Make the `httpx` logger less annoying when Transformers v5 is installed â€” ready â€” by hmellor (åˆå¹¶äº: 2025-12-11 23:44 (UTC+8))
- [#28309](https://github.com/vllm-project/vllm/pull/28309) [KVConnector] Add KV events to KV Connectors â€” ready,ci/build,v1,kv-connector â€” by hickeyma (åˆå¹¶äº: 2025-12-11 22:30 (UTC+8))
- [#30337](https://github.com/vllm-project/vllm/pull/30337) fix: enhance human_readable_int function â€” ready â€” by andyxning (åˆå¹¶äº: 2025-12-11 15:30 (UTC+8))
- [#30483](https://github.com/vllm-project/vllm/pull/30483) [Misc] Improve error message for `is_multimodal` â€” ready,qwen â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 22:39 (UTC+8))
- [#30473](https://github.com/vllm-project/vllm/pull/30473) Fix typo of endpoint name in CLI args docs â€” frontend,ready â€” by kmaehashi (åˆå¹¶äº: 2025-12-11 19:07 (UTC+8))
- [#30050](https://github.com/vllm-project/vllm/pull/30050) [Misc][PCP&DCP] relocate PCP feature check â€” ready,v1 â€” by pisceskkk (åˆå¹¶äº: 2025-12-11 19:36 (UTC+8))
- [#30458](https://github.com/vllm-project/vllm/pull/30458) [Deprecation] Remove fallbacks for `embed_input_ids` and `embed_multimodal` â€” speculative-decoding,ready,qwen â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 14:58 (UTC+8))
- [#30469](https://github.com/vllm-project/vllm/pull/30469) [Deprecation] Remove missed fallback for `embed_input_ids` â€” ready â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 18:06 (UTC+8))
- [#30476](https://github.com/vllm-project/vllm/pull/30476) [Bugfix] Fix `task` still being passed in tests/benchmarks â€” performance,ready,v1 â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 18:33 (UTC+8))
- [#30463](https://github.com/vllm-project/vllm/pull/30463) [Deprecation] Deprecation `--convert reward`, use `--convert embed` instead. â€” documentation,ready â€” by noooop (åˆå¹¶äº: 2025-12-11 18:18 (UTC+8))
- [#30444](https://github.com/vllm-project/vllm/pull/30444) [Fix] Update lazing loading of video loader backend â€” ready,multi-modality â€” by jeremyteboul (åˆå¹¶äº: 2025-12-11 18:14 (UTC+8))
- [#30376](https://github.com/vllm-project/vllm/pull/30376) [Fix]fix import error from lmcache â€” ready,kv-connector â€” by wz1qqx (åˆå¹¶äº: 2025-12-11 17:23 (UTC+8))
- [#29882](https://github.com/vllm-project/vllm/pull/29882) [bugfix] fix MiniMaxM2ReasoningParser streaming output not separating reasoning_content. â€” frontend,ready â€” by JaviS-Rei (åˆå¹¶äº: 2025-12-11 17:05 (UTC+8))
- [#29710](https://github.com/vllm-project/vllm/pull/29710) [perf] Use direct copy (broadcast) instead of cat for k_nope/k_pe in MLA prefill â€” performance,ready,v1 â€” by minosfuture (åˆå¹¶äº: 2025-12-11 16:20 (UTC+8))
- [#30285](https://github.com/vllm-project/vllm/pull/30285) Ensure minimum frames for GLM 4.6V compatibility â€” ready â€” by gh-wf (åˆå¹¶äº: 2025-12-11 13:26 (UTC+8))
- [#30455](https://github.com/vllm-project/vllm/pull/30455) [Doc] Add Baidu Kunlun XPU support â€” documentation,ready â€” by xyDong0223 (åˆå¹¶äº: 2025-12-11 12:52 (UTC+8))
- [#30428](https://github.com/vllm-project/vllm/pull/30428) [Chore] Fix torch precision warning â€” ready,v1 â€” by yewentao256 (åˆå¹¶äº: 2025-12-11 12:05 (UTC+8))
- [#30396](https://github.com/vllm-project/vllm/pull/30396) [Deprecation] Remove deprecated plugin and compilation fields for v0.13 release â€” documentation,ready â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 11:59 (UTC+8))
- [#30397](https://github.com/vllm-project/vllm/pull/30397) [Deprecation] Remove deprecated task, seed and MM settings â€” documentation,performance,frontend,ready,qwen â€” by DarkLight1337 (åˆå¹¶äº: 2025-12-11 11:59 (UTC+8))
- [#29439](https://github.com/vllm-project/vllm/pull/29439) [Bugfix] Fix grouped_topk pytorch impl when num_experts can't be grouped properly â€” ready â€” by divakar-amd (åˆå¹¶äº: 2025-12-11 11:47 (UTC+8))

### å…³é—­ä½†æœªåˆå¹¶çš„ PR
- [#17982](https://github.com/vllm-project/vllm/pull/17982) [Misc]Added span attribute gen_ai.system to identify spans from vLLM â€” needs-rebase,stale â€” by LakshmiPriyaSujith (å…³é—­äº: 2025-12-12 10:16 (UTC+8))
- [#21869](https://github.com/vllm-project/vllm/pull/21869) [Bugfix] Fix PyNcclCommunicator device assertion for un-indexed CUDA devices â€” stale â€” by CarlosArguilar (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#21988](https://github.com/vllm-project/vllm/pull/21988) [Disagg][Perf] Add env var to allow gpu model work runs in non-default CUDA stream, improving disagg TTIT/TTFT â€” ready,needs-rebase,stale,v1 â€” by liuzijing2014 (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22094](https://github.com/vllm-project/vllm/pull/22094) Fix error message for max_input_length (bugfix of #22092) â€” frontend,stale â€” by RobertFischer (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#30471](https://github.com/vllm-project/vllm/pull/30471) [Optimization]: Add fused router for GPTOSS â€” gpt-oss â€” by ijpq (å…³é—­äº: 2025-12-12 09:43 (UTC+8))
- [#22681](https://github.com/vllm-project/vllm/pull/22681) [Kernel][AMD] Reduce AITER attention CPU overhead â€” rocm,stale,v1 â€” by mxz297 (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22729](https://github.com/vllm-project/vllm/pull/22729) [Bugfix] hermesè§£æå™¨è¾“å‡ºå­˜åœ¨è¢«è¿‡æ»¤é—®é¢˜ â€” frontend,stale,tool-calling â€” by astrophel0 (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#22775](https://github.com/vllm-project/vllm/pull/22775) [Perf] Add GLM-4.5V tuning configs â€” performance,stale â€” by houseroad (å…³é—­äº: 2025-12-12 10:15 (UTC+8))
- [#25640](https://github.com/vllm-project/vllm/pull/25640) [Misc] Fix internal invocation of _register_fake â€” æ— æ ‡ç­¾ â€” by ijpq (å…³é—­äº: 2025-12-12 09:44 (UTC+8))
- [#29237](https://github.com/vllm-project/vllm/pull/29237) [Optimization] Add Fused Triton Kernel for GPT-OSS Router â€” gpt-oss â€” by ijpq (å…³é—­äº: 2025-12-12 09:44 (UTC+8))
- [#27281](https://github.com/vllm-project/vllm/pull/27281) [WIP][torch.compile] Add Triton-distributed GEMM+AllReduce fusion compile pass â€” æ— æ ‡ç­¾ â€” by jasonlizhengjian (å…³é—­äº: 2025-12-12 05:22 (UTC+8))
- [#21879](https://github.com/vllm-project/vllm/pull/21879) [Kernel][Machete] Larger tile shape for large K when mem bound â€” æ— æ ‡ç­¾ â€” by czhu-cohere (å…³é—­äº: 2025-12-12 03:12 (UTC+8))
- [#30440](https://github.com/vllm-project/vllm/pull/30440) [fix] Fix qwen3_coder tool call per parameter streaming â€” frontend,ready,tool-calling,qwen â€” by koush (å…³é—­äº: 2025-12-12 02:11 (UTC+8))
- [#30369](https://github.com/vllm-project/vllm/pull/30369) [Fix] Add default rope theta for qwen1 model â€” qwen â€” by iwzbi (å…³é—­äº: 2025-12-12 01:31 (UTC+8))
- [#30278](https://github.com/vllm-project/vllm/pull/30278) [CPU][Bugfix] Fix CPU Profiler issue â€” v1 â€” by zhili03 (å…³é—­äº: 2025-12-12 00:48 (UTC+8))
- [#30122](https://github.com/vllm-project/vllm/pull/30122) [Bugfix][Async] fix update_async_output_token_ids for async + spec â€” v1 â€” by izhuhaoran (å…³é—­äº: 2025-12-12 00:20 (UTC+8))
- [#28252](https://github.com/vllm-project/vllm/pull/28252) [WIP][KVConnector] Retrive KV events from LMCache â€” needs-rebase,kv-connector â€” by hickeyma (å…³é—­äº: 2025-12-11 22:41 (UTC+8))
- [#27718](https://github.com/vllm-project/vllm/pull/27718) Feature/kv cache average lifetime â€” documentation,needs-rebase,v1 â€” by alhridoy (å…³é—­äº: 2025-12-11 19:17 (UTC+8))
- [#26892](https://github.com/vllm-project/vllm/pull/26892) Fix uniform_decode=True in prefill when using CUDA Graph with single-token prompt â€” v1,nvidia â€” by Sugar-zsg (å…³é—­äº: 2025-12-11 16:22 (UTC+8))
- [#30468](https://github.com/vllm-project/vllm/pull/30468) [Feat] EPD Mooncake store â€” documentation,v1,kv-connector â€” by khuonglmhw (å…³é—­äº: 2025-12-11 15:33 (UTC+8))
- [#30467](https://github.com/vllm-project/vllm/pull/30467) [Optimization]: Add fused router for GPTOSS â€” gpt-oss â€” by ijpq (å…³é—­äº: 2025-12-11 15:42 (UTC+8))
- [#30457](https://github.com/vllm-project/vllm/pull/30457) [Feat] Mooncake storage connector for E-PD disaggregate â€” documentation,v1,kv-connector â€” by khuonglmhw (å…³é—­äº: 2025-12-11 15:22 (UTC+8))
- [#30454](https://github.com/vllm-project/vllm/pull/30454) [Optimization]: Add fused router for GPTOSS â€” gpt-oss â€” by ijpq (å…³é—­äº: 2025-12-11 13:36 (UTC+8))
- [#26617](https://github.com/vllm-project/vllm/pull/26617) feat: implement compact encoder cache for memory optimization â€” v1 â€” by liangwen12year (å…³é—­äº: 2025-12-11 13:08 (UTC+8))
- [#30422](https://github.com/vllm-project/vllm/pull/30422) [ROCm][CI][Bugfix] Fallback for grouped_topk when num_experts can't be grouped properly â€” rocm â€” by micah-wil (å…³é—­äº: 2025-12-11 12:01 (UTC+8))
- [#30451](https://github.com/vllm-project/vllm/pull/30451) [Core] Optimize encoder cache with mask-based storage â€” v1,multi-modality â€” by sunYtokki (å…³é—­äº: 2025-12-11 11:10 (UTC+8))
- [#30450](https://github.com/vllm-project/vllm/pull/30450) [Core] Optimize encoder cache with mask-based storage â€” v1,multi-modality â€” by sunYtokki (å…³é—­äº: 2025-12-11 11:02 (UTC+8))